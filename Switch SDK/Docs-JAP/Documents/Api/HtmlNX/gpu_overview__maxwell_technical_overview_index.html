<!-- HTML header for doxygen 1.8.8-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.10"/>
<title>NintendoSDK API Reference: Maxwell Technical Overview</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="openclose.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { init_search(); });
</script>
<link href="stylesheet.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">NintendoSDK API Reference
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- 構築: Doxygen 1.8.10 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'検索');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>総合概要</span></a></li>
      <li class="current"><a href="pages.html"><span>諸情報</span></a></li>
      <li><a href="modules.html"><span>モジュール</span></a></li>
      <li><a href="namespaces.html"><span>名前空間</span></a></li>
      <li><a href="annotated.html"><span>クラス</span></a></li>
      <li><a href="files.html"><span>ファイル</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="検索" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="_page_graphics_for_n_x.html">グラフィックス for NX</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Maxwell Technical Overview </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>目次</h3>
<ul><li class="level1"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_1">1. Scope</a></li>
<li class="level1"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_2">2. Overview</a><ul><li class="level2"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_2_1">2.1. BLOCK DIAGRAM</a></li>
<li class="level2"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_2_2">2.2. GRAPHICS PROCESSOR CLUSTER (GPC)</a><ul><li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_overview_TPC">2.2.1. Texture Processing Cluster (TPC)</a></li>
</ul>
</li>
<li class="level2"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_2_3">2.3. FRAME BUFFER PARTITION (FBP)</a></li>
</ul>
</li>
<li class="level1"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_3">3. ARCHITECTURE HIGHLIGHTS</a><ul><li class="level2"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_3_1">3.1. MAXWELL&#39;S SM</a></li>
<li class="level2"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_3_2">3.2. DEDICATED ACCELERATORS FOR SHADER OFFLOAD</a></li>
<li class="level2"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_highlight_FRAME_BUFFER_COMPRESSION">3.3. FRAME BUFFER COMPRESSION</a></li>
<li class="level2"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_highlight_TILED_CACHING_ARCHITECTURE">3.4. TILED CACHING ARCHITECTURE</a></li>
<li class="level2"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_3_5">3.5. UNIFIED L2 CACHE</a></li>
</ul>
</li>
<li class="level1"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_4">4. UNIT DESCRIPTIONS</a><ul><li class="level2"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_4_1">4.1. HUB UNITS</a><ul><li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_4_1_1">4.1.1. Host</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_4_1_2">4.1.2. FE (Front End)</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_4_1_3">4.1.3. PD (Primitive Distributor)</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_4_1_4">4.1.4. CWD (Compute Work Distributor)</a><ul><li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_4_1_4_1">4.1.4.1. QMDs</a></li>
</ul>
</li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_4_1_5">4.1.5. CE (Copy Engine)</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_unit_descriptions_RSTR2D">4.1.6. RSTR2D (2d Rasterizer)</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_4_1_7">4.1.7. Video and Display Engines: Differences between NX and dGPUs</a></li>
</ul>
</li>
<li class="level2"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_4_2">4.2. GRAPHICS UNITS</a><ul><li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_4_2_1">4.2.1. Setup</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_4_2_2">4.2.2. Raster</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_4_2_3">4.2.3. Zcull</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_4_2_4">4.2.4. PROP (Pre &quot;ROP&quot;)</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_4_2_5">4.2.5. Raster Op Unit (ROP)</a><ul><li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_4_2_5_1">4.2.5.1. ZROP (Z ROP)</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_4_2_5_2">4.2.5.2. CROP (Color ROP)</a></li>
</ul>
</li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_4_2_6">4.2.6. GPM (Graphics Pipe Manager)</a></li>
</ul>
</li>
<li class="level2"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_4_3">4.3. THE TPC</a><ul><li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_4_3_1">4.3.1. SM (Streaming Multiprocessor)</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_4_3_2">4.3.2. TEX (Texture Unit)</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_4_3_3">4.3.3. Primitive Engine (PE and PES)</a></li>
</ul>
</li>
<li class="level2"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_4_4">4.4. MEMORY SUBSYSTEM</a><ul><li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_4_4_1">4.4.1. MMU (Memory Management Unit)</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_4_4_2">4.4.2. Crossbar (XBAR)</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_4_4_3">4.4.3. Level 2 Cache</a></li>
</ul>
</li>
</ul>
</li>
<li class="level1"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_5">5. MAPPING THE GRAPHICS PIPELINE</a><ul><li class="level2"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_5_1">5.1. The ES 3.0 / DX9 PIPELINE</a></li>
<li class="level2"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_5_2">5.2. OPENGL4.3/DX11 ENHANCEMENTS</a><ul><li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_5_2_1">5.2.1. Tessellation and Geometry Shaders</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_5_2_2">5.2.2. Stream Output</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_5_2_3">5.2.3. Unordered Access Views (UAVs)</a></li>
</ul>
</li>
<li class="level2"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_5_3">5.3. POST-DX11 GRAPHICS FEATURES</a></li>
<li class="level2"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_5_4">5.4. TILED CACHING</a></li>
<li class="level2"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_graphics_FRAME_BUFFER_COMPRESSION">5.5. FRAME BUFFER COMPRESSION</a><ul><li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_graphics_Z_AND_STENCIL_PACKING">5.5.1. Z and Stencil Packing</a></li>
</ul>
</li>
</ul>
</li>
<li class="level1"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_6">6. MAPPING THE COMPUTE PIPELINE</a></li>
<li class="level1"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_7">7. PROGRAMMING CONSIDERATIONS</a><ul><li class="level2"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_7_1">7.1. CPU-&gt;GPU COMMAND TRANSFER</a><ul><li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_7_1_1">7.1.1. Host operation</a><ul><li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_7_1_1_1">7.1.1.1. Host data scheduling</a></li>
</ul>
</li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_7_1_2">7.1.2. Synchronization</a><ul><li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_7_1_2_1">7.1.2.1. Semaphores</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_7_1_2_2">7.1.2.2. Syncpoints</a></li>
</ul>
</li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_7_1_3">7.1.3. Channel Scheduling</a></li>
</ul>
</li>
<li class="level2"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_7_2">7.2. PROGRAMMING STATE</a><ul><li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_7_2_1">7.2.1. Method Stream</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_7_2_2">7.2.2. Constant Buffers</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_7_2_3">7.2.3. Texture Headers</a></li>
</ul>
</li>
<li class="level2"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_programming_DATA">7.3. DATA</a><ul><li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_7_3_1">7.3.1. Enhanced Memory Operations</a><ul><li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_7_3_1_1">7.3.1.1. Atomics and Reductions</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_7_3_1_2">7.3.1.2. Surface LD/ST</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_7_3_1_3">7.3.1.3. Texture LDs</a></li>
</ul>
</li>
</ul>
</li>
<li class="level2"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_7_4">7.4. MEMORY MANAGEMENT</a><ul><li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_7_4_1">7.4.1. Graphic MMU, or GMMU</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_7_4_2">7.4.2. NX System MMU, or SMMU</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_7_4_3">7.4.3. Surface Addressing</a><ul><li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_7_4_3_1">7.4.3.1. Pitch Addressing</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_7_4_3_2">7.4.3.2. Block Linear Addressing</a></li>
</ul>
</li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_7_4_4">7.4.4. Multisample Textures</a></li>
</ul>
</li>
</ul>
</li>
<li class="level1"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8">8. PERFORMANCE CHARACTERISTICS</a><ul><li class="level2"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_performance">8.1. PERFORMANCE SUMMARY</a></li>
<li class="level2"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_2">8.2. SM</a><ul><li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_2_1">8.2.1. SM and Its Neighbors</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_2_2">8.2.2. SM Internals</a><ul><li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_2_2_1">8.2.2.1. Micro-architectural Organization</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_2_2_2">8.2.2.2. SIMT Execution Model</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_2_2_3">8.2.2.3. Instruction Set and Assembly</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_2_2_4">8.2.2.4. Memory Model</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_2_2_5">8.2.2.5. Compute-Specific Features</a></li>
</ul>
</li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_2_3">8.2.3. Software View of the SM</a><ul><li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_2_3_1">8.2.3.1. Function Units</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_2_3_2">8.2.3.2. Shared Resources</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_2_3_3">8.2.3.3. Local, Global, and Surface Memory Accesses</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_performance_FP16">8.2.3.4. FP16 support</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_2_3_5">8.2.3.5. Warp-specific instructions</a></li>
</ul>
</li>
</ul>
</li>
<li class="level2"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_3">8.3. CACHING SUBSYSTEM</a><ul><li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_3_1">8.3.1. L1 behavior</a><ul><li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_3_1_1">8.3.1.1. L1 and compressed data</a></li>
</ul>
</li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_3_2">8.3.2. L2 behavior</a><ul><li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_3_2_1">8.3.2.1. High Level View</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_3_2_2">8.3.2.2. L2 bandwidth</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_3_2_3">8.3.2.3. CBC (Compression Bit Cache)</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_3_2_4">8.3.2.4. Decompression</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_3_2_5">8.3.2.5. Cache eviction policy</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_3_2_6">8.3.2.6. Write policy</a></li>
</ul>
</li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_3_3">8.3.3. Memory Management Subsystem</a><ul><li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_3_3_1">8.3.3.1. The GMMU</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_3_3_2">8.3.3.2. The SMMU</a></li>
</ul>
</li>
</ul>
</li>
<li class="level2"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_4">8.4. WORK SCHEDULING</a><ul><li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_4_1">8.4.1. Host and Host1x</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_4_2">8.4.2. Channels and Subchannels</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_4_3">8.4.3. Synchronization</a><ul><li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_4_3_1">8.4.3.1. Semaphores</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_4_3_2">8.4.3.2. Sync Points</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_4_3_3">8.4.3.3. Wait for Idle (WFI)</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_4_3_4">8.4.3.4. Pixel Barriers</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_4_3_5">8.4.3.5. Compute Sked Synchronization</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_4_3_6">8.4.3.6. GPU/CPU data synchronization</a></li>
</ul>
</li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_4_4">8.4.4. Frontend Unit/State</a><ul><li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_4_4_1">8.4.4.1. Inline2memory Copies</a></li>
</ul>
</li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_4_5">8.4.5. Synchronous copies</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_4_6">8.4.6. State Flow</a><ul><li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_4_6_1">8.4.6.1. Constants</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_4_6_2">8.4.6.2. Differences between Compute and Graphics</a></li>
</ul>
</li>
</ul>
</li>
<li class="level2"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_5">8.5. GRAPHICS WORLD SPACE PIPELINE</a><ul><li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_5_1">8.5.1. Index Fetch</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_5_2">8.5.2. Vertex Consolidation</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_5_3">8.5.3. Vertex Fetch</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_5_4">8.5.4. Inter-Stage Buffer Entries (ISBEs)</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_5_5">8.5.5. Circular Buffers</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_5_6">8.5.6. Viewport, Stream Output and Clip (VSC)</a><ul><li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_5_6_1">8.5.6.1. Clipping</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_5_6_2">8.5.6.2. Culling</a></li>
</ul>
</li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_5_7">8.5.7. Tesselation &amp; Geometry Shading</a><ul><li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_5_7_1">8.5.7.1. Alpha/Beta Phases</a></li>
</ul>
</li>
</ul>
</li>
<li class="level2"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6">8.6. GRAPHICS PIXEL PIPELINE</a><ul><li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_1">8.6.1. Tiled Caching</a><ul><li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_1_1">8.6.1.1. Choosing Cache Tile Size</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_1_2">8.6.1.2. When to insert a tile cache flush</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_1_3">8.6.1.3. Events that can cause tile to flush</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_1_4">8.6.1.4. Tiling performance considerations</a></li>
</ul>
</li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_2">8.6.2. Setup</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_3">8.6.3. Coarse Raster</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_4">8.6.4. Zcull</a><ul><li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_4_1">8.6.4.1. Zcull and Shader Modified Z</a></li>
</ul>
</li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_5">8.6.5. Fine Raster</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_6">8.6.6. STRI and TRAM</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_7">8.6.7. PROP</a><ul><li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_7_1">8.6.7.1. Raster Tiling and Subtiling</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_7_2">8.6.7.2. Multiple Render Targets</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_7_3">8.6.7.3. ROP State</a></li>
</ul>
</li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_8">8.6.8. ZROP</a><ul><li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_8_1">8.6.8.1. Shader-Modified Z</a></li>
</ul>
</li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_9">8.6.9. CROP</a><ul><li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_9_1">8.6.9.1. MSAA Pixel to Sample Expansion</a></li>
</ul>
</li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_10">8.6.10. Work Distribution to SMs</a><ul><li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_10_1">8.6.10.1. Work distribution in the graphics pipeline</a></li>
</ul>
</li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_11">8.6.11. Transitions between earlyZ and lateZ</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_12">8.6.12. New feature performance</a><ul><li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_12_1">8.6.12.1. FastGS</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_12_2">8.6.12.2. Vertex Position Coordinate Swizzle and Viewport Multicast</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_12_3">8.6.12.3. Programmable Sample Position</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_12_4">8.6.12.4. FP16 atomics</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_12_5">8.6.12.5. Pixel Shader Interlock</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_12_6">8.6.12.6. Iterated Blend</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_12_7">8.6.12.7. Target Independent Rasterization</a></li>
</ul>
</li>
</ul>
</li>
<li class="level2"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_7">8.7. COMPUTE PIPELINE</a><ul><li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_7_1">8.7.1. Launch Throughput</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_7_2">8.7.2. Dependent Launch Latency</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_7_3">8.7.3. Work distribution in the compute pipeline</a></li>
</ul>
</li>
<li class="level2"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_perf_TEXTURE">8.8. TEXTURE</a></li>
<li class="level2"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_9">8.9. COMPRESSION</a><ul><li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_9_1">8.9.1. Generating compressed data</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_9_2">8.9.2. GPU compression-aware clients</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_9_3">8.9.3. Naive clients</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_9_4">8.9.4. Color compression formats</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_9_5">8.9.5. Depth compression formats</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_8_9_6">8.9.6. Dst reduce optimization (lossy compression)</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_perf_read_modify_write">8.9.7. Read-modify-write expands</a></li>
</ul>
</li>
</ul>
</li>
<li class="level1"><a href="#gpuOverview_MaxwellTechnicalOverview_guide_sec_9">9. GLOSSARY OF TERMS</a></li>
</ul>
</div>
<div class="textblock"><h1><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_1"></a>
1. Scope</h1>
<p>This document describes the Maxwell GPU architecture used in the NX SOC, and some of its performance characteristics. The intended audience is game developers.</p>
<p>This document focuses on the graphics and compute pipeline, and supporting engines. This document doesn’t cover video or display pipelines.</p>
<h1><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_2"></a>
2. Overview</h1>
<p>This section focuses on a very high-level view of the GPU subsystem.</p>
<h2><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_2_1"></a>
2.1. BLOCK DIAGRAM</h2>
<p>Figure 1 is a top-level block diagram of the Maxwell architecture, as used in NX.</p>
<div class="image">
<img src="MaxwellTechnicalOverview_img01.jpg" alt="MaxwellTechnicalOverview_img01.jpg"/>
</div>
 <div style="text-align: center;" markdown="1"> <b>Figure 1: Top-level block diagram of the Maxwell Architecture(integrated)</b> </div><p> <br />
</p>
<p>There are four major sections of the Maxwell GPU:</p>
<ul>
<li>The Hub, which contains GPU control units that are not replicated when scaling up the architecture to higher performance. This section include the HOST, which talks to the CPU and reads command buffers, units for scheduling and distributing work to the GPCs (PD for graphics and CWD for compute), and dedicated engines like the copy engine and raster2D.</li>
<li>The Graphics Processing Cluster (GPC), which contains most of the graphics and compute processing units.</li>
<li>The Frame Buffer Partition (FBP), which contains the Shared L2 cache along with ROP units, and on NX, interfaces to the SOC's memory controller.</li>
<li>The XBAR (Crossbar), which connects the other units together.</li>
</ul>
<p>There are two units of replication that deserve special attention, the Graphics Processor Cluster (GPC) and the Frame Buffer Partition (FBP). These will be described in more detail in the following sections. Discrete GPUs can have multiple of each, while NX has only one GPC and one FBP.</p>
<h2><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_2_2"></a>
2.2. GRAPHICS PROCESSOR CLUSTER (GPC)</h2>
<p>The GPC contains most of the graphics and compute processing units. It can be divided into three separate sets of units:</p>
<ul>
<li>The Texture Processing Cluster (TPC). The number of TPCs per GPC can vary across the product line. The TPC includes the Streaming Multiprocessor (SM), where all shader code is run.</li>
<li>Fixed function graphics units, which perform vertex fetch and processing, primitive processing, rasterization, and communicating with the SM and ZROP and CROP (in the FBP).</li>
<li>The GPM and associated units which manage movement of data between fixed function units and the SMs.</li>
</ul>
<p>Integrated and low-end Maxwell GPUs contain only one GPC, while higher end dGPUs contain multiple GPCs. This allows for multiple setup and raster engines, which increases the primitive throughput beyond one primitive per clock, and increases the pixel fill rate.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_overview_TPC"></a>
2.2.1. Texture Processing Cluster (TPC)</h3>
<p>The TPC is the core unit of graphics functionality, including graphics and compute shader functions, texture and primitive engine (PE) functions, including attribute fetch and plane equation calculations. A GPC can contain up to five TPC units.</p>
<p>The TPC is composed of the following units:</p>
<ul>
<li>The Streaming Multiprocessor (SM), which executes all shader programs, and contains shared memory.</li>
<li>Two Texture units (TEX), which performs global, local and surface loads to memory, along with the texture mapping functions.</li>
<li>The Primitive Engine (PE), which performs all per-primitive operations: Attribute fetch, plane equation calculations, and communication between shader stages. Culling, clipping viewport transformation and tessellation are performed by the PES block, shared between multiple TPCs.</li>
</ul>
<h2><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_2_3"></a>
2.3. FRAME BUFFER PARTITION (FBP)</h2>
<p>The Frame Buffer Partition contains the following units:</p>
<ul>
<li>The L2 cache, which interfaces to all other units and the memory controller. Along with being a cache, the L2 also performs global atomic operations and compression support. In the graphics pipe, the L2 is used to pass data between units (without flushing to memory) and is used as the tiling buffer.</li>
<li>The ROP units, which perform Z and stencil compare and update, along with color blending and update.</li>
<li>In discrete GPU chips, the FBP also contains the memory controller. In SOCs, the L2 interfaces to the memory controller, which is outside the GPU block.</li>
</ul>
<h1><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_3"></a>
3. ARCHITECTURE HIGHLIGHTS</h1>
<p>The Maxwell GPU architecture is the most efficient and advanced GPU NVIDIA has developed (so far). Some of the key architectural capabilities that contribute to its performance and power efficiency are summarized below.</p>
<h2><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_3_1"></a>
3.1. MAXWELL'S SM</h2>
<p>Maxwell's SM is our latest Streaming Multiprocessor architecture. Particularly for modern graphics applications, the shader is the most important unit in the GPU. The SM is optimized for a wide diversity of shader workloads, including texture heavy domains, advanced lighting workloads such as deferred lighting/shading, and GPGPU (Compute) programs. Shader efficiency (performance @ watt) is a combination of two factors:</p>
<ol type="1">
<li>Energy per operation at a given frequency</li>
<li>Delivered performance relative to peak operations per clock</li>
</ol>
<p>The Kepler SM implementation in Tegra K1 was the best in the industry on the first metric above. Maxwell's SM architecture reduces energy per operation by an additional 35%, comparing in the same process technology. The Maxwell SM also raises the bar again on delivered performance efficiency, achieving more than 1.3x improvement in delivered performance relative to peak GFLOPs, compared to the Kepler SM.</p>
<p>The Maxwell SM achieves these results through a significant re-architecture of the processor architecture, including a new scheduler architecture and a new internal datapath organization that reduces sharing of execution units, saving area and power and reducing computation latency.</p>
<h2><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_3_2"></a>
3.2. DEDICATED ACCELERATORS FOR SHADER OFFLOAD</h2>
<p>Maxwell is designed to ensure that the shader processors are only used for actual user provided program execution. We have a dedicated color pixel datapath (CROP) to perform all pixel output operations including format conversion and blending. Each CROP unit can perform a full speed four wide blend (Src*SrcAlpha+Dst*DstAlpha) per clock, or 8 MAD operations per pixel. We also have a dedicated high throughput Z pixel datapath (ZROP) to perform all Z operations.</p>
<p>Finally, connected to the shader itself we have a "special function
unit" that handles all attribute interpolations in dedicated hardware. Each SFU can perform one Ax+By+C operation per clock, or 2 MAD operations.</p>
<p>While the utilization of these accelerators will vary depending on workload, we have found that there are many workloads for which these dedicated accelerators have very high utilization - for example any textured blend workload will make heavy use of both SFU and CROP.</p>
<p>For Tessellation we take a similar approach, using dedicated Patch Assembly and Tessellator units between the programmable tessellation stages, rather than implementing these functions as shader preambles.</p>
<h2><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_highlight_FRAME_BUFFER_COMPRESSION"></a>
3.3. FRAME BUFFER COMPRESSION</h2>
<p>NVIDIA has supported frame buffer compression for many years, continuously improving algorithms and performance. Maxwell has custom format-specific compression algorithms for Z (i.e. Z16, Z24) and color (i.e. RGBA 8-8-8-8, SRGB, RGBA 10-10-10-2, FP16, etc.) with multiple layers of compression algorithms per format to achieve the best possible compression. In NX compression is transparent to all GPU clients, but not Host1x clients, which don't go through the GPU's L2. Note: VIC (Video image compositor, a separate engine) and Display do support compression in NX, but isn’t supported in NVN.</p>
<p>ROP is the only client that can write data compressed, and ROP can operate on Z and Color data while compressed to improve performance and power efficiency. For other clients, L2 decompresses data before sending to the client, allowing all GPU engines to read from compressed surfaces directly, including texture.</p>
<p>The compression ratios supported by Maxwell include:</p>
<ul>
<li>"Zero bandwidth clear" format - memory blocks in this format do not result in any memory traffic, the memory value is known based solely on compression state information. The compression state does consume a small amount of bandwidth, but at about 1/1024th of the data payload bandwidth, this is effectively negligible. This format ensures that surface clears are free as is reading clear data.</li>
<li>8:1 compression : for any 1xAA color memory block that is locally uniform, or a 4xAA or 8xAA memory block with fully covered samples that are similar in value, or any Z memory block with relatively low geometric complexity (some geometry edges are allowed), in FP32 Z format.</li>
<li>4:1 compression: for any 4xAA color memory block that is locally uniform, or any Z memory block (more geometry edges allowed).</li>
<li>2:1 compression, also known as "arithmetic compression" : for any color memory block which, when encoded as a set of differences from a reference value, fits within a half sized memory container.</li>
</ul>
<p>The following figures illustrate some applications for compression. Figure 2 below is a sample screenshot from "Metro Last Light". Figure 3 shows the same image with all compressible blocks colored in magenta. Almost all of this content is compressible, including much of the high-complexity foreground, illustrating the ability of Maxwell's compression algorithms to handle diverse image content.</p>
<div class="image">
<img src="MaxwellTechnicalOverview_img02.jpg" alt="MaxwellTechnicalOverview_img02.jpg"/>
</div>
 <div style="text-align: center;" markdown="1"> <b>Figure 2: Metro Last Light Image</b> </div><p> <br />
 </p><div class="image">
<img src="MaxwellTechnicalOverview_img03.jpg" alt="MaxwellTechnicalOverview_img03.jpg"/>
</div>
 <div style="text-align: center;" markdown="1"> <b>Figure 3: Metro Last Light Compressed Image</b> </div><p> <br />
</p>
<p>Note: This example shows the effects of compression on the final image. In Maxwell, all ROP writes attempt to be compressed, so bandwidth is saved during the rendering of the scene. Amount of overall compression seen will be different during rendering.</p>
<h2><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_highlight_TILED_CACHING_ARCHITECTURE"></a>
3.4. TILED CACHING ARCHITECTURE</h2>
<p>Maxwell adds a tiling/binning stage to the rendering pipeline, enabling Maxwell to collapse overdraw in color and Z rendering. Maxwell supports optimizations including Z buffer discard and on chip 4xAA resolve for fully binned frames.</p>
<p>One significant challenge for tiling is geometry bandwidth. Tilers require added geometry bandwidth due to the need to process primitives twice, once to determine bin or screen coverage information, and a second time per tile to actually rasterize the primitive. With effective compression and caching of color and Z traffic, the geometry bandwidth cost often exceeds the color/Z traffic savings for more complex scenes, if it is allowed to spill off chip. To address this issue, Maxwell's tiler leverages the GPU L2 cache structure. Geometry data is sent to the L2 and can either be allowed to spill off chip, or can be held in L2 as an on-chip FIFO allocation, with the tiler switching to a new tiling phase when that FIFO is filled. The latter approach allows Maxwell to effectively handle complex scenes with high geometry content - portions of the scene with good tiling opportunity (lens flare or smoke type effects with heavy blending for example) get the expected benefit, while in other portions of the scene with heavy geometry, keeping that geometry data on chip ensures that Maxwell doesn't pay a bandwidth penalty, and can still get an opportunistic bandwidth benefit from overlaps in that geometry stream.</p>
<h2><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_3_5"></a>
3.5. UNIFIED L2 CACHE</h2>
<p>Like previous GPU generations, Maxwell has a unified L2 cache, used by all gpu engines. A unified L2 cache offers the benefits of working set caching to all of the clients within the GPU (geometry, texture, compute, rop) out of a shared pool of memory. GPUs need to handle a wide diversity of workloads, and a unified cache offers the potential to efficiently provide memory to the clients that need it most at a given point in time. Maxwell is the third architectural generation of unified cache from NVIDIA and has our latest refinements in cache management algorithms.</p>
<h1><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_4"></a>
4. UNIT DESCRIPTIONS</h1>
<p>This section describes units in the Maxwell architecture and what they do.</p>
<h2><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_4_1"></a>
4.1. HUB UNITS</h2>
<p>The following units are in the HUB section of the GPU.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_4_1_1"></a>
4.1.1. Host</h3>
<p>The HOST is the interface to the CPU, and schedules work on the engines. It interfaces to the CPU using register reads and writes for initialization. The Host controls scheduling of different application contexts, and different work queues (Channels) within a single application context. It's also in charge of resolving control flow dependencies between the channels/applications.</p>
<p>The SW conveys the contexts to be scheduled to the Host unit via an in memory data structure called a "Runlist".</p>
<p>Work for a Channel is put into a pushbuffer, and fetched from memory by the Host unit. The Host unit forward methods (instructions) to the top of the engine pipe, after resolving any dependency operations in the method stream. The Host method fetcher can operate on a different channel, or context than the rest of the graphics pipe.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_4_1_2"></a>
4.1.2. FE (Front End)</h3>
<p>The FE unit interprets the commands from runlist (through the host) does consistency checking, and translate it into commands that the rest of the GPU can understand. The FE is also the clearing house for ensuring the graphics pipe is flushed at the right time before proceeding and other synchronization operations.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_4_1_3"></a>
4.1.3. PD (Primitive Distributor)</h3>
<p>The PD is the first dedicated unit for the 3D graphics pipeline. It fetches triangle lists from memory and distributes primitives to GPCs and TPCs. The PDB subunit handles additional distribution tasks involved in tessellation and/or geometry shading. The SCC subunit performs certain tasks for synchronizing between draw commands and state changes that affect downstream graphics units.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_4_1_4"></a>
4.1.4. CWD (Compute Work Distributor)</h3>
<p>CWD is the first dedicated unit for the Compute/CUDA pipeline. In traditional compute operations it breaks grids into thread blocks (also called CTA - cooperative thread arrays) and distributes them across the TPCs. The SKED subunit provides nested parallelism support, builds lists of compute work tasks in memory, and fetches the list, and CWD sends Thread blocks of work to the TPCs.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_4_1_4_1"></a>
4.1.4.1. QMDs</h4>
<p>A QMD is an in-memory description of a compute task. SKED reads this description on task launch and manages the resulting scheduling actions. For example, the QMD specifies whether the task is a grid (fixed 3-D array of CTAs) or a queue (dynamic FIFO of CTAs). It also specifies priority, end-of-grid semaphore operations, program offset, etc.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_4_1_5"></a>
4.1.5. CE (Copy Engine)</h3>
<p>The Copy Engine is used to DMA data from one place in memory to another. NX has a synchronous copy engine. The discrete GPUs also have one or two asynchronous copy engines. Synchronous means that ordering between the copy and operations in the GPU before or after the copy will be handled automatically by the GPU. Asynchronous copy engines allows for copies to happen in parallel with graphics processing. NX does not include asynchronous copy engines.</p>
<p>The copy engine can convert data between a linear memory layout and the GPU's 2d block linear format during a copy, and vice versa.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_unit_descriptions_RSTR2D"></a>
4.1.6. RSTR2D (2d Rasterizer)</h3>
<p>RSTR2D is a small, dedicated control unit for 2D rendering functions, like blits and rectangular fills. It uses the Texture pipes and part of the pixel pipeline to perform its functions, but it allows for most of the 3D graphics pipeline units to be disabled. Benefits of RSTR2D is a much simpler programming model, independent state and lower power operation than using the full graphics pipeline.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_4_1_7"></a>
4.1.7. Video and Display Engines: Differences between NX and dGPUs</h3>
<p>On discrete GPUs, video encode and decode, along with the display pipeline, are connected to the Host and the hub, routing memory requests through the L2. On NX, the video encode, decode engines, the VIC and the display pipeline are connected to host1x and directly to the memory subsystem, bypassing the L2. NX's HOST1X, video engines and display are not described in this document.</p>
<h2><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_4_2"></a>
4.2. GRAPHICS UNITS</h2>
<p>There are several fixed function graphics units in the GPU that reside either in the GPC or the FBP. They are described in the following sections.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_4_2_1"></a>
4.2.1. Setup</h3>
<p>The setup unit takes the primitive's screen-space x, y and z coordinates and generates edge equations for the rasterizer, a plane equation for z, and partial results used by the PE's STRI unit to calculate attribute plane equations.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_4_2_2"></a>
4.2.2. Raster</h3>
<p>The raster units take the edge equations from setup and generates bitmasks of pixel (or sample) coverage. The raster is divided into two units: a coarse raster unit which generates 16x16-pixel tiles that have some coverage, and a fine raster unit which determines exact coverage for samples within these tiles in 8x8 sample blocks. This coverage data is sent to PROP.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_4_2_3"></a>
4.2.3. Zcull</h3>
<p>Between coarse and fine raster is Zcull, which is a low-resolution on-chip Z buffer to trivially reject or accept tiles based on conservative depth comparisons. It operates on the 16x16 coarse-raster tiles, rejecting tiles if possible, and marking tiles that can be trivially accepted, so a depth test is not required later.</p>
<p>Every tile that exits ZCull can be in one of three states:</p>
<ul>
<li>DISCARDED. If the entire tile is behind the conservative depth range, and depth test is enabled, the tile is discarded.</li>
<li>TRIVIALLY ACCEPTED: If the entire tile is in front of the conservative depth range, and depth test is enabled, the tile is sent down the pipeline marked “trivially accepted”, and processed normally, except source Z isn’t read and depth test isn’t performed.</li>
<li>AMBIGUOUS. If neither of the above two states apply, the tile is sent downstream and processed normally, with the full Z-test being performed if enabled.</li>
</ul>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_4_2_4"></a>
4.2.4. PROP (Pre "ROP")</h3>
<p>PROP performs per-pixel pipeline operations that are outside the read modify write loop of ZROP and CROP. This includes format conversion, alpha test, alpha-to-coverage and address generation. It also handles synchronization and reconfigures the pipeline between early Z mode (depth test and update performed before shader) and late Z mode (depth test and update performed after shader).</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_4_2_5"></a>
4.2.5. Raster Op Unit (ROP)</h3>
<p>The PROP talks to two units that make up the ROP: the ZROP and CROP. Both units perform read-modify-write operations on data in memory based on the xy address of the pixel, and support compression natively, often working on compressed data directly to improve performance.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_4_2_5_1"></a>
4.2.5.1. ZROP (Z ROP)</h4>
<p>ZROP performs the Z and Stencil compare and read-modify write. It is optimized to perform Z compare on compressed Z values as fast as possible. ZROP also performs Z compression and writes Z values to the L2 in compressed format when possible.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_4_2_5_2"></a>
4.2.5.2. CROP (Color ROP)</h4>
<p>CROP performs color blending of source color and alpha with the destination color when blending is enabled. It also formats, aligns, and writes color data to L2. It is optimized to blend compressed data at an accelerated rate, and compresses the final result when possible.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_4_2_6"></a>
4.2.6. GPM (Graphics Pipe Manager)</h3>
<p>GPM is responsible for distributing vertex and pixel work to the TPCs, and collecting pixel data from the TPCs, reordering it and sending it to the PROP. It includes the Work Distribution Crossbar (WDX) unit that handles vertex distribution for world and screen space, and the MPC unit that interfaces to the SM. The WDX unit includes the binner for tiled caching support.</p>
<h2><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_4_3"></a>
4.3. THE TPC</h2>
<p>As described in <a class="el" href="gpu_overview__maxwell_technical_overview_index.html#gpuOverview_MaxwellTechnicalOverview_guide_overview_TPC">2.2.1. Texture Processing Cluster (TPC)</a>, the TPC is the core unit of graphics functionality. See below for a detailed block diagram of the TPC.</p>
<div class="image">
<img src="MaxwellTechnicalOverview_img04.jpg" alt="MaxwellTechnicalOverview_img04.jpg"/>
</div>
 <div style="text-align: center;" markdown="1"> <b>Figure 4: TPC Block Diagram</b> </div><p> <br />
</p>
<p>At the top of the block diagram is the Primitive Engine (PE) unit, containing processing functions that prepare geometry for shader execution. Then an instruction cache holds instructions for all programs currently active in the SM. Below the instruction cache are four shader execution units. Pairs of shader execution units share a texture/L1 data cache capable of four bilinearly filtered pixels per clock. Finally, there is a Shared Memory / "MIO" unit that is available to all four shader execution units and is used primarily for GPU compute programs that include shared memory allocations. See below for more information on these units.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_4_3_1"></a>
4.3.1. SM (Streaming Multiprocessor)</h3>
<p>The SM is the core processing unit in the GPU, running all shader programs.</p>
<p>Each SM has four shader execution units. Each subunit has its own instruction buffer, warp scheduler, register file, and 32 CUDA cores (math units), with support for a diverse set of instructions, including "FMA" (Fused Multiply Add), a 32 bit floating point multiply add that is fully compliant with the latest IEEE 754 precision requirements. Therefore there are 128 FMA operations per SM.</p>
<p>All four execution units can perform load, store and atomic operations to a shared memory block, which is used explicitly in compute programs and used to store intermediate geometry data in graphics operation.</p>
<p>Each pair of execution units also contains dedicated LD/ST units for memory operations, and Special Function Units (SFU). The most important function of the SFU is to handle all attribute (especially texture attribute) interpolation arithmetic. Attribute interpolation math is typically of the form Ax+By+C per attribute, and in many competing GPU architectures, is performed by general shader core math. In Maxwell this math is handled by dedicated units, so attribute interpolation is "free" (does not consume core math resources). Each SFU unit can perform an Ax+By+C calculation per clock, the equivalent of two FMA operations. Therefore, in total there are another 64 FMA operations per SM within the SFU units.</p>
<p>The SFU units also handle transcendental functions (rcp, rsqrt, log, sin, etc).</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_4_3_2"></a>
4.3.2. TEX (Texture Unit)</h3>
<p>TEX receives coordinates from the SMs, performs LOD and anisotropic calculations, walks the filter kernel, fetches, and filters the texture data. DXT1-5, and BC6-7 compression formats are supported. ASTC low dynamic range, 2d texture formats are also supported on NX. TEX contains an L1 cache to improve locality and is also used as the LD/ST pipeline for the SM. TEX throughput varies depending on filtering mode and texel format used.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_4_3_3"></a>
4.3.3. Primitive Engine (PE and PES)</h3>
<p>This engine performs primitive processing in the world space &amp; pixel pipeline. It's split into two units: The per TPC engine (PE) and the engine shared between pairs of TPCs (PES). It is comprised of the following sub-functions:</p>
<ul>
<li>Receive primitive data from PD, and perform vertex attribute fetch and copy data between the L2 and the SM's shared memory (PE).</li>
<li>Copy data to/from the L2 when multiple primitive shaders are used, and work is distributed between TPCs (PE).</li>
<li>Tessellate patches after the Hull Shader when Tessellation is enabled (PES).</li>
<li>Performs the Viewport, and Clipping, and performs Stream output at the end of the primitive processing pipeline (PES).</li>
<li>Perform plane equation calculations in the pixel pipeline for the SM (PE).</li>
</ul>
<h2><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_4_4"></a>
4.4. MEMORY SUBSYSTEM</h2>
<p>The GPU memory system is optimized to make 32 byte memory accesses. Caches, compression, and the memory controller are tuned to be as efficient as possible with 32 byte accesses.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_4_4_1"></a>
4.4.1. MMU (Memory Management Unit)</h3>
<p>The MMU performs the standard functions of converting virtual addresses to physical addresses and providing access protection. It also performs some non-standard functions: it tracks the compression format for individual memory pages and is used for sparse texturing and tiled resources - two new features in Maxwell.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_4_4_2"></a>
4.4.2. Crossbar (XBAR)</h3>
<p>The crossbar connects GPC and HUB units to the L2. Most control and all commands and data is communicated through the XBAR.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_4_4_3"></a>
4.4.3. Level 2 Cache</h3>
<p>All GPU Clients communicate to memory through the L2 cache. L2 bandwidth is configurable by adding "slices" - duplicate, address mapped copies of the L2. Along with providing caching, the L2 also includes hardware to perform global atomics and support for compression.</p>
<p>Compression is supported in L2 with a special cache used for per-tile compressed data status and with a decompressor, so when non-rop clients access compressed data it's uncompressed automatically. The ROP works on compressed data internally, so the L2 ships the unmodified data to the CROP and ZROP along with per-tile compression status.</p>
<h1><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_5"></a>
5. MAPPING THE GRAPHICS PIPELINE</h1>
<p>This section walks through the graphics pipeline and how it maps onto the Maxwell units.</p>
<h2><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_5_1"></a>
5.1. The ES 3.0 / DX9 PIPELINE</h2>
<p>This section describes the traditional graphics pipeline (pre-tessellation and compute), and how it maps onto the Maxwell architecture.</p>
<p>First, the CPU programs the HOST to fetch commands and state from the pushbuffer (also called runlists) and passing the commands to the Front End (FE), which checks for consistency and converts commands and state into bundles that are passed down the graphics pipeline.</p>
<p>The Primitive Distributor (PD) receives the command stream, fetches primitive lists from memory, and groups and distributes primitives to TPCs, through the Graphics Pipe Manager (GPM). The Primitive Engine (in the TPC) fetches vertex attributes from memory and puts them in the Streaming Multiprocessor's (SM's) shared memory.</p>
<p>The SM fetches the vertex attributes from the shared memory, performs the vertex shader and writes the results back to shared memory. The PE then fetches the attributes, performs Viewport Transform and Clipping, and puts all attribute data in a circular buffer that is mapped to main memory (but designed never to leave the L2 cache, based on cache policies).</p>
<p>Next, the WDX unit receives the geometry stream and performs the "binning" and tiling steps. See <a class="el" href="gpu_overview__maxwell_technical_overview_index.html#gpuOverview_MaxwellTechnicalOverview_guide_highlight_TILED_CACHING_ARCHITECTURE">3.4. TILED CACHING ARCHITECTURE</a> for more information on binning and tiled caching.</p>
<p>Next, geometry data for the active tile is sent from WDX to the Setup unit, which fetches the coordinate attributes from the circular buffers and calculates the edge equations for raster and the plane equations for Z.</p>
<p>Raster receives the edge equations and generates 16x16 pixel tiles that have some coverage. ZCull does coarse, conservative Z reject on the hidden tiles, and then Fine raster creates pixels with coverage data. Fine raster also creates per 8x8 Z data for ZROP.</p>
<p>PreROP (PROP) receives the Z and Coverage data, and if in early Z mode, sends the data to ZROP. ZROP perform hidden surface removal and sends back the coverage for the visible fragments.</p>
<p>Visible fragments are then sent to the SM for pixel shading. The pixel shader program calls texture to fetch texture data and perform texture filtering.</p>
<p>Final Color values are output by the SM, and sent to PROP which performs source only color modification, then routes the color to the right Color ROP (CROP) for final blending.</p>
<p>In Late Z mode (any time the shader or color pipeline can modify coverage or Z), PROP sends the data to ZROP after the shader instead of before the shader, for Z test and update. Pixels that pass Z test are then sent to the CROP for blending and color write.</p>
<h2><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_5_2"></a>
5.2. OPENGL4.3/DX11 ENHANCEMENTS</h2>
<p>With the advent of OpenGL4.3 and DX11, the graphics data flow has been enhanced significantly, with the addition of Tessellation and Geometry Shading.</p>
<p>This section will describe the major changes to the graphics data flow for OpenGL 4.3/DX11.</p>
<p>OpenGL 4.3 also added Compute, which will be described in the next chapter.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_5_2_1"></a>
5.2.1. Tessellation and Geometry Shaders</h3>
<p>The biggest change to the graphics pipeline was the addition of three new world-space shaders, to perform different levels of data expansion for tessellation and Geometry shading.</p>
<p>Figure 5 shows the new shaders added to the pipeline. Circled are the blocks added for tessellation. When tessellating, the input primitive to the graphics pipeline is a patch, made up of control points. The vertex shader does per-control point transformations. The control points are then assembled into patches which are processed by the Hull shader, which produces new patches after transformation. The Tessellator is a fixed function engine in the PE that performs data expansion, creating many points, lines or triangles from a given patch. The Domain shader performs per-vertex shading on the expanded primitives.</p>
<p>Finally, the Geometry shader works on independent primitives, doing per-primitive shading, and can even generate multiple primitives from one input primitive.</p>
<p>In OpenGL4.3/DX11, tessellation can be turned on or off, and Geometry shading can also be turned on and off independently. When off, Maxwell bypasses the new shader and fixed function stages to improve performance and power efficiency.</p>
<p>In Maxwell, Vertex and Hull shading will run on the same SM, and then after tessellation (which results in data expansion), work will be redistributed and then Domain and Geometry shading will run on the same (possibly new) SM. After all world space shading, the PE performs Viewport and Clip before setup and raster as it does in the DX9 model.</p>
<div class="image">
<img src="MaxwellTechnicalOverview_img05.jpg" alt="MaxwellTechnicalOverview_img05.jpg"/>
</div>
 <div style="text-align: center;" markdown="1"> <b>Figure 5: OpenGL4.3/DX11 world space pipeline data flow</b> </div><p> <br />
</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_5_2_2"></a>
5.2.2. Stream Output</h3>
<p>Stream output was added in DX10, allowing for geometry output to be streamed to memory in-order. This data can then be fed back into the graphics pipe later, as attributes and primitives fetched in PD/PE. Stream output happens after the last world space shader and before viewport transform. Stream output and rasterization can independently be controlled - either one or both can be processing the output of the world space pipeline.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_5_2_3"></a>
5.2.3. Unordered Access Views (UAVs)</h3>
<p>DX11 and OpenGL4.3 also added support for Unordered Access Views. This allows the graphics shaders to load or store from memory buffers directly. As the name implies, there’s no guarantee of API ordering when UAVs are read or written.</p>
<h2><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_5_3"></a>
5.3. POST-DX11 GRAPHICS FEATURES</h2>
<p>Maxwell contains many features that were added after the dx11 specification, but none change the overall dataflow in the graphics pipeline. More detail on these features will follow later.</p>
<h2><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_5_4"></a>
5.4. TILED CACHING</h2>
<p>Traditional tiling involves breaking up the render target into regular size tiles, and rendering all geometry in a tile before moving on to the next one. This is effectively rearranging the data to improve render target locality, keeping intermediate data on-chip, improving performance and reducing memory bandwidth.</p>
<p>NVIDIA's version of tiling is called "Tiled Caching", and leverages the capabilities of the L2 to provide intermediate primitive storage and render target caching.</p>
<p>When tiled caching, the circular buffer between the PE and Setup is larger, and holds the primitives that will be rendered. The render target is broken into "tiles", small enough the data can live in the L2 cache. Each tile is rendered sequentially, using the data in the circular buffer. On-chip structures track which primitives intersect which tiles, so setup only works on primitives for the current tile. The circular buffer lives in the L2, and there's special optimizations to ensure the circular buffer stays in the L2 and doesn't generate any memory traffic. For each tile, WDX fetches the relevant primitives and sends them to the pixel pipeline. The L2 cache acts as the tiling buffer, reducing overall bandwidth to one write per pixel in a tile.</p>
<p>Maxwell has optimizations to eliminate the Z write and do on-chip AA resolves, reducing overall bandwidth.</p>
<p>There are cases where, because of second-order effects, tiled caching can be slightly slower than conventional immediate mode rendering. Those are described in the performance section. In NVN, the app has explicit control over when tiled caching is enabled and how it is setup.</p>
<h2><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_graphics_FRAME_BUFFER_COMPRESSION"></a>
5.5. FRAME BUFFER COMPRESSION</h2>
<p>Maxwell uses frame buffer compression to reduce bandwidth to and from DRAM, reducing memory-system power as well. As ZROP and CROP prepare to write tiles of depth or color data to the L2, they determine whether the data in the tile can be represented in compressed form. If so, instead of writing the full tile, they write either:</p>
<ul>
<li>1/8 of the bytes in the tile (8:1 compression)</li>
<li>1/4 of the bytes in the tile (4:1 compression)</li>
<li>1/2 of the bytes in the tile (2:1 compression)</li>
<li>Zero-bandwidth-clears</li>
</ul>
<p>There are a variety of compression formats based on the type of surface being accessed. The tile is stored in the most compressed form that is feasible. Compression is lossless; if the data in the tile cannot be exactly encoded in a compressed representation, it is written out uncompressed. The compressed data for a tile is written to the same location in the DRAM buffer as the uncompressed tile would be, but it only occupies a fraction of the space. Thus, compression saves bandwidth, but not storage.</p>
<p>Compression status for each tile is maintained in the form of two compress status bits per tile, which are cached on chip, but can spill to DRAM if necessary. When a compressible tile is accessed, logic in the L2 queries the compress status bits, reads the appropriate amount of data from DRAM, and decompresses the data for the client. A few special clients (texture, ZROP, and CROP) can operate on compressed data directly, saving power and increasing throughput.</p>
<p>Maxwell accelerates clears by setting the compress status bits for tiles to be cleared to &lt;clear&gt; state, indicating that the data values for the tile can be found in an on-chip table, without any access to DRAM (on either the clear operation itself, or subsequent reads). We call these Zero-Bandwidth Clears. The Rasterizer and ROP units support an accelerated clear mode that rapidly traverses the tiles of a rectangle to be cleared, so clears are much faster than normal rendering.</p>
<p>Compression is applied at the roptile level (i.e. 256 bytes arranged as an 8x8 sample block in the case of 4 bytes pixel data). The figure below illustrates different compression scenarios.</p>
<div class="image">
<img src="MaxwellTechnicalOverview_img06.jpg" alt="MaxwellTechnicalOverview_img06.jpg"/>
</div>
 <div style="text-align: center;" markdown="1"> <b>Figure 6: Mapping 4 quadrants of a rop tile to different compression ratios</b> </div><p> <br />
</p>
<div class="image">
<img src="MaxwellTechnicalOverview_img07.jpg" alt="MaxwellTechnicalOverview_img07.jpg"/>
</div>
 <div style="text-align: center;" markdown="1"> <b>Figure 7: Decompressing Rop Tiles, 8:1, 4:1, then 2:1</b> </div><p> <br />
</p>
<p>Each roptile contains eight 32 bytes "subpackets". For uncompressed access, all subpackets must be read or written, while for compressed access, only a subset corresponding to the compression ratio need to be accessed. Figure 7 illustrates the active subpackets for different compression ratios.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_graphics_Z_AND_STENCIL_PACKING"></a>
5.5.1. Z and Stencil Packing</h3>
<p>When accessing stencil and Z, often only one of the two components will be used. Maxwell has optimized memory layout to ensure only the fields used are accessed, with no excess memory fetches.</p>
<p>For Z24S8, 32 Stencil values are packed into one 32 bytes memory access, where the corresponding Z24 values are packed into 3 subsequent 32 bytes blocks.</p>
<div class="image">
<img src="MaxwellTechnicalOverview_img11.jpg" alt="MaxwellTechnicalOverview_img11.jpg"/>
</div>
 <div style="text-align: center;" markdown="1"> <b>Figure 8: Z24S8 Layout</b> </div><p> <br />
</p>
<p>For Z32_S8X24 formats, Stencil is again packed into a 32 bytes block, with Z32 occupying four 32 bytes blocks, and three used for the X24 region. When using Z32_S8X24, Z, and Stencil can be accessed together or independently, and X24 is never fetched.</p>
<div class="image">
<img src="MaxwellTechnicalOverview_img12.jpg" alt="MaxwellTechnicalOverview_img12.jpg"/>
</div>
 <div style="text-align: center;" markdown="1"> <b>Figure 9: Z32_S8X24 Layout</b> </div><p> <br />
</p>
<p>Z32 without stencil packs 32 Z32 formats into a 256 bytes block, with no whitespace between blocks.</p>
<p>For 24bit Z/8 bit stencil and Z32_S8X24, only the Z data is compressed, stencil is stored uncompressed and each can be accessed independently or together.</p>
<p>When Non-ZROP units access these formats, L2 unpacks the data and presents it as Z24S8 or Z32_S8X24 per-sample values.</p>
<h1><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_6"></a>
6. MAPPING THE COMPUTE PIPELINE</h1>
<p>This section walks through the GPU-Compute (CUDA/OpenCL/Renderscript/OpenGL Compute) dataflow and how it maps onto Maxwell. Figure 10 illustrates how compute work flows through the pipeline.</p>
<div class="image">
<img src="MaxwellTechnicalOverview_img08.jpg" alt="MaxwellTechnicalOverview_img08.jpg"/>
</div>
 <div style="text-align: center;" markdown="1"> <b>Figure 10: Compute data flow</b> </div><p> <br />
</p>
<p>There are two major changes in how the GPU is used for compute vs graphics. The first is that almost all of the graphics pipeline is bypassed, to make the programming model as simple and flexible as possible. The second is that compute works on "grids" (arrays of threads all running the same program), that are diced into thread blocks, where threads within a block can communicate and coordinate with each other.</p>
<p>Compute operations start with the same flow as graphics - the CPU writes commands to the pushbuffer, host then reads the pushbuffer, and FE checks valid command state.</p>
<p>CWD performs a similar function to PD, distributing work to multiple SMs. CWD launches kernel grids, dices grids into thread blocks and distributes the thread blocks (CTAs) to SMs. CWD tracks CTA completion, launching new CTAs and performs synchronization between grids. In the case of Dynamic Parallelism, when threads on the GPU can create new GPU compute work, CWD and its SKED unit also manage this function.</p>
<p>MPC is the resource allocator for the SM, allocating shared memory, RF and other thread resources to the CTA send from CWD. MPC also expands a CTA into multiple threads and warps (32 threads running off the same PC).</p>
<p>The SM executes the CTA, using Shared, Local and Global memory, constant buffers and optionally texture fetches. On Maxwell, all Thread Local and Global memory accesses go through the texture unit. Threads in a CTA communicate through shared memory, using barriers to ensure read/write ordering.</p>
<p>Atomic operations can be launched by the SM to local, shared or Global memory. For local and global memory, atomic operations are performed by dedicated logic in the L2. For shared memory, the atomic operations are performed in the Shared memory part of the SM.</p>
<h1><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_7"></a>
7. PROGRAMMING CONSIDERATIONS</h1>
<p>This section documents areas where the GPU needs to communicate with other engines, and how it's programmed.</p>
<h2><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_7_1"></a>
7.1. CPU-&gt;GPU COMMAND TRANSFER</h2>
<p>This section describes work is sent from the CPU to the GPU.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_7_1_1"></a>
7.1.1. Host operation</h3>
<p>The CPU sends commands to the GPU through records in memory called Pushbuffers. Host is responsible for scheduling work from multiple contexts and launching it on the GPU.</p>
<p>Host operates on a two level command structure. The GPFIFO as the top level command list, which references pushbuffers as a second level.</p>
<div class="image">
<img src="MaxwellTechnicalOverview_img09.jpg" alt="MaxwellTechnicalOverview_img09.jpg"/>
</div>
 <div style="text-align: center;" markdown="1"> <b>Figure 11: GPFIFOs and Pushbuffers</b> </div><p> <br />
</p>
<p>Work is sent to the GPU using pushbuffer segments (also called Command buffers) in memory. A pushbuffer segment is written to memory, containing methods, some data, and pointers to data. Another area of memory contains the GPFIFO - a circular buffer containing methods, data, pointers to data, and pointers to pushbuffer segments. Each GPFIFO and associated pushbuffer segments is a "Channel": each channel can be scheduled by Host independently, except where synchronization events enforce ordering. The HOST supports multiple channels - each with its own MMU-protected address space and its own GPFIFO and pushbuffers. The driver is responsible for updating data in the HOST informing it that new work is available.</p>
<p>The host will time-slice between all channels with work available.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_7_1_1_1"></a>
7.1.1.1. Host data scheduling</h4>
<p>Figure 12 describes the hierarchy of work and how its scheduled by host. At the top most level is the runlist, which references all work to be executed by the GPU. This work is broken up into Time Slice Groups (TSG). Each TSG is independent work with its own address space, consisting of multiple channels. The Scheduler implements a round robin, timeslice based scheduling algorithm to switch between TSGs. Channels within a TSG share an address space, but execute independently and are cooperatively multi-tasked. Within a TSG channels switch when they run out of work, or fail a synchronization check. Synchronization between channels (either in the same TSG or across TSGs) can be accomplished using Semaphores, or additionally on NX, syncpoints. The work within a channel is dispatched in order. Each channel has a GPFIFO circular buffer in memory, where work can be added dynamically and is consumed by the HW. Work is added by writing a new entry then updating the head pointer. The entries in the GPFIFO are pointers and length fields referencing pushbuffer segments, records in memory containing commands and inline data. Pushbuffer segments must be completely written to memory before adding the reference to the pushbuffer to the GPFIFO buffer.</p>
<div class="image">
<img src="MaxwellTechnicalOverview_img13.jpg" alt="MaxwellTechnicalOverview_img13.jpg"/>
</div>
 <div style="text-align: center;" markdown="1"> <b>Figure 12: Host comand/data hierarchy</b></div><p> <br />
</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_7_1_2"></a>
7.1.2. Synchronization</h3>
<p>On discrete GPUs, synchronization between channels and between CPU and GPU are done through Semaphores. On NX, Syncpoints can also be used to synchronize between channels on Host and Host1x.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_7_1_2_1"></a>
7.1.2.1. Semaphores</h4>
<p>Semaphores are locations in memory used for synchronization. The CPU, the Host and the graphics engine can write semaphores with given values, and Host can poll these locations waiting for specific values to be read, and then schedule work. This polling is called a "Semaphore acquire", while writing a semaphore is a semaphore release.</p>
<p>The Host unit can generate a non-stalling interrupt after releasing a semaphore to inform the CPU a semaphore has been updated.</p>
<p>Host can also trigger an interrupt, which can be used to notify the CPU to check Semaphore/syncpoint locations. This can reduce the performance and power overhead of polling till a Semaphore is updated.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_7_1_2_2"></a>
7.1.2.2. Syncpoints</h4>
<p>Furthermore, NX's Host supports on-chip syncpoints, which allows for synchronization without writing or polling memory. Syncpoints can be used for GPU Host-GPU Host synchronization, host-cpu synchronization, and host-host1x synchronization.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_7_1_3"></a>
7.1.3. Channel Scheduling</h3>
<p>Commands can be organized into groups of channels or subchannels, to send to the GPU. Channels run independently (outside of explicit synchronization), and require saving and restoring contexts when switching channels. Multiple channels can use the same GPU pipeline. Each channel can be broken into multiple subchannels, which are a group of commands targeted at a specific GPU pipeline, run synchronously, and do not require context switching when changing subchannels. The rest of this section describes how channels and subchannels work in more detail.</p>
<p>Channels are scheduled on Host using a runlist. Channels are put into groups which share a context, and a timeslice called a timeslice group (TSG). Host will rotate through the runlist in a round robin fashion, scheduling the work available for the TSGs. If no work is left for the TSG, or if all channels in the TSG are waiting on a synchronization event with the appropriate hint flag set, or if the timeslice for that TSG expires, the Host will move to the next TSG on the runlist. When it reaches the end of the runlist it wraps back to the beginning. Within a TSG Host moves between channels as the channels fail synchronization events.</p>
<p>The Host initiates a context switch on the engine when switching between TSGs, but only if a channel in the TSG has work for the engine. No context switch will occur if all channels in the TSG are blocked on synchronization events. A context switch requires the engine to finish pending work and drain before the context save and new context restore.</p>
<h2><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_7_2"></a>
7.2. PROGRAMMING STATE</h2>
<p>Data in the command stream out of host consists of Methods, immediate data, pointers to data and state. This section documents the various forms of state in the Maxwell architecture.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_7_2_1"></a>
7.2.1. Method Stream</h3>
<p>The command stream out of Host can update state using methods. Methods are used to generate state bundles in FE, and FE also checks to ensure the current state of the machine is valid, and forces an error when invalid state is programmed.</p>
<p>Methods can either update state or also cause actions to be performed. For instance, a state method can configure the blending state used by CROP, and then a subsequent Draw method can cause a list of primitives to be rendered using the current pipeline's state.</p>
<p>FE also supports software methods. These are methods that either trigger operations on a local, small embedded processor or interrupt the CPU to allow operations to be performed.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_7_2_2"></a>
7.2.2. Constant Buffers</h3>
<p>Constant buffers are used by all shader stages, and are read-only surfaces that are optimized for uniform access across threads. Each graphics shader can reference up to 18 constant buffers, some of the buffers may be reserved by the driver. Compute shaders can access 8. Each constant buffer can be up to 64 KBytes deep.</p>
<p>In the graphics pipeline, Constant buffers can be updated as part of the command stream, or can be static surfaces in memory. Maxwell allows up to 128 versions, or sets, of constant buffers to be in flight at any time for graphics. For Compute, each grid can reference its own constant buffers, but in-band updates are not supported.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_7_2_3"></a>
7.2.3. Texture Headers</h3>
<p>There are two types of texture headers: surface headers, that define the format and the layout of the surfaces, and sampler headers, which define the filtering mode.</p>
<p>Bindless textures in Maxwell allow a shader to access up to 1M sets of surface and sampler headers.</p>
<p>Textures and samplers are identified in shader code with an integer index into the pool. Texture lookups in hardware can be configured in one of two modes:</p>
<ul>
<li>use the same index for both header and sampler</li>
<li>use 20-bit index for texture and 12-bit index for sampler, packed into one 32-bit word</li>
</ul>
<p>For the separate mode, indices have to be combined in the driver/application or shader code.</p>
<h2><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_programming_DATA"></a>
7.3. DATA</h2>
<p>There's many forms of data in the GPU pipeline. The following are used in graphics mode.</p>
<ul>
<li>Index buffers. These are pointers to vertices that define primitives. Primitives can be points, lines, triangles, screen-aligned rectangles, or patches. Index buffers are fetched by PD.</li>
<li>Vertex arrays. Multiple vertex arrays can be used, vertex are arrays of structures defining vertices, fetched by PE. Indices from PD and state allow referencing specific vertex data in multiple vertex arrays.</li>
<li>Texture Surfaces. Read-only surfaces read by texture.</li>
<li>Unordered access views (image load/store). Can be accessed by any shader. Ordering of read/writes are not guaranteed between threads.</li>
<li>Thread local memory: this is memory seen only by the current thread.</li>
<li>Render Target surfaces. Surfaces read and written by ROP. Can be displayed or used as textures.</li>
</ul>
<p>Compute had three forms of data:</p>
<ul>
<li>Thread local memory: this is memory seen only by the current thread.</li>
<li>Shared memory. A CTA or thread block can read/write shared memory to communicate across threads. Shared memory is limited to 48 KBytes per thread block, and 64-96 KBytes of shared memory exists on each SM (64 Kbytes on NX, 96 Kbytes on GeForce).</li>
<li>Global Memory. Used to communicate between grids and for CPU to GPU communication.</li>
</ul>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_7_3_1"></a>
7.3.1. Enhanced Memory Operations</h3>
<p>Along with Loads and Stores to different memory locations, there are the following memory operations:</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_7_3_1_1"></a>
7.3.1.1. Atomics and Reductions</h4>
<p>Both Atomics and Reductions are memory operation instructions that do atomic read modify writes of memory. The SM issues the instruction, and the L2 performs the read-modify write, without allowing other requests to the same address to happen between the read and write.</p>
<p>Reduction operations perform the read-modify-write using source data from the SM, without returning any data to the SM. Atomic operations do the read modify write using source data, and return data back to the SM.</p>
<p>Atomics and reductions can be performed to local, global and shared memory.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_7_3_1_2"></a>
7.3.1.2. Surface LD/ST</h4>
<p>Surface Load and Store instructions are also supported, where the SM provides a surface identifier and coordinate address to the LD/ST pipe, and the global memory address is calculated and the operation is performed based on surface header state. There are two types of surface operations: one that returns raw data to the SM, the other that format converts the data as part of the operation.</p>
<p>Surface operations enable direct access to block linear surfaces without substantial address calculation overhead, and perform bounds checking on all accesses.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_7_3_1_3"></a>
7.3.1.3. Texture LDs</h4>
<p>Texture Load operations behave like surface LDs, except there's no read/write ordering guarantees, and texture read data is cached.</p>
<h2><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_7_4"></a>
7.4. MEMORY MANAGEMENT</h2>
<p>This section describes the memory management subsystems in the GPU and in the NX SOC.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_7_4_1"></a>
7.4.1. Graphic MMU, or GMMU</h3>
<p>The GPU has its own memory management unit, logically before accesses to the L2. This MMU performs address translation, and defines the surface type and format, and indicates the compression algorithm to use on a given page. It supports 128, 64, and 4 KBytes pages, although compression is not supported on 4 KBytes pages. A virtual address (VA) may map to system memory or video memory.</p>
<p>If a given page isn't resident, one of two behaviors can happen: A fatal fault can be generated, or for sparse resources (a post-dx11 feature), a read fault returns default data to the requests and flags a fault has happened, allowing forward progress. Write faults flag a fault happened and throw away the write data.</p>
<p>The GMMU has multiple levels of hierarchy, with multiple L0 and L1 TLBs and one L2 TLB. The GMMU sits between the requesting units and the L2 cache, supporting simultaneous address translations for all requests to the L2.</p>
<p>GMMU supports 40-bit virtual address. When GMMU is working together with SMMU in NX, the upper physical address bits above 34 is assumed to be 0s as NX supports 34-bit PA.</p>
<p>GMMU supports 2 page sizes at the same time for a context in GPU. One small, 4 KBytes, page size and one big, 64 KBytes or 128 KBytes, page size. Which big page size will be used is set during boot time.</p>
<p>Big page table coverage (alternately, page directory entry) is 64 MBytes when using 64 KBytes big page or 128 MBytes coverage when using 128 KBytes page size. Small page table has same coverage as big page table.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_7_4_2"></a>
7.4.2. NX System MMU, or SMMU</h3>
<p>NX also has a system MMU, or SMMU, that all non-CPU memory accesses go through. It's optimized for 4 KBytes pages, allowing the GPU to use os-compatible memory allocation with compression. This MMU is between the L2 and the FB, in the memory controller.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_7_4_3"></a>
7.4.3. Surface Addressing</h3>
<p>Two general methods for addressing surfaces are available: pitch and block linear.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_7_4_3_1"></a>
7.4.3.1. Pitch Addressing</h4>
<p>Pitch addressing uses the familiar pitch-linear mapping, in which pixels are assigned incrementing addresses across each successive row of the image. The surface address is calculated by:</p>
<div style="text-align: center;" markdown="1"> <em>pixel_byte_address = surface_offset + pitch*pixel_y + bytes_per_pixel*pixel_x</em> </div><p>Although pitch addressing is standard across almost every hardware and software platform, it limits performance because it has poor memory locality and does not support compression. Locality is poor because each line in a surface image generally maps to a different DRAM page. Compression requires a format with 2D locality.</p>
<p>Pitch buffers must be aligned to a multiple of 64 Bytes and the pitch must be a multiple of 64 Bytes. Pitch buffers don't support compression or depth buffers.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_7_4_3_2"></a>
7.4.3.2. Block Linear Addressing</h4>
<p>Block linear addressing is similar to pitch addressing, but maps blocks of pixels using pitch-linear addressing, rather than individual pixels. The blocks provide spatial locality for addressing. The address of a block can be calculated by:</p>
<div style="text-align: center;" markdown="1"> <em>block_byte_ address = surface_offset + block_pitch*block_y + bytes_per_block*block_x</em> </div><p>Here <em>block_x</em> and <em>block_y</em> refer to the <em>x</em> and <em>y</em> values of a block in the surface. A block (also referred to as a GOB) is 512 bytes and matches the allocation alignment restrictions for block linear surfaces. The pixel address is the block_address plus the offset of the pixel within the GOB. Compression takes advantage of the 2D locality provided by the block linear mapping. The following section provides more detail on the block linear mapping, since it is a bit more complex and is the mapping of choice for high-performance render and texture surfaces.</p>
<p>The purpose of block linear addressing is to provide 2D address locality within square (or approximately square) blocks of pixels. Square regions give best DRAM row-bank locality as we rasterize and display (which can be traversed in either the horizontal or vertical direction).</p>
<p>Figure 13 shows the block linear mapping pictorially. A surface is divided vertically into rows of blocks. Each row is one block tall and an integral number of blocks wide. The image width can be arbitrary, but the surface width is rounded up to the next multiple of the block width.</p>
<div class="image">
<img src="MaxwellTechnicalOverview_img10.jpg" alt="MaxwellTechnicalOverview_img10.jpg"/>
</div>
 <div style="text-align: center;" markdown="1"> <b>Figure 13: Block linear mapping</b> </div><p> <br />
</p>
<p>Block linear buffers must be aligned to a 512 Bytes for functional correctness. Large buffers should be aligned to 8 KBytes for optimal performance. (4 KBytes on NX)</p>
<p>Memory can transition from being used for pitch or block linear buffers on 4 KBytes page boundaries.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_7_4_4"></a>
7.4.4. Multisample Textures</h3>
<p>Multisample textures are arranged in memory in a manner identical to single-sample textures, where each pixel in the multisample texture is treated as a block of samples, where the samples of each pixel are arranged in a rectangular pattern. A multisample texture with a size of w x h pixels is stored identically to:</p>
<ul>
<li>a 2w x h texture, if the sample count is 2</li>
<li>a 2w x 2h texture, if the sample count is 4</li>
<li>a 4w x 2h texture, if the sample count is 8</li>
<li>a 4w x 4h texture, if the sample count is 16</li>
</ul>
<p>NVN currently does not support image loads and stores accessing multisample textures. Applications can work around this limitation by creating an equivalent single-sample texture using the same memory and loading/storing using this single-sample texture. Within each pixel, the samples are stored in pitch order. For example, sample <em>s</em> at pixel (<em>x</em>, <em>y</em>) in a 4x multisample texture will be found at pixel (2x+(s&amp;1), 2y+((s&gt;&gt;1)&amp;1)) in an equivalent single-sample texture.</p>
<h1><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8"></a>
8. PERFORMANCE CHARACTERISTICS</h1>
<p>This section describes in detail how Maxwell's capabilities perform.</p>
<h2><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_performance"></a>
8.1. PERFORMANCE SUMMARY</h2>
<p>The following table lists the performance Characteristics of Maxwell's Units. Note these tables don't list clock frequency, since clocks can change based on product SKUs, and power envelopes.</p>
<table class="doxtable">
<tr>
<th>Item </th><th>Value </th><th>Units  </th></tr>
<tr>
<td>SM FMAs(cores) </td><td>128 </td><td>Per SM </td></tr>
<tr>
<td>Shared Memory </td><td>64 </td><td>KBytes per SM </td></tr>
<tr>
<td>Vertex throughput (PES) </td><td>.88 </td><td>Vertices/clk/PES </td></tr>
<tr>
<td>Attribute throughput (PES) </td><td>8 </td><td>Attributes/clk/PES </td></tr>
<tr>
<td>Primitive throughput (PESs culled) </td><td>1 </td><td>Prim/clk/PEs </td></tr>
<tr>
<td>Primitive throughput, Setup (not culled by PES) </td><td>.75 </td><td>Prim/clk/GPC </td></tr>
<tr>
<td>ZCull </td><td>256 </td><td>Pixels/clk/GPC </td></tr>
<tr>
<td>Fine Raster </td><td>64 </td><td>Samples/clk/GPC </td></tr>
<tr>
<td>Texture throughput </td><td>8 </td><td>Bilerps/clk per TPC, 32 bit and 64 bit texels </td></tr>
<tr>
<td>PROP </td><td>14.4 </td><td>Pixels/clk/GPC </td></tr>
<tr>
<td>ZROP </td><td>64 </td><td>Compressed Z Samples/clk/ZROP(1) </td></tr>
<tr>
<td>ZROP </td><td>32 </td><td>Uncompressed Stencil Samples/clk/ZROP(1) </td></tr>
<tr>
<td>ZROP </td><td>64 </td><td>Compressed Stencil Samples/clk/ZROP, Stencil-only surfaces(1) </td></tr>
<tr>
<td>CROP </td><td>8 </td><td>Pixels/clk/CROP, 32 bit(2) </td></tr>
<tr>
<td>L2 throughput </td><td>32 </td><td>Bytes/clk/slice write </td></tr>
<tr>
<td>L2 throughput (cache hits) </td><td>32 </td><td>Bytes/clk/slice read </td></tr>
</table>
<p><b>Table 1: Per-unit capabilities</b></p>
<p>(1) The total rate of ZROP is limited by the Fine Raster unit throughput.<br />
 (2) The total rate of CROP is limited by the PROP unit throughput.<br />
</p>
<p>The Following Table Lists the # of Units in NX</p>
<table class="doxtable">
<tr>
<th>Item </th><th>NX  </th></tr>
<tr>
<td>SMs &amp; TPCs </td><td>2 </td></tr>
<tr>
<td>GPCs </td><td>1 </td></tr>
<tr>
<td>PESs </td><td>1 </td></tr>
<tr>
<td>ROPs (ZROP + CROP) </td><td>2 </td></tr>
<tr>
<td>L2 Slices </td><td>2 </td></tr>
<tr>
<td>DRAM Width (bits) </td><td>64 </td></tr>
<tr>
<td>DRAM Type </td><td>LPDDR4 </td></tr>
</table>
<p><b>Table 2: Unit count per product</b></p>
<p>The following table lists overall throughput and capacity for NX. Clock rates are deliberately not listed, since they can change from product to product and can change depending on the power envelope and other conditions.</p>
<table class="doxtable">
<tr>
<th>Item </th><th>NX  </th></tr>
<tr>
<td>SM FMAs/clk (fp32) </td><td>256 </td></tr>
<tr>
<td>SM FMAs/clk (fp16)(1) </td><td>512 </td></tr>
<tr>
<td>SM attribute interpolations/clk </td><td>64 </td></tr>
<tr>
<td>Attribute throughput, 4 wide vector/clk </td><td>2 </td></tr>
<tr>
<td>Primitive throughput, PESs culled/clk </td><td>1 </td></tr>
<tr>
<td>Primitive throughput, Setup/clk </td><td>1 </td></tr>
<tr>
<td>ZCull, pix/clk </td><td>256 </td></tr>
<tr>
<td>ZCull capacity, Megapixels </td><td>0.5-4 </td></tr>
<tr>
<td>Fine Raster, samples/clk </td><td>64 </td></tr>
<tr>
<td>Texture throughput, bilerps/clk </td><td>16 </td></tr>
<tr>
<td>L1 capacity, per texture pipe, KBytes </td><td>12 </td></tr>
<tr>
<td>Shared Memory Capacity, per SM, KBytes </td><td>64 </td></tr>
<tr>
<td>PROP, pixels/clk </td><td>14.4 </td></tr>
<tr>
<td>ZROP, samples/clk (2) </td><td>128 </td></tr>
<tr>
<td>CROP, pixels/clk (3) </td><td>16 </td></tr>
<tr>
<td>L2 read throughput, bytes/clk </td><td>64 </td></tr>
<tr>
<td>L2 write throughput, bytes/clk </td><td>64 </td></tr>
<tr>
<td>L2 Capacity, KBytes </td><td>256 </td></tr>
<tr>
<td>L2 Compressed Bit Cache (CBC) footprint, MB </td><td>58 </td></tr>
</table>
<p><b>Table 3: Overall product peak perf and capacity</b></p>
<p>(1) Emulated using FP32 on discrete GPU <br />
 (2) Limited to a lower number by fine raster samples/clk <br />
 (3) Limited to a lower number by PROP pixels/clk <br />
</p>
<h2><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_2"></a>
8.2. SM</h2>
<p>This section describes the SM's position in the overall graphics pipeline, the SM's execution model and HW organization, and details of SW-visible SM resources.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_2_1"></a>
8.2.1. SM and Its Neighbors</h3>
<p>The SM is the GPU's programmable processor core. Shader programs can be one of several types: vertex shader (VS), tessellation initialization or tessellation control shader (TCS, also known as hull shader - HS), tessellation evaluation or domain shader (TES or DS), geometry shader (GS), pixel shader (PS), and compute shader (CS). Shader programs are launched on the SM in units of at most 32 threads called warps.</p>
<p>Figure 14 shows how shader work flows into and out of the SM. The GPM (Graphics Program Manager) unit serves work to the Texel Processing Clusters (TPCs) in The GPC.</p>
<div class="image">
<img src="MaxwellTechnicalOverview_img17.jpg" alt="MaxwellTechnicalOverview_img17.jpg"/>
</div>
 <div style="text-align: center;" markdown="1"> <b>Figure 14: SM and Its Neighbors</b> </div><p> <br />
</p>
<p>Each TPC comprises an SM, two texture units (Tex), a primitive engine (PE), and a resource management unit called the module pipe controller (MPC).</p>
<p>Incoming work from the GPM heads to the module pipe controller (MPC) unit of each TPC. MPCs manage all the resources needed to launch warps for all shader types on the respective TPCs. Key SM resources that the MPC manages include:</p>
<ol type="1">
<li>Inter-stage buffer elements (ISBEs) in a shared memory to hold attributes between world-space shader stages (VS/TCS/TES/GS)</li>
<li>Register file for all shader types</li>
<li>Triangle RAM (TRAM)(1) entries in shared memory for PS launches</li>
<li>Shared memory and barriers for CS launches</li>
</ol>
<p>Depending on the shader type, one or more warps are often launched and retired together as part of a single larger atomic unit. Pixel shader warps are launched as part of a larger group called subtiles. The size of a subtile is software-configurable and primarily serves as a performance knob. Larger subtiles can help improve cache locality for texture, color, and depth traffic, whereas smaller subtiles can help cut down the time spent synchronizing across warps of a subtile at launch, during execution, and at retirement. Likewise, compute shader warps are launched and retired as part of larger groups called co-operative thread arrays (CTAs). The size of CTAs directly map to API-level constructs and cannot be changed except at the application level.</p>
<p>When warps complete in the SM, MPC retires their resources and makes them available for new warp launches. For world-space (primitive-processing) shaders, MPC works with PE to make sure vertex attribute values written into ISBEs are copied back into the backing store in the L2 cache. For PS, it works with GPM to ensure pixel output values have been drained from the SM before reusing relevant registers for newer warp launches. For compute shaders, MPC communicates to a central control unit called Compute Work Distributer (CWD) that the relevant TPC is ready to accept new compute CTAs.</p>
<p>The GPC Cache Controller (GCC) unit has a "level 1.5" cache to hold instructions and constant data, backing smaller L1 instruction and constant caches within the SM itself. The L1.5 cache is in turn backed up by the main L2 cache. This unit also plays an important role in managing constant buffer versions.</p>
<p>(1) TRAM holds primitive attribute information as plane equations. These equations are used by interpolation operations of pixel shaders to derive attribute values at individual pixel positions.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_2_2"></a>
8.2.2. SM Internals</h3>
<p>This section provides an introduction to the SM's internal organization and some of its unique features.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_2_2_1"></a>
8.2.2.1. Micro-architectural Organization</h4>
<p>Figure 15 gives a block diagram of the SM. Each SM comprises two partitions. A partition, in turn, contains two sub-partitions, shown as dark gray boxes in the middle. Instructions are fetched from an L1 instruction cache (ICC) that is shared among both partitions. They are then scheduled and dispatched to an array of execution units, by control logic that is local to each sub-partition. All shader program types are processed in this fashion.</p>
<p>While some function units are private to each sub-partition and have fixed pipeline latency, other function units (those handling less common or long latency operations) are shared among two or more sub-partitions and/or have variable latency. The former are called coupled units and the latter are referred to as decoupled units. Register dependencies involving coupled producers can be determined statically and are handled using simple software-controlled cycle counters. Register dependencies involving decoupled producers are handled via a software-controlled score boarding mechanism.</p>
<div class="image">
<img src="MaxwellTechnicalOverview_img18.jpg" alt="MaxwellTechnicalOverview_img18.jpg"/>
</div>
 <div style="text-align: center;" markdown="1"> <b>Figure 15: SM Block Diagram</b> </div><p> <br />
</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_2_2_2"></a>
8.2.2.2. SIMT Execution Model</h4>
<p>The SM implements an execution model called Single Instruction Multiple Threads (SIMT), which allows individual threads to have their own unique control flow, while still executing as part of a warp. The SM derives a lot of its efficiency from having a narrow front-end (which fetches and schedules once per warp-wide instruction) and a wide execution back-end (where individual threads of a warp are able to fetch operands, execute, and writeback in a bulk fashion). The SM is most efficient when all threads in a warp execute at the same program counter (PC) and is least efficient when each thread has a PC that is completely different from the other threads of a warp.</p>
<p>In order to maintain high utilization of the back-end function units, the SM implements a joint HW-SW mechanism to minimize control-flow divergence. This mechanism works as follows: the HW maintains a single active mask for a warp at all times. At potential divergence points, the compiler introduces a set sync (SSY) instruction, which tells the HW to push a warp's active mask (a 32 bit mask of active threads that execute at the same PC) along with the convergence address on a HW stack. Divergent threads will get a reduced active mask, depending on the actual threads taking a particular divergent path. At the end of each divergent path, the compiler introduces a SYNC instruction, which causes the executing group of threads to branch to and wait at the convergence point until all threads comprising the original pre-divergence active mask have arrived at the convergence point. At that point convergent execution resumes, leading to maximal function unit utilization. The compiler can use dominance/post-dominance analyses to determine where to place the SSY and SYNC ops to guarantee proper code generation for both functionality and performance. In the example in Figure 16, execution diverges at block A and converges at block D. Block A has the SSY op and blocks B and C each have a SYNC op to ensure re-convergence.</p>
<div class="image">
<img src="MaxwellTechnicalOverview_img19.jpg" alt="MaxwellTechnicalOverview_img19.jpg"/>
</div>
 <div style="text-align: center;" markdown="1"> <b>Figure 16: Intra-warp control flow re-convergence</b> </div><p> <br />
</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_2_2_3"></a>
8.2.2.3. Instruction Set and Assembly</h4>
<p>The SM implements a rich RISC instruction set architecture (ISA), with predicate and condition code (CC) support. Instruction operands can be registers, constant bank entries, or immediates. The ISA supports single and half-precision floating point, integer arithmetic, logical, shifting and bit-manipulation, transcendental and interpolation operations, in addition to loads and stores to various types of memory.</p>
<p>Most high-level assembly (e.g. GLASM) instructions have a one-to-one mapping in the SM's ISA, leading to efficient translation. While math operations write to a single destination register (scalar form), all memory access operations (including texture lookups) allow for multiple destination registers to be returned to the SM or written to memory (vector form). On NX, half-precision math operations are allowed to use a special vector form, known as SIMD fp16, whereby such ops write two fp16 results, one each to the upper and lower halves of the same 32 bit register. The SM allows free interleaving of scalar, SIMD fp16, and vector operations in the instruction stream and does not restrict their placement in any fashion.</p>
<p>The typical format of an assembly instruction is:</p>
<p>@{!}Pg Opcode Rd, Ra, Sb {, Sc}</p>
<ul>
<li>Pg – guard predicate</li>
<li>Rd - destination register</li>
<li>Ra - source register</li>
<li>Sb - source register, constant bank entry or immediate</li>
<li>Sc - source register, constant bank entry or immedate</li>
</ul>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_2_2_4"></a>
8.2.2.4. Memory Model</h4>
<p>The memory model of the SM has 3 distinct regions: local, shared, and global. Local memory is primarily intended for thread-local data like thread stacks and registers spills, shared memory for data-sharing among threads of compute CTAs, and global memory for general data and communication across SMs globally for all shader types. Local and global memory locations reside in DRAM and can be cached on chip as well. Compute CTAs attempting to share data across threads via shared memory must use synchronization (BAR.SYNC) and/or memory barriers (MEMBAR.CTA) between stores and loads to ensure data written by any one thread is visible to other threads in the CTA. Similarly, threads that need to share data via global memory must use a more heavyweight barrier (MEMBAR.GL) to ensure stores are visible globally.</p>
<p>Issuing a Membar causes a stall until the ordering point is reached by all threads. MEMBAR.CTA traverses through the store path, through the L1, before reaching a point where memory ordering can be guaranteed between both partitions of an SM. MEMBAR.GL causes a longer stall, since its has to ensure memory ordering can be guaranteed across all SMs.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_2_2_5"></a>
8.2.2.5. Compute-Specific Features</h4>
<p>In addition to being able to use resources like registers, local, and global memory like other shader types, On NX compute shaders have access to a dedicated 64 KBytes shared memory scratchpad and 17 barrier registers to support co-operative updates to shared memory data structures. In addition to supporting shared memory loads and stores (LDS and STS ops respectively), the SM also supports several flavors of atomic operations on shared memory locations (ATOMS op in the ISA), enabling a wide variety of compute algorithms to be mapped effectively to the SM.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_2_3"></a>
8.2.3. Software View of the SM</h3>
<p>An individual warp of any shader type will reside in a single sub-partition throughout its lifetime. Thus, from a software scheduling and resource allocation point of view, the function units and resource constraints of a single sub-partition will determine shader program performance.</p>
<p>Warps of a PS subtile are spread across 2 sub-partitions of a partition. Warps of a CS CTA can be steered to two partitions that share an L1, or can be spread across all 4 sub-partitions in an SM.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_2_3_1"></a>
8.2.3.1. Function Units</h4>
<p>The SM’s function units support a rich variety of operations as first class instructions. The following table gives the latencies of various operation types and their steady-state throughput.</p>
<table class="doxtable">
<tr>
<th>Op Types </th><th>Variable Latency ? </th><th>Latency </th><th>Threads/clk  </th></tr>
<tr>
<td>FFMA, FADD, FMUL,IADD,<br />
 LOP, MOV, XMAD,<br />
VMAD,"H*2"(1) </td><td>No </td><td>6 </td><td>128 </td></tr>
<tr>
<td>Less common FP and Integer </td><td>No </td><td>6 </td><td>64 </td></tr>
<tr>
<td>Transcendentals, Conversions </td><td>Yes </td><td>13+ </td><td>32 </td></tr>
<tr>
<td>Interpolation </td><td>Yes </td><td>32+ </td><td>32 </td></tr>
<tr>
<td>Shared and atttribute memory ops </td><td>Yes </td><td>24+ </td><td>32 </td></tr>
<tr>
<td>Control flow </td><td>Yes </td><td>5+ </td><td>32 </td></tr>
<tr>
<td>Texture ops </td><td>Yes </td><td>80+ </td><td>8 </td></tr>
<tr>
<td>Global loads/stores </td><td>Yes </td><td>80+ </td><td>16 </td></tr>
</table>
<p><b>Table 7: Latency and Throughput Rates for Instructions</b></p>
<p>(1)H*2 are 2-ways simd instructions and include HADD2, HFMA2, HMUL2, HSET2, HSETP2</p>
<p>FP and integer operations are fixed latency due to each SM sub-partition having a private copy of the relevant function units (coupled). These instructions take an additional 7 cycles to write to any predicate or CC destination registers. Other, less common operations like transcendentals, conversions, etc. have variable latency due to multiple sub-partitions sharing a single copy of such function units (decoupled), which can cause variable queueing delays to be added to actual function unit latencies.</p>
<p>Instruction issue is in program order. Unlike modern CPUs which have out-of-order instruction issue, the onus of extracting instruction level parallelism (ILP) is on the GPU compiler, which needs to schedule instructions appropriately to minimize shader warp latency. Data hazards can be introduced due to either coupled or decoupled operations. The compiler can minimize data hazards by scheduling dependent operations sufficiently far apart. In addition to modeling latencies for flow dependencies or read-after-write (RAW) hazards based on the above latency table, the compiler must also be aware of write-after-read (WAR) hazards introduced by anti-dependencies originating at decoupled operations. If a decoupled operation reads N source registers, the compiler should try to avoid scheduling an op that writes to any of those source registers for at least 8 + (N-1) x 2 cycles.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_2_3_2"></a>
8.2.3.2. Shared Resources</h4>
<p>Unlike function units which become ready to accept new instructions from any warp every few cycles (as determined by their repeat rates), some of the SM's resources are partitioned among inflight warps (e.g. register file, warpids, etc.) for the duration of their individual lifetimes. For such resources, a single warp's resource usage directly impacts occupancy, how many warps can run in parallel. Thus, in addition to minimizing shader latency, the compiler has to simultaneously manage a shader's resource requirements (e.g. register file), to strike the right tradeoff between warp latency and warp occupancy. Warp occupancy is also quantized according to the subtile or CTA size.</p>
<p>The Figure below illustrates how warp occupancy is impacted due to an abstract resource. The shader in green gets 2 warps in flight, while the shader in dark blue is able to get 4 warps in flight due its reduced resource usage. The shader in light blue uses only slightly more of this abstract resource compared to the dark blue shader, but suffers a 25% reduction in warp occupancy (4 warps reduced to 3). Notice that due to an odd-sized resource demand, the third shader leaves a non-trivial portion of the total resources un-utilized, but does not leave enough room for a 4th warp to be launched.</p>
<div class="image">
<img src="MaxwellTechnicalOverview_img20.jpg" alt="MaxwellTechnicalOverview_img20.jpg"/>
</div>
 <div style="text-align: center;" markdown="1"> <b>Figure 17: Shared resources determining SM warp occupancy</b> </div><p> <br />
</p>
<p><b>Register File (RF)</b></p>
<p>The most important resource under the compiler's control is the number of registers used by a shader program. Each sub-partition has 512 warp-wide registers, allocated by the HW in chunks of 8 dynamically. The expression to calculate warp count from a register usage is given by the following equation:</p>
<div class="image">
<img src="MaxwellTechnicalOverview_img21.jpg" alt="MaxwellTechnicalOverview_img21.jpg"/>
</div>
<p>The 'roundUpToMultipleOf8(regs)' term in the denominator is to handle the HW's allocation granularity of 8 registers. This also implies that the compiler should always prefer a multiple of 8 as a register target. Figure 18 gives the various register target sweetspots and their corresponding warp counts. Points on the x-axis represent each of the register target sweetspots and the y-axis gives the corresponding register target and warp count.</p>
<div class="image">
<img src="MaxwellTechnicalOverview_img22.jpg" alt="MaxwellTechnicalOverview_img22.jpg"/>
</div>
 <div style="text-align: center;" markdown="1"> <b>Figure 18: Register Target Sweet Spots</b> </div><p> <br />
</p>
<p><b>Triangle RAM (TRAM)</b></p>
<p>The number of TRAM entries in use at any given point is directly proportional to the number of overlapping prims (more correctly, the number of primitive changes in the rasterized stream of pixel quads arriving at MPC) and the number of attributes used in the corresponding pixel shader. While SW cannot do much to reduce attribute requirements of a pixel shader, this information may be useful to the application developer who wants to improve the performance of TRAM-limited portions of a frame. On NX the TRAM is sized at 16 KBytes and is stored in shared memory (the rest used for ISBEs).</p>
<p><b>Inter-stage Buffer Elements (ISBEs)</b></p>
<p>ISBE is used to convey attribute values between world-space shader stages. On NX Available ISBE space is 48 KBytes and is stored in shared memory. ISBE space will limit performance in primitive-heavy buckets that use lots of attributes.</p>
<p><b>Shared Memory</b></p>
<p>A total of 64 KBytes of shared memory is available for use by compute shader CTAs. The number of inflight CTAs is inversely proportional to the amount of shared memory used by shaders. Thus, in addition to RF, compute warp parallelism is limited by shared memory resource usage as well.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_2_3_3"></a>
8.2.3.3. Local, Global, and Surface Memory Accesses</h4>
<p>All shader types can access up to 16 MBytes of local memory per thread, which can be used to hold app-created local arrays as well as register spills that the compiler generates. Local memory resides in DRAM and gets cached at the TEXL1 cache as well as the L2. Local memory locations are accessed with load-local (LDL) and store-local (STL) assembly instructions. Local memory accesses that hit in the TEXL1 cache can take at least 80 cycles to return, and those that go to L2 and beyond can take more than 200 cycles to return to the SM. Given the high cost of spills, the compiler/asm programmer needs to be very cautious when it comes to generating spills.</p>
<p>The most general form of memory is global memory which is available to all shader types and is globally visible across the GPU. Global memory allocations reside in DRAM and are cached at all levels (TEXL1 and L2). The GPU also supports allocation of global memory as block-linear surfaces (e.g. 2D, 2D_ARRAY, 3D). Such surfaces provide a very cache-friendly layout of data such that neighboring points on a surface (across dimensions) are located close to each other in memory layout as well. This improves locality of accesses. Surface accesses also get bounds-checked prior to accessing memory, which can be convenient for certain programming idioms.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_performance_FP16"></a>
8.2.3.4. FP16 support</h4>
<p>On NX, The SM supports half-precision floating point (fp16) math operations to execute as either singletons or as SIMD pairs. Singleton fp16 operations are able to read fp16 operands from either half of a 32 bit register, or read fp32 operands and down-convert prior to processing. Similarly, they are able to write fp16 output to either half of a 32 bit register, or fp32 output after up-conversion.</p>
<p>SIMD fp16 pairs combine two half-precision (16 bit) floating point math operations (of the same opcode) into a single instruction that reads from one or both halves of 32 bit source registers and writes to both halves of a single 32 bit destination register.</p>
<p>Only floating math operations have fp16 equivalents. Instructions like interpolations, transcendentals, or memory access operations do not support fp16. Thus, any dependencies originating at or ending at these operations will need to make sure relevant operands are 32 bit. Pixel outputs need to be 32 bit. Likewise, constant banks can be accessed only as 32 bit entities. For all these conversions, SW can use operand modifiers available in half-precision ops to perform inline conversions.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_2_3_5"></a>
8.2.3.5. Warp-specific instructions</h4>
<p>There are also two warp-specific instructions: VOTE and SHFL. VOTE allows each thread in a warp to set a binary value that is visible to all threads in the warp. SHFL allows data to be passed between threads in a warp without using shared memory. While these instructions aren't visible through industry standard APIs, these two instructions are very useful for coalescing atomic operations across a warp, greatly increasing atomic throughput.</p>
<h2><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_3"></a>
8.3. CACHING SUBSYSTEM</h2>
<p>This section discusses the behavior of the major caches in the GPU: the TLBs in the MMU, the L1 and L2 caches.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_3_1"></a>
8.3.1. L1 behavior</h3>
<p>The L1 data cache is implemented through the Texture unit. In addition to caching Texture memory data and performing level-of-detail and filtering operations, the Texture unit also handles accesses for Global memory, Thread Local memory, and Surface memory. These data types and associated memory operations are described in <a class="el" href="gpu_overview__maxwell_technical_overview_index.html#gpuOverview_MaxwellTechnicalOverview_guide_programming_DATA">7.3. DATA</a>. The performance of traditional texturing operations is described in <a class="el" href="gpu_overview__maxwell_technical_overview_index.html#gpuOverview_MaxwellTechnicalOverview_guide_perf_TEXTURE">8.8. TEXTURE</a>. This section is for “L1 behavior” for global, thread local, and surface behaviors in order to distinguish from the traditional texturing operations. While this section often uses the name “L1”, it should be understood that the L1 data cache and the Texture data cache are one and the same.</p>
<p>The L1 returns data in request order. This means that hits will have the same latency as misses, if there is a small number of misses in flight.</p>
<p>There are two L1 caches per SM/TPC, one for each partition.</p>
<p>The following table lists the basic tag and data properties of the L1 (and Texture) cache.</p>
<table class="doxtable">
<tr>
<th>Property </th><th>Value  </th></tr>
<tr>
<td>Tag sets </td><td>4 </td></tr>
<tr>
<td>Tagged lines per set </td><td>24 </td></tr>
<tr>
<td>Tag bandwidth per clock </td><td>4 (1 lookup for each tag set) </td></tr>
<tr>
<td>Replacement policy </td><td>not most recently used </td></tr>
<tr>
<td>Line size </td><td>128 bytes </td></tr>
<tr>
<td>Sectors per line </td><td>4 sectors (32 Bytes/sector) </td></tr>
<tr>
<td>Tagged data capacity </td><td>12 KBytes </td></tr>
<tr>
<td>Untagged data capacity </td><td>20 KBytes (extra pool) </td></tr>
<tr>
<td>Total data capacity </td><td>32 KBytes </td></tr>
<tr>
<td>Data banks per line </td><td>16 </td></tr>
<tr>
<td>Bytes per data bank </td><td>8 Bytes </td></tr>
<tr>
<td>Texture threads per clock </td><td>4 maximum </td></tr>
<tr>
<td>Surface LD/ST threads per clock </td><td>8 maximum </td></tr>
<tr>
<td>Local LD/ST threads per clock </td><td>8 maximum </td></tr>
<tr>
<td>Global LD/ST threads per clock </td><td>8 maximum </td></tr>
<tr>
<td>L1 raw internal read data bandwidth </td><td>128 Bytes/clk </td></tr>
<tr>
<td>L1 read return data bandwidth to SM </td><td>32 Bytes/clk </td></tr>
<tr>
<td>L1 store data bandwidth to L2 </td><td>32 Bytes/clk </td></tr>
<tr>
<td>L1 fill data bandwidth from L2 </td><td>32 Bytes/clk </td></tr>
<tr>
<td>Texture data </td><td>Read only, cached </td></tr>
<tr>
<td>Global LD/ST </td><td>Read cached, Write Invalidate (1) </td></tr>
<tr>
<td>Local LD/ST </td><td>Read cached, Write Invalidate (1) </td></tr>
<tr>
<td>Surface Loads </td><td>Uncached </td></tr>
</table>
<p><b>Table 11: L1 Throughput</b></p>
<p>(1) Configurable. Can also be treated as uncached.</p>
<p>The L1 caches used across the GPU handle cache coherence issues differently, depending on access type. One basic behavior is that store operations are not cached in L1 but are treated as write-through-invalidate. Tags aren't allocated for stores, but if they hit on tagged lines previously allocated for loads, those lines are invalidated. Therefore, the L1 caches only reads. A second basic behavior is that Surface LD/ST operations are not cached in L1. This ensures L2 is the point of coherency for these operations.</p>
<p>In pixel shaders, Subtiles are grouped to stay on one SM partition and use one L1. This helps improve access locality.</p>
<p>In compute, CTAs can either be steered to live on one partition or span both partitions. See Figure 19. Selecting which mode depends on the resources needed per CTA, and the number of CTAs that can fit on one SM.</p>
<p>For CTAs that span both partition, since there's no implicit L1 coherence between subpartions, software either has to disable caching or invalidate the L1s whenever data written by one thread will be used by another. Memory barriers ensure all writes exit the L1 before letting any subsequent reads proceed.</p>
<p>If CTAs are configured to span only 1 SM partition, then the lack of L1 coherency protocol does not limit caching the read results from read/write allocation. All the Global or Thread Local memory accesses from a given CTA will be handled by a single L1 cache, so L1 may cache read operations with both the .CI and the .CA (cache all) policy hint.</p>
<div class="image">
<img src="MaxwellTechnicalOverview_img23.jpg" alt="MaxwellTechnicalOverview_img23.jpg"/>
</div>
 <div style="text-align: center;" markdown="1"> <b>Figure 19: CTA spanning options</b> </div><p> <br />
</p>
<p>The SM sends requests and Data to the L1, a group of requests sent in one clock is called a “wavefront”.</p>
<p>The L1 cache is based on a texture cache developed for streaming read-only data. Requests are maintained in-order for returning data to SM, even though fill data for misses may be returned from the L2 in any order. An extra-pool of on-chip memory is used to store data returned from the L2 early.</p>
<p>There are various conditions where in-line serialization may occur. For example, tag conflicts occur when multiple threads in the same wavefront access different locations in the same tag set. Similarly, data conflicts occur when multiple threads in the same wavefront access different locations within the same internal data storage bank. Tag sets are interleaved in x and y to reduce conflicts when requests are spatially close. For Global or Surface LD / ST operations, multiple threads access the same address are not treated as conflicts. Data is broadcast to all threads reading the same location, and the full operand from 1 thread among all threads storing to the same location is selected for the actual write in an implementation dependent manner. However, multiple thread targeting the same address for Global and Surface Atomics/Reductions are always serialized.</p>
<p>The peak BW for Global and Thread Local LD/ST operations is limited to 8 transactions per clk or 32 bytes of data per clock.</p>
<table class="doxtable">
<tr>
<th>Operand size </th><th>Transactions/clk </th><th>Bytes/clk  </th></tr>
<tr>
<td>8 bits </td><td>8 </td><td>8 </td></tr>
<tr>
<td>16 bits </td><td>8 </td><td>16 </td></tr>
<tr>
<td>32 bits </td><td>8 </td><td>32 </td></tr>
<tr>
<td>64 bits </td><td>4 </td><td>32 </td></tr>
<tr>
<td>128 bits </td><td>2 </td><td>32 </td></tr>
</table>
<p><b>Table 12: Global/Thread Local LD/ST Peak Bandwidth</b></p>
<table class="doxtable">
<tr>
<th>Operand size </th><th>Address Condition </th><th>Transactions/clk </th><th>Bytes/clk to L2 </th><th>Atomic bytes/clk to SM  </th></tr>
<tr>
<td>32 bits </td><td>Same or divergent </td><td>1 </td><td>4 </td><td>32 </td></tr>
<tr>
<td>64 bits </td><td>Same or divergent </td><td>1 </td><td>8 </td><td>32 </td></tr>
<tr>
<td>32 bits </td><td>Contiguous </td><td>8 </td><td>25.6 </td><td>32 </td></tr>
<tr>
<td>64 bits </td><td>Contiguous </td><td>4 </td><td>25.6 </td><td>32 </td></tr>
</table>
<p><b>Table 13: Global Atomics/Reductions Peak Bandwidth</b></p>
<table class="doxtable">
<tr>
<th>Operand size </th><th>Address Condition </th><th>Transactions/clk </th><th>Bytes/clk to L2 </th><th>Atomic bytes/clk to SM  </th></tr>
<tr>
<td>32 bits </td><td>Same or divergent </td><td>1 </td><td>8 </td><td>4 </td></tr>
<tr>
<td>64 bits </td><td>Same or divergent </td><td>1 </td><td>16 </td><td>8 </td></tr>
<tr>
<td>32 bits </td><td>Contiguous </td><td>4 </td><td>25.6 </td><td>16 </td></tr>
<tr>
<td>64 bits </td><td>Contiguous </td><td>2 </td><td>25.6 </td><td>16 </td></tr>
</table>
<p><b>Table 14: Global Atomic Compare and Swap Peak Bandwidth</b></p>
<p>The peak BW for Surface LD/ST depending on if it’s accessing data in raw form or formatting the data, the number of coordinates needed to access the surface dimensionality, and if the header reference describing the surface is a bound or bindless.</p>
<table class="doxtable">
<tr>
<th>Surface Dimension </th><th>Header Mode </th><th>8 bits </th><th>16 bits </th><th>32 bits </th><th>64 bits </th><th>128 bits  </th></tr>
<tr>
<td>1D_BUFFER, 1D, 2D </td><td>const </td><td>8 </td><td>16 </td><td>32 </td><td>32 </td><td>32 </td></tr>
<tr>
<td>1D_BUFFER, 1D, 2D </td><td>bindless </td><td>4 </td><td>8 </td><td>16 </td><td>32 </td><td>32 </td></tr>
<tr>
<td>1D_ARRAY, 2D_ARRAY </td><td>const/bindless </td><td>4 </td><td>8 </td><td>16 </td><td>32 </td><td>32 </td></tr>
<tr>
<td>3D </td><td>const/bindless </td><td>4 </td><td>8 </td><td>16 </td><td>32 </td><td>32 </td></tr>
</table>
<p><b>Table 15: Raw Surface LD/ST Peak Bandwidth (Bytes/clk)</b></p>
<table class="doxtable">
<tr>
<th>Surface Dimension </th><th>Header Mode </th><th>1:1 Reduction </th><th>2:1 Reduction </th><th>4:1 Reduction  </th></tr>
<tr>
<td>1D_BUFFER, 1D, 2D </td><td>const </td><td>32 </td><td>16 </td><td>8 </td></tr>
<tr>
<td>1D_BUFFER, 1D, 2D </td><td>bindless </td><td>16 </td><td>8 </td><td>4 </td></tr>
<tr>
<td>1D_ARRAY, 2D_ARRAY </td><td>const/bindless </td><td>16 </td><td>8 </td><td>4 </td></tr>
<tr>
<td>3D </td><td>const/bindless </td><td>16 </td><td>8 </td><td>4 </td></tr>
</table>
<p><b>Table 16: R Formatted Surface ST Peak Bandwidth (Bytes/clk)</b></p>
<table class="doxtable">
<tr>
<th>Surface Dimension </th><th>Header Mode </th><th>1:1 Reduction </th><th>2:1 Reduction </th><th>4:1 Reduction  </th></tr>
<tr>
<td>1D_BUFFER, 1D, 2D </td><td>const </td><td>32 </td><td>16 </td><td>8 </td></tr>
<tr>
<td>1D_BUFFER, 1D, 2D </td><td>bindless </td><td>32 </td><td>16 </td><td>8 </td></tr>
<tr>
<td>1D_ARRAY, 2D_ARRAY </td><td>const/bindless </td><td>32 </td><td>16 </td><td>8 </td></tr>
<tr>
<td>3D </td><td>const/bindless </td><td>32 </td><td>16 </td><td>8 </td></tr>
</table>
<p><b>Table 17: RG and RGBA Formatted Surface ST Peak Bandwidth (Bytes/clk)</b></p>
<table class="doxtable">
<tr>
<th>Surface Dimension </th><th>Header Mode </th><th>1:1 Reduction </th><th>2:1 Reduction </th><th>4:1 Reduction  </th></tr>
<tr>
<td>1D_BUFFER, 1D, 2D </td><td>const/bindless </td><td>16 </td><td>8 </td><td>4 </td></tr>
<tr>
<td>1D_ARRAY, 2D_ARRAY </td><td>const/bindless </td><td>16 </td><td>8 </td><td>4 </td></tr>
<tr>
<td>3D </td><td>const/bindless </td><td>16 </td><td>8 </td><td>4 </td></tr>
</table>
<p><b>Table 18: R Formatted Surface LD Peak Bandwidth (Bytes/clk)</b></p>
<table class="doxtable">
<tr>
<th>Surface Dimension </th><th>Header Mode </th><th>1:1 Reduction </th><th>2:1 Reduction </th><th>4:1 Reduction  </th></tr>
<tr>
<td>1D_BUFFER, 1D, 2D </td><td>const/bindless </td><td>32 </td><td>16 </td><td>8 </td></tr>
<tr>
<td>1D_ARRAY, 2D_ARRAY </td><td>const/bindless </td><td>32 </td><td>16 </td><td>8 </td></tr>
<tr>
<td>3D </td><td>const/bindless </td><td>32 </td><td>16 </td><td>8 </td></tr>
</table>
<p><b>Table 19: RG Formatted Surface LD Peak Bandwidth (Bytes/clk)</b></p>
<table class="doxtable">
<tr>
<th>Surface Dimension </th><th>Header Mode </th><th>1:1 Reduction </th><th>2:1 Reduction </th><th>4:1 Reduction  </th></tr>
<tr>
<td>1D_BUFFER, 1D, 2D </td><td>const/bindless </td><td>32 </td><td>32 </td><td>16 </td></tr>
<tr>
<td>1D_ARRAY, 2D_ARRAY </td><td>const/bindless </td><td>32 </td><td>16 </td><td>8 </td></tr>
<tr>
<td>3D </td><td>const/bindless </td><td>32 </td><td>16 </td><td>8 </td></tr>
</table>
<p><b>Table 20: RGBA Formatted Surface LD Peak Bandwidth (Bytes/clk)</b></p>
<p>Surface Atomics and Reductions are only supported for raw data and for 32-bit and 64-bit data operands.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_3_1_1"></a>
8.3.1.1. L1 and compressed data</h4>
<p>There's two types of compressed data in the GPU: Losslessly compressed (using a proprietary algorithm) render target data, written by the ROP, and lossy read only texture data defined by industry standard compression algorithms. L1 treats these two types of data differently.</p>
<p>For compressed render target data, when L1 reads the data, the L2 decompresses miss data on return if its ZBC or arithmetically compressed. If the data is reduction compressed, its sent back to the L1 compressed and the data is expanded as its written into the L1.</p>
<p>For read only data compressed using BC1-BC7 formats or ASTC formats on NX, the data is stored in DRAM compressed and compacted. The L1 reads this data as-is, storing in the L1 compressed, and passes the compressed data to the Texture pipeline, which decompresses before use.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_3_2"></a>
8.3.2. L2 behavior</h3>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_3_2_1"></a>
8.3.2.1. High Level View</h4>
<p>Conceptually, the L2 architecture is straightforward. First, the L2 selects an incoming memory request from one of the various clients to service. For a write, the data is written into the cache, with byte enables. For a read, L2 determines whether the data is present (cache hit) or not (cache miss). In the case of a cache hit, the request is read from data banks and returned to the requestor. A cache miss causes L2 to generate a fill request to FB. When FB returns the fill request, then the read response to the client can be returned. L2 has to send the requests through some additional processing stages if it is an atomic or compressed request.</p>
<p>NX has 2 slices of L2 cache with total capacity of 256 KBytes and line size of 128 Bytes.</p>
<p>L2 is composed of individual slices, with slices interleaved every 1 KBytes on NX. Each slice can perform one tag lookup per clock. There are multiple virtual channels that can access tag look up pipeline. A blocking of one virtual channel does not impact the issuing from other virtual channels.</p>
<p>L2 cache line size is 128 Bytes comprised of four 32 Bytes sectors, It is physically implemented as 4 banks of 32 Bytes wide data arrays. Each bank can process one 32 Bytes read or one 32 Bytes write per cycle. Read sources range from clients, system agents, FB return data, and other internal sources. Multiple requestors arbitrate to gain r/w access to these banks. Bank conflicts can limit L2 performance.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_3_2_2"></a>
8.3.2.2. L2 bandwidth</h4>
<p>Figure 20 shows the major I/O connections between an L2 slice and other function The peak bandwidth for each path is shown.</p>
<div class="image">
<img src="MaxwellTechnicalOverview_img24.jpg" alt="MaxwellTechnicalOverview_img24.jpg"/>
</div>
 <div style="text-align: center;" markdown="1"> <b>Figure 20: L2 slice bandwidth</b> </div><p> <br />
</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_3_2_3"></a>
8.3.2.3. CBC (Compression Bit Cache)</h4>
<p>Meta data is used to store compression information for the compressible address space. Any L2 access need look up the stored meta data so that the right portion of data is read out, and decompressed for data return. To speed up the access, each L2 slice has a CBC structure to hold the compression information. CBC is designed to have extremely high hit rate for graphics applications. CBC miss penalty is very high as the latency to load data from memory can be significant.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_3_2_4"></a>
8.3.2.4. Decompression</h4>
<p>L2 is responsible for decompressing packets before returning data to naive clients. A naive client is a memory requestor that has no concept of compression or special logic to process decompressed data. It includes all memory requestors except the ROP. A partial write from a naive client to a compressed region also requires L2 to decompress data first before data merge. Data is then written uncompressed. Decompression for color and depth shares the same pipeline.</p>
<p>Note that the texture pipe is a partial naive client - it leverages L2 to perform arithmetic decompression and ZBC decompression, but reduced data is sent directly to the texture pipe, and expanded on write to the L1. This reduces XBAR overhead in some regimes, especially when texturing from a Multisample buffer or doing an AA resolve</p>
<p>Compression generally helps to save DRAM bandwidth, increase effective L2 footprint, and improve performance. Sometimes, with arithmetic compression configuration, decompressing one sector requires reading two sectors. If majority of L2 read requests are 1-sector (32 Bytes) reads, there could be performance degradation. It is the same case for 3 or 4-plane depth compression, which also compresses 4 sectors to 2. Decompressing one sector also requires both compressed sectors.</p>
<p>The Decompression logic in L2 is capable of decompressing 8 Z-values per clock, per slice. This means for 24 and 32 bit Z values, Z can be decompressed at XBAR-limited rates of 32 Bytes per clock per slice. However, for 16-bit Z, 8 Z values per clock yields an uncompressed throughput of 16 bytes/clk. In modes with reasonably high L2 hit rate, this can cause a performance regression over uncompressed Z. Often, ZBC only compression on Z16 data will perform better than ZPlane only compression (there are only these two compression modes), since the decompressor can handle the former at full speed.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_3_2_5"></a>
8.3.2.5. Cache eviction policy</h4>
<p>L2 has three types of eviction classes, evict_first, evict_normal and evict_last. L2 lines can be assigned any of these eviction classes by the requestor. When not-tiled caching, setting ROP to evict_first, circular buffers to evict_last and the rest to evict_normal ensures the L2 keeps all circular buffer traffic on-chip, and favors Texture locality over rop locality. With Tiled caching, it may make sense to swap eviction classes for ROP and texture.</p>
<p>Only a fixed number of ways can be allocated as evict_last. When the number of evict_last crosses a configurable threshold, the LRU evict_last entry will be demoted to evict_first to open a space for the incoming evict_last request. Class evict_last is restricted to PITCH access, such as circular buffers, which are designed to never write to DRAM.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_3_2_6"></a>
8.3.2.6. Write policy</h4>
<p>NX’s L2 cache is a writeback cache. A thresholding mechanism is used to ensure for any given set, only a limited number of ways are marked as dirty before the eviction process proceeds. Requests can be marked as "volatile", which forces uncached behavior - write are directly written to memory and read data doesn’t remain cached in the L2. Back to back volatile reads to the same cacheline serialize at the L2, waiting for earlier reads to complete before later ones are sent to the DRAM.</p>
<p>Note: When tiled caching, ensure the max dirty ways threshold is set high enough to ensure one tile's worth of render target stays on chip. Then when transitioning from one tile to the next, the LRU replacement policy will force the previous tile's dirty data to be evicted.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_3_3"></a>
8.3.3. Memory Management Subsystem</h3>
<p>The GPU has a memory management unit called the GMMU, which performs memory, protection, address translation and produces a per-page KIND, that can be used to tell the L2 and ROP how to compress/decompress the data. It is optimized for 64 KBytes or 128 KBytes pages.</p>
<p>On NX, the address out of the L2 is sent to the System MMU, or SMMU, for final address translation to 4 KBytes pages. This section describes the performance characteristics of both systems.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_3_3_1"></a>
8.3.3.1. The GMMU</h4>
<p>GMMU has a TLB hierarchy of uTLB (1 per TPC), L1 TLB (1 per GPC), and L2 TLB (1 per GPU, not available in NX) where uTLBs connect to L1 TLB which connect to page table walker. During page table walk, GMMU always fetches 16 Page Table Entries (PTEs, 1 demand and 15 prefetch) and store them in same TLB cacheline.</p>
<p>Each TLB level is able to stream misses to the downstream TLB at full rate. The GPU load/store pipeline takes into account miss latency to the final level TLB (L1 for NX). Unless the working set overflows the final level TLB, the GMMU shouldn't limit performance.</p>
<p>Assuming 64 KBytes big page size, each TLB line covers up to 1 MBytes of data footprint. Usually, it is expected to have maximum coverage. On NX, GMMU TLB coverage is 1 GBytes for 128 KBytes pages and 512 MBytes for 64 KBytes pages.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_3_3_2"></a>
8.3.3.2. The SMMU</h4>
<p>The SMMU is the system-MMU providing address translation for all clients in the system except the CPU. In the GPU context, it is downstream from the GMMU.</p>
<p>The system can be configured to only use the GMMU for the final VA-&gt;PA translation for the GPU or it can be setup such that GMMU translates to the intermediate physical address and SMMU translates to the final physical address.</p>
<p>There are some common use cases where SMMU is bypassed (GMMU translated address is used directly to access the memory). GMMU page table memory, compression backing store, and protected region (VPR, WPR) accesses are done through SMMU bypass. Optionally, memory can be "carved out" that bypasses the SMMU for the GPU's render targets to reduce TLB misses.</p>
<p>The SMMU in NX consists of the following structures:</p>
<ul>
<li>TLB</li>
<li>PTC (Page Table Cache)</li>
</ul>
<p>PTC includes the logic for page table walk and the second level cache for page table translation. In NX, provides a memory footprint coverage of 24MB of physical memory. As with the GMMU, the SMMU is designed to handle misses from the TLB to the PTC at full speed. A miss in the PTC cache triggers the request to the DRAM. These page table miss requests to the memory controller are marked as highest priority requests to the memory controller and is serviced as highest priority.</p>
<p>In case multiple surfaces are being simultaneously accessed by various engines in the system, if each of the surface is aligned to 2N*32 KBytes base address (N being an integer), then all the starting access for the surface would map to a single set of the TLB. This would reduce the effective usable size of TLB by half. To mitigate this issue, it is beneficial to map base addresses of half of the surfaces to 2N*32 KBytes and other half of surfaces as (2N+1)*32 KBytes (N being any integer).</p>
<h2><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_4"></a>
8.4. WORK SCHEDULING</h2>
<p>This section discusses performance implications regarding scheduling and launching work on the GPU, including synchronization, state and constant update overhead.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_4_1"></a>
8.4.1. Host and Host1x</h3>
<p>In general most of the operation of Host is overlapped by downstream engine work. The following items, however can effect performance:</p>
<ul>
<li>SW controls how long a TSG runs by setting a timeslice (typically in the 1-2ms range) for the TSG in the runlist. How frequently a TSG comes up for scheduling can be adjusted by repeating the TSG on the runlist (typically each TSG only appears once per runlist).</li>
<li>Switching between channels can be done without switching the entire GPU pipeline, which allows the scheduling HW to search dependencies while waiting for engine work to finish. The state of the channel is stored in memory and loaded with physical requests, the overhead to swap channels is that of a memory latency, around 300ns.</li>
<li>Host has a large latency buffer to allow it to hide the fetch latency of the pushbuffer in steady state operation. This allows host to supply the graphics pipe with up to one method per clock of sustained state on NX.</li>
</ul>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_4_2"></a>
8.4.2. Channels and Subchannels</h3>
<p>Switching between two channels, call them A and B, causes a "context
switch". Host stops sending methods from channel A's pushbuffer, then waits for FE to report that all prior methods from A have completed. Next Host saves all the context state for A into a context buffer in memory, then loads all the state for context B, and starts sending new methods from channel B's pushbuffer. The context save/restore process takes on the order of 20-100us, plus the time to idle channel A which can be significantly longer. The wait for idle time is purely a function of the amount of work outstanding in the graphics engine at the moment the context switch is requested. An application may want to split apart extremely large draw commands (millions of vertices) or split apart extremely long running compute grids if the wait for idle time becomes too large. A single Draw command or a single grid launch cannot be preempted in the middle. The work from that single command must always run to completion before the context is considered idle.</p>
<p>Channels are independent. They have independent method state and independent page tables. They execute asynchronously on the engine, and the interleaving of channels is determined by Host at runtime. Semaphores and syncpoints can be used to communicate between channels and create dependencies that enforcing ordering between the channels at runtime. An application needs to choose whether it wants to break its workload into a set of cooperating channels, or use a single channel with multiple "subchannels".</p>
<p>A subchannel is a collection of methods that all go to the same "pipeline". There are five subchannels serviced by Host and FE: 3d pipeline, compute pipeline, 2d pipeline, copy engine pipeline, and the inline2memory copy pipeline. Each subchannel has independent method state but all subchannels in the same channel share the same page table. A channel's pushbuffer can contain methods for all 5 subchannels. Pushbuffer methods are always executed in order, however when two consecutive methods (X and Y) are from different subchannels, then the HW automatically does a "subchannel switch".</p>
<p>A subchannel switch causes FE to wait for idle after method X and do a flush to guarantee all prior action methods have completed and all memory writes are visible in L2. Next FE invalidates all the pipeline caches associated with subchannel Y (but not the L2 cache), before launching method Y down the new pipeline. This forces the new pipeline to fetch any data that may have been written by the prior subchannel from L2 or FB memory, and not from some stale pipeline cache.</p>
<p>Subchannel switches are a form of implicit synchronization between pipelines without requiring semaphores or explicit WFI methods. In this way for example, the compute pipeline can write out a buffer that is then immediately read back by the 3d pipeline, or the copy engine can update memory which is then immediately read by the compute pipeline, etc. A subchannel switch still incurs the wait for idle time, the same as a context switch, but there is no save/restore overhead like with a context switch. The order of execution between subchannels is strictly defined by the order of the methods in the channel's pushbuffer, whereas the order of execution between channels is determined by Host at runtime based on semaphores and timeslicing.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_4_3"></a>
8.4.3. Synchronization</h3>
<p>The following sections describe the major forms of synchronization on the GPU.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_4_3_1"></a>
8.4.3.1. Semaphores</h4>
<p>Semaphores are the most common form of synchronization between channels on the graphics engine. One channel does a semaphore release after a particular event completes, while another channel does a semaphore acquire to wait for that event. In some cases, the same channel may do both the release and acquire. For instance, the application may write out a buffer with the compute pipeline, release a semaphore, acquire the same semaphore, and then read the buffer as vertex data in the 3d subchannel.</p>
<p>For an acquire operation, Host polls the semaphore's location in memory and blocks further pushbuffer methods until it succeeds. The Acquire command specifies whether Host should context switch to another channel on an Acquire failure, or stick with the same channel until its timeslice expires. If the same channel is both releasing and acquiring the semaphore, then the application should request Host not switch on the acquire failure since meaningful work is still being accomplished while Host is polling. If different channels are releasing/acquiring a semaphore, then the application should request Host switch immediately on a failure so that the channel doing the release is given a chance to run without having to wait for an entire Host timeslice.</p>
<p>Semaphores can be released from four locations on the GPU:</p>
<ol type="1">
<li>From Host after all prior methods are launched to FE. This is a "frontend" semaphore. Host semaphores can optionally request a Wait for Idle (WFI) so that all prior work is not just launched but also completed before the semaphore is updated.</li>
<li>From FE at the completion of the 3d pixel pipeline, after ROP. This is a "backend" semaphore. This semaphore never causes a WFI but instead pipelines along with other 3d work. The semaphore is released when all prior draw commands have completed and any ROP memory writes are pushed to L2.</li>
<li>From FE at the completion of the 3d stream output pipeline. This is also a "backend" semaphore. It only synchronizes memory writes done by stream output, not ROP.</li>
<li>From FE at the completion of a compute grid. It synchronizes all compute shader writes. It does not cause a WFI or interfere with other grids that may be executing concurrently.</li>
</ol>
<p>For best performance, an application should prefer backend semaphores since they are pipelined and allow other parts of the graphics or compute pipeline to continue running. Host semaphores generally require the WFI option to be useful for synchronization. The one exception is for pushbuffer tracking. A host semaphore without a WFI can be used to determine when a pushbuffer has been read, even if the work implied by the methods is still running in the 3d or compute pipeline.</p>
<p>Note, semaphores cause an implicit flush operation in HW, which guarantees memory writes from prior ROP operations, or prior stream output operations, or prior compute shaders have reached the L2 cache. This is sufficient for synchronization between two channels both executing on the graphics engine, since both channels will use the L2 for all their memory reads and writes. It is not sufficient for communication with the CPU on an NX system since the CPU can read/write memory without consulting the L2 cache. For communication with the CPU, sync point operations are preferred because they also flush the L2. See below for details.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_4_3_2"></a>
8.4.3.2. Sync Points</h4>
<p>The NX GPU contains a special form of semaphores called "sync points". A sync point is basically an on-chip semaphore using dedicated registers in the Host1X unit. Sync points provide a means for synchronizing operations between channels or between the GPU and CPU. Incrementing a sync point register is analogous to a semaphore release. As one engine is performing increments, another engine may be performing a sync point wait, which stalls until the selected sync point register reaches a threshold value, thereby performing an operation analogous to a semaphore acquire. The driver manages the sync point register resources. Typically one register is dedicated for synchronizing 3d pixel operations and another is used for stream output.</p>
<p>Sync point increment operations are performed by either Host or FE. FE can do them after the completion of the pixel pipeline or after the completion of the stream output pipeline. There is no sync point equivalent for a semaphore release after the completion of a grid. FE sync point increments can optionally be commanded to clean the L2. If cleaning is requested, all dirty lines in the L2 are written to memory and only a clean copy is kept in the L2. Cleaning is done before the sync point is incremented. This is required when synchronizing with the CPU since CPU reads will not consult the L2 cache and expect to find the latest data in memory.</p>
<p>Sync point wait operations are performed by Host, very similar to Host acquires. The sync point wait can request that Host immediately switch to another channel if the sync point has not reached the threshold, or Host can be told to not switch until its timeslice expires. Again, switching makes sense if some other engine or the CPU is doing the sync point increment. Not switching makes sense if the same channel is doing the increment and the wait.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_4_3_3"></a>
8.4.3.3. Wait for Idle (WFI)</h4>
<p>Methods exist to explicitly request either Host or FE to wait for idle (WFI). This is a reasonable form of synchronization between two operations in the same channel, since the second operation will be blocked until the WFI and all prior operations complete. However this is not a sufficient synchronization mechanism between two channels or between the GPU and CPU. For those cases, an explicit synchronization event via a semaphore or sync point is required.</p>
<p>As mentioned earlier, subchannel switching is a form of implicit WFI synchronization. Subchannel switches also cause cache invalidates so that one subchannel can write data that the second subchannel wants to read.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_4_3_4"></a>
8.4.3.4. Pixel Barriers</h4>
<p>A special method exists to cause a Pixel shader barrier. This method forces all ROP operations from prior rendering operations to complete before any subsequent pixel shader work starts. The barrier is implemented after Early Z and before the pixel shader. This is useful for transitioning a render target from ROP ownership to pixel shader ownership for use as a texture with minimum overhead. Pixel Shader Barrier can only be used when earlier shader stages, such as Vertex or Tessellation evaluation shader, do not bind the render target as a texture since earlier stages are before the point of the barrier in the pipeline.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_4_3_5"></a>
8.4.3.5. Compute Sked Synchronization</h4>
<p>An application may request up to two semaphore releases at the completion of a grid. These semaphores indicate the given grid has completed all work and all writes are in the L2, but they do not guarantee anything about other grids possibly executing in parallel on the GPU.</p>
<p>Semaphores operations can be used to create a chain of dependent grids. For instance, assume compute has the following dependency graph of grids.</p>
<div class="image">
<img src="MaxwellTechnicalOverview_img14.jpg" alt="MaxwellTechnicalOverview_img14.jpg"/>
</div>
 <div style="text-align: center;" markdown="1"> <b>Figure 21</b> </div><p> <br />
</p>
<p>For proper operation, Grid B, C, and D should acquire A's semaphore before launching. Grid B, C, and D may execute in parallel. Grid E should acquire the release from D, and F should acquire the release from grid B, C, and E before launching.</p>
<p>F could also simply do a WFI right before launching and not do any semaphore acquires. This would insure all prior grids are complete. Sometimes a WFI is faster than a complicated set of semaphore acquires, if there is truly no other parallel compute work to perform.</p>
<p>The compute scheduler (SKED/CWD) also has the ability to chain dependent grids together that only depend on exactly one prior grid. For instance Grid E could be chained to D, such that when D is finished E automatically is launched by HW. This dependent grid chaining is even faster than semaphores.</p>
<p>Note the compute pipeline does not support sync points directly but they can be emulated using Host operations. So for proper GPU to CPU communication on NX, compute should release a semaphore at the end of a grid. Then Host should acquire this semaphore, do an L2 clean operation, then execute a sync point increment in Host to communicate with the CPU.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_4_3_6"></a>
8.4.3.6. GPU/CPU data synchronization</h4>
<p>Compute programs should use sys mem barriers to ensure data is visible to the CPU, while graphics programs should flush pending writes, followed by using a semaphore to transfer ownership. The CPU needs to treat data from the GPU as uncached. Furthermore, on NX, the L2 needs to be flushed before the semaphore to ensure coherent visibility.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_4_4"></a>
8.4.4. Frontend Unit/State</h3>
<p>The FE unit is responsible for interpreting pushbuffer commands from Host and controlling the rest of the graphics and compute pipeline units. It handles WFI operations, subchannel switches, context switches, semaphore synchronization, and state validation.</p>
<p>Most pushbuffer commands that modify state or launch an action (such as a Draw command) are handled by FE at 1 method per clock. FE also verifies the correctness of a command such as invalid fields or inconsistent state across multiple methods via hardware based method checks. Incorrect or inconsistent methods cause an interrupt back to the driver.</p>
<p>FE has special logic for handling state methods. It contains shadow state of the graphics pipeline which allows it to filter redundant state methods at 1 method per clock. The driver can also program a state validation engine inside the FE unit to perform more complicated state manipulation and validation using a simple macro program. These programs typically take a few clocks to execute their logic per pushbuffer method.</p>
<p>FE can also filter entire draw commands or compute launches if rendering is disabled by the application. The application typically does a z-only pre-pass to determine if any pixels are visible in a set of primitives. It then uses the occlusion query interface to enable or disable rendering so that these same draw commands can be filtered in a second pass. FE does the draw filtering so that no other downstream units even see the commands when rendering is disabled.</p>
<p>FE handles wait for idle operations for the graphics and compute pipelines. Some state methods cause an implicit WFI operation. FE is able to chain together N state methods that require a WFI if they are consecutive in the pushbuffer. Only 1 WFI is performed instead of N.</p>
<p>FE also handles the wait for idle operations for subchannel switch operations. An application should minimize subchannel switches if possible to reduce the WFI penalty. Separate subchannels exist for the 3d, 2d, compute, inline2memory, and Copy Engine pipelines.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_4_4_1"></a>
8.4.4.1. Inline2memory Copies</h4>
<p>FE can copy inline pushbuffer data to a memory buffer using the inline2memory (i2m) pipeline at 8 bytes per clock. This is useful for downloading small data structures such as: small vertex buffers, index buffers, constant buffers, shader programs, texture headers, and texture samplers. In the compute pipeline, i2m is useful for downloading constant buffers and QMDs. For very large copies, the Copy engine is preferred over i2m because it has higher bandwidth, see below.</p>
<p>Inline2memory commands exist in several places. They exist as a separate i2m subchannel and they also exist within the 3d and within the compute subchannel. When used as a separate subchannel, hardware automatically performs all the required synchronization, such as a WFI when the copy is finished and cache invalidate methods for the 3d or compute pipelines. This is the simplest way for an application to use i2m since all synchronization is performed automatically by HW.</p>
<p>When i2m methods are used with the 3d or compute subchannel, then all synchronization is the responsibility of the application. A semaphore can be released with the i2m operation completes which other commands can wait on. Any cache invalidates become the responsibility of the application. Using i2m within the 3d or compute subchannel can be faster since no WFI is required to drain the 3d or compute pipelines of prior work.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_4_5"></a>
8.4.5. Synchronous copies</h3>
<p>The copy engine is the DMA engine used for moving data from one memory location to another. The copy engine is designed to saturate PCIe Gen3 bandwidth at operating frequencies of 880MHz or higher.</p>
<p>The copy engine's internal datapath is 16 Bytes wide and its read and write interfaces to the memory subsystem are also 16 Bytes wide. The write interface is used for sending out write data, read and write requests to the memory subsystem. As a result, the data utilization of the interface depends on the request size. When making 256 Bytes requests (the largest request size supported by copy engine), 16 cycles of 16 Bytes data are written out every 18 cycles (2 cycles of read and write request overhead) resulting in an effective write interface BW of 14.2 Bytes/cycle. When making smaller requests, the effective interface BW drops with request size (for example, when making 128 Bytes requests, it is 12.8 Bytes/cycle) which in turn reduces the overall copy engine throughput.</p>
<p>To hide read latency, the copy engine has an internal read latency buffer that allows multiple read requests to be outstanding. The copy engine can also have reads from a number of different copies outstanding (if the copies are pipelined copies - see below). The copy engine's internal buffers are designed to hide system memory read latency over PCIe Gen3 when copies are 2KBytes or greater.</p>
<p>The copy engine allows copies to be designated as pipelined or non-pipelined in the launch command. The copy engine can start sending the read requests of a pipelined copy immediately after the read requests from the previous copy have been sent out. For non-pipelined copy, the read requests from a non-pipelined copy will not be sent out until the previous copy completes (all the read data comes back and all the writes are sent out). So marking a copy as non-pipelined introduces a bubble of at least one read latency. Independent copies with no possibility of memory overlap should be marked as pipelined for best performance, but copies that can have memory overlap should be non-pipelined for functional correctness.</p>
<p>On NX, the copy engine is part of the Graphics engine and is driven by the same method stream driving Graphics. Work assignment to the Graphics engine and the copy engine happen in sequence through inter-engine subchannel switch. Methods from a new subchannel will not start execution until all work from the previous subchannel have completed.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_4_6"></a>
8.4.6. State Flow</h3>
<p>State bundles and draw commands are generated by FE from the method stream, and flow down in a pipelined manner to the various functional units. That is to say, the new state will be applied to subsequent rendering work, received possibly as soon as the very next clock after the state bundle is processed.</p>
<p>State is pipelined in the GPU along with data. This means large number of state changes can be in flight at any one time, reducing the overhead of draw calls or frequent state changes.</p>
<p>There are two major categories of state bundles, depending on the method that generated them:</p>
<ul>
<li>True state bundles which immediately update a piece of local state in a unit. These are processed in the single clock it takes the receiving unit to receive the state bundle.</li>
<li>"Trigger" bundles generated from "action" methods. There are two main subcategories:<ul>
<li>Trigger bundles that take a small fixed number (possibly 1) of clocks for the receiving unit to process. Usually, the reason for multiple clocks is the serialization of some communication related to the state change with a neighboring unit.</li>
<li>Trigger bundles that take variable number of clocks e.g. to wait for prior work to drain out of certain pipe stages, or for some amount of a resource to free up. In the latter case, frequently used resources are sized to minimize the probability of needing to wait for it to free up.</li>
</ul>
</li>
</ul>
<p>Any unit that is responsible for decoding a particular state bundle also has hard-coded knowledge of whether any downstream unit ever needs to decode that same state bundle. If a unit determines that the state bundle is not decoded downstream, the last unit to decode the state bundle also "kills" the bundle, so that it does not propagate it down the pipeline.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_4_6_1"></a>
8.4.6.1. Constants</h4>
<p>In the method stream, there exist two major operations related to constant buffers.</p>
<p><b>Constant Buffer Binding</b></p>
<p>The method stream sends a descriptor of the constant buffer (base address and size), followed by one more subsequent methods that each respectively specify a shader stage and one of the 18 constant "banks" to which to bind the constant buffer.</p>
<p>Binding of a "new" constant buffer allocates one entry in an SCC internal resource called the constant buffer table (cbtbl). A constant buffer is considered new if it is not currently bound to any shader bank. Conversely, when there are no more shader banks binding a particular constant buffer, the corresponding cbtbl entry is marked as pending deallocation, but the entry is not truly deallocated until all prior work has exited the pixel shader. For Z-only rendering, the entries free in the raster pipeline. Cbtbl entries are de-allocated in order. When tiling, this means constants won't be freed until the last tile is finished rendering.</p>
<p>Binding a constant buffer is method-stream limited, so the biggest performance issues have to do with the limit of 128 constant buffer changes in flight.</p>
<p><b>Inline Constant Buffer Updates</b></p>
<p>Maxwell has the capability to partially update a constant buffer, and keep both the old and new versions of the buffer in use simultaneously. This capability is used by the driver and OpenGL applications, although other APIs don't expose this capability, and require constant buffer be treated as read-only surface, created and freed by the application.</p>
<p>If a constant buffer is updated but no longer in use, the performance cost of inline constant updates is set by the method throughput. If the constant buffer is still in use, then a new version of the constant buffer is created, by writing 256 byte blocks of the old constant buffer that are updated to a structure in the L2. Each write takes 9 clocks.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_4_6_2"></a>
8.4.6.2. Differences between Compute and Graphics</h4>
<p>There are a few differences between the compute pipeline and the graphics pipeline:</p>
<ul>
<li>The compute pipeline is stateless, where each Queue Meta Data (QMD) is self-contained with all state needed to execute the grid, where graphics inherits state from previous work launches.</li>
<li>Compute supports up to eight constant buffers, where graphics supports 18. Furthermore, up to three constant buffers are reserved for the driver and compiler. This means the driver needs to rename any constant buffers above the compute limit. If the application uses more constant buffers than compute can support, the driver will use global loads to access those constant buffers.</li>
<li>Thread blocks are launched into the SM in scanline order: all the threads with the same Y followed by all the threads with the next Y. This can mean that loads and stores to block linear surfaces may have inefficient access patterns. If thread blocks are defined as 8 or 4 wide, then each warp will contain a 8x4 or 4x8 block of threads, enabling more efficient memory accesses.</li>
</ul>
<h2><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_5"></a>
8.5. GRAPHICS WORLD SPACE PIPELINE</h2>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_5_1"></a>
8.5.1. Index Fetch</h3>
<p>Indices are fetched by the PD unit in response to draw commands in the method stream. A draw command also requires at least 1 or 2 methods (requiring respective number of clocks) to send BEGIN and END. In most common cases, special "elided" draw methods can send the BEGIN and END in a single method that takes just one clock.</p>
<p>In NX, the (up to) 2 clock method overhead for the draw command is "hidden" behind the 2 clocks it actually takes for the GPC to process 2 primitives, so as long as the draw command draws more than two primitives, the draw command overhead can be ignored.</p>
<p>On NX, Index fetch bandwidth in PD is 16 bytes per clock. This is fast enough to ensure PD is not the limit for 32 or 16 bit indices.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_5_2"></a>
8.5.2. Vertex Consolidation</h3>
<p>After PD fetches the indices associated with a draw command, the PD unit processes the indices into units of work called "batches", each of which are to be consumed by a single instance of a vertex shader. The main function of this processing step in PD is to remove duplicate indices so that a single batch contains only unique indices (up to 32), plus additional primitive information to associate these unique indices in the batch with all the primitives (up to 32) of the batch. This reduces the number of vertex shader threads necessary to process primitives that share indices, which is very typical if an application sends down independent triangles that are actually part of an object model made out of a mesh of triangles.</p>
<p>For NX, worst case batch formation (vertex consolidation) in PD occurs at rates far exceeding the GPC's 1 prim/clock processing rate.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_5_3"></a>
8.5.3. Vertex Fetch</h3>
<p>There is one Primitive Engine (PE) in each TPC, and each PE has a Vertex Attribute Fetch unit (VAF). Each VAF is capable of fetching 1 vector attribute per vertex per clock, and is not affected by the number of scalar attributes per vector which can be between 1 and 4.</p>
<p>The VAF unit has a dedicated L1 streaming cache which forwards miss requests to L2. The VAF L1 cache only eliminates duplicate requests within a batch of 32 vertices. The VAF L1 cache is fully associative, and consists of 32 cache lines, each of which is 32 bytes in size. This allows VAF to handle either AOS or SOA arrangements of vertex attributes in memory efficiently. Miss requests to L2 are always 32 bytes. If a vector attribute straddles two 32-byte lines, then two requests are generated. The VAF unit can process at most one 32-byte request per clock, and so it is theoretically possible it can become request limited.</p>
<p>It is expected that the L2 cache eliminates duplicate requests between batches (e.g. due to a vertex being present in two consecutive batches), so that each vertex is ideally only fetched once from memory.</p>
<p>The VAF supports reading vertex data from multiple attribute streams. This is useful when the same data is used in multiple passes, with different attributes. For instance: separating position from other attributes in two streams allows fetching only position during a Z-prepass, and then fetching all the attributes during the full render.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_5_4"></a>
8.5.4. Inter-Stage Buffer Entries (ISBEs)</h3>
<p>The TPC stores per-primitive information and vertex attributes in a structure called an ISBE (Inter-stage Buffer Entry). ISBEs are stored in a circular buffer that resides in shared memory (which is normally used by compute) when executing graphics work. An ISBE stores the information that needs to be conveyed between fixed function PE units and SM Shader cores in the world space pipeline. Figure 22 illustrates the format of an ISBE. The HW only stores vertex attributes that are written and consumed between consecutive shader stages or HW blocks e.g. if an attribute is generated by a Vertex Shader (VS), but not consumed by the Tessellation control shader (also called Hull Shader), then it is not allocated any memory in the ISBE. Similarly, other special attributes like Primitive ID and Per-Patch attributes are only allocated when needed.</p>
<div class="image">
<img src="MaxwellTechnicalOverview_img15.jpg" alt="MaxwellTechnicalOverview_img15.jpg"/>
</div>
 <div style="text-align: center;" markdown="1"> <b>Figure 22: ISBE Layout</b> </div><p> <br />
</p>
<p>The TPC generally allocates separate ISBEs for input and output attributes for a particular shader stage, such as the Vertex Shader. However, if the compiler detects that all input attributes are read before any output attributes are written, then it enables an optimization where the input and output attribute ISBEs are overlapped in the Shared memory storage, which allows more Vertex shader warps to run concurrently in the SM. In addition, between two consecutive shader stages that run in the same SM, such as Tessellation evaluation shader (TES, also called domain shader, or DS) -&gt; Geometry Shader, the HW will overlap the storage for the TES input ISBE with the storage for the GS output to reduce attribute storage requirements.</p>
<p>The following is the formula for calculating the size of an ISBE, which can be used to determine the total VS warp occupancy of the SM. The following formulas are for Vertex-Shader only (DX9-style) graphic configuration.</p>
<div style="text-align: center;" markdown="1"> <em>VS input ISBE attribute size = Number of input vertex attributes (scalars) x 128 Bytes</em> </div><p> <br />
 </p><div style="text-align: center;" markdown="1"> <em>VS output ISBE attribute size = Number of output vertex attributes (scalars) x MAX(4, Next power of 2 no. of vertices in batch between 4 and 32 (most often rounded up to 32)) x 4 Bytes/scalar</em> </div><p> <br />
 </p><div style="text-align: center;" markdown="1"> <em>Number of VS warps if input and output can be overlapped = ISBE storage capacity / ( MAX( VS input ISBE attribute size , VS output ISBE attribute size ) + 128 Bytes for Indices + Primitive ID present ? 128 Bytes : 0 )</em> </div><p> <br />
 </p><div style="text-align: center;" markdown="1"> <em>Number of VS warps if input and output cannot be overlapped = ISBE storage capacity / ( VS input ISBE attribute size + VS output ISBE attribute size + 128 Bytes for Indices + Primitive ID present ? 128 Bytes : 0 )</em> </div><p> <br />
</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_5_5"></a>
8.5.5. Circular Buffers</h3>
<p>Circular buffers reside in L2 and are used to communicate attributes in two places in the pipeline. First, between the Tessellation control shader and Tessellation evaluation shader stages, which we call the Alpha Circular Buffer. This is done to re-distribute work due to possible high tessellation or geometry shader expansion. Second, between the Viewport/Clip/Cull subunit (VSC) of the PES at the end of the world space pipeline and Setup, which we call the Beta Circular Buffer. The Beta CB only stores vertex attributes for vertices that survive culling in the VSC unit.</p>
<p>Cache lines in L2 that contain Beta CB attributes are marked as EVICT_LAST so that they are not written out to framebuffer. In addition, after the attributes in the CBEs (Circular Buffer Entries) are consumed, the L2 cache lines are invalidated. These mechanisms ensure that the attributes stay on-chip.</p>
<p>The following formula is used to calculate the size of a circular buffer entry:</p>
<div style="text-align: center;" markdown="1"> <em>Circular Buffer Entry size = Round up to next multiple of 128 bytes ( # of vertices in CBE * 16 Bytes for raster attributes ) + Round up to next multiple of 32 bytes ( # vertices in CBE * # of scalar attributes per vertex (excluding XYZ position, including 1/w) * 4 Bytes per attribute )</em> </div><p> <br />
</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_5_6"></a>
8.5.6. Viewport, Stream Output and Clip (VSC)</h3>
<p>The VSC subunit in PES implements several functions: clipping, culling, viewport transform, perspective correction of attributes, stream output, and copy out for writing Alpha CBEs. It also contains new features vertex position swizzle and viewport multi-projection.</p>
<p>The unit is designed to process either 0.88 vertex or 1 primitive per clock. The VSC unit reads indices and vertex attributes from the ISBE of the last world-space shader stage, and sends visible primitives to Setup, through WDX, and writes the associated non-culled vertices to the Beta CB. Data in the geometry pipeline is processed in batches of up to 32 vertices, with the corresponding number of primitives associated with those vertices. VSC can process 8 attributes/clk for perspective correction, stream output or copy out.</p>
<p>For tessellated geometry, a batch can have up to 42 triangle primitives that share the same 32 vertices. For non-tessellated geometry, a batch is limited to 32-(number of vertices/primitive+1), or 30 triangles, 31 lines or 32 points. This means non-tessellated geometry can have anywhere from 10 to 30 triangles per batch. At best, due to overhead of fetching indices and attributes from the L1, VSC can achieve 0.83 primitives per clock for non-tessellated triangles (culled- setup or STRI will limit visible primitive throughput). The rates for tessellated primitives vary and can be higher at high tessellation factors up to 1 primitive/clk, since there can be more primitives than vertices.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_5_6_1"></a>
8.5.6.1. Clipping</h4>
<p>Clipped primitives are processed at a significantly lower rate than non-clipped primitives. In general, the rate of processing is a function of the number of planes that are intersected by the primitive. The pipeline is also optimized to handle single plane clipping more efficiently than more general multi-plane clipping.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_5_6_2"></a>
8.5.6.2. Culling</h4>
<p>The VSC pipeline can cull 1 primitive/clk. We implement several culling optimizations. This includes culling backfacing and zero area primitives, along with primitives outside the rendering frustum. Only vertices that belong to non-culled primitives are written to the Beta Circular Buffer.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_5_7"></a>
8.5.7. Tesselation &amp; Geometry Shading</h3>
<p>The performance of tessellation is a function of both the efficiency of the shaders and efficiency of the fixed function pipeline. In the pre-tessellator shader stages, namely Vertex Shader and Tessellation Control Shader (or Hull Shader), we support packing up to 12 patches in a single warp. In the common case, where patches consist of 3 control points, we effectively pack 10 patches resulting in 30 useful threads in VS or TCS warp.</p>
<p>In the post-tessellator shader stages, namely Tessellation Evaluation Shader and Geometry Shader, we split work into tasks where each task represents a single TES warp (32 threads) and if TES is not enabled, then one or two GS warps. The tasks are distributed round-robin across all TPCs in the GPU. The packing of tessellated patches into TES warps is a function of the tessellation factor. At low tessellation factors, where a patch is tessellated into fewer than 32 vertices, we support packing multiple tessellated patches into a single warp for the TES shader. We also use common vertices between the exterior and interior region of the tessellated patch. At high tessellation factors, where a patch is tessellated into more than 32 vertices, the patch is broken into multiple tasks, where each task represents a portion of the tessellated mesh. In this case, each TES warp represents a single patch. In addition, the exterior and interior regions of the tessellated patch are processed in separate tasks. For the interior region, we typically store an 8x4 grid of vertices, which represents 42 tessellated primitives. To minimize ISBE storage in cases where a task represents just one patch, we copy only the input data relevant to the patch being processed in the TES warp into Shared memory, which typically provides a 10x reduction in the input data.</p>
<p>In general, the pipeline can process tessellated primitives at the peak rates of fixed function blocks such as VSC when tessellating at high tessellation factors.</p>
<p>The performance of geometry shader is largely a function of the storage needed for the GS output, which is also stored in Shared memory. We use a GS specific structure in shared memory for GS output called GS FIFO.</p>
<p>When both tessellation and Geometry Shader are enabled, it is possible for a single TES warp to represent 42 primitives, as noted earlier. In this case, assuming GS instancing is disabled, we pack the 42 primitives into two GS warps, one that contains 32 primitives, and one that contains 10 primitives. When GS instancing is enabled, we pack instances into warps as described above. In addition, if the number of GS threads needed exceeds 64, then we break the work into tasks, where the TES shader work is replicated across tasks, but the GS portion processes different GS instances.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_5_7_1"></a>
8.5.7.1. Alpha/Beta Phases</h4>
<p>In our world-space pipeline, we call the shader stages prior to tessellator, namely VS and TCS (HS), the Alpha phase. We call the shader stages after tessellator, namely TES (DS) and GS, the Beta phase. When either TES or GS are enabled, our world-space pipeline time slices between the Alpha and Beta phases. We run the Alpha phase until we have run out of resources to hold Alpha phase output, such as the Alpha Circular Buffer in L2 getting full. We then execute the Beta phase until all Alpha output is consumed.</p>
<h2><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6"></a>
8.6. GRAPHICS PIXEL PIPELINE</h2>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_1"></a>
8.6.1. Tiled Caching</h3>
<p>Tiled rendering is implemented in the binner which is part of WDX unit that feeds Setup at the beginning of the screen-space pipeline. Since the binner runs at the beginning of the screen-space pipeline, geometry processing is performed exactly once no matter how many cache tiles a primitive intersects.</p>
<p>The screen is subdivided into tiles called cache tiles, as illustrated in Figure 23. Cache tiles reside in L2 cache. The binner buffers primitives interleaved with state. When the on-chip structures that buffer primitives fill up, a binner flush is invoked. A binner flush causes the tiler to visit every non-empty tile on screen, and sends the appropriate state and primitives that intersects each tile.</p>
<div class="image">
<img src="MaxwellTechnicalOverview_img16.jpg" alt="MaxwellTechnicalOverview_img16.jpg"/>
</div>
 <div style="text-align: center;" markdown="1"> <b>Figure 23: Illustration of cache tiles used by tiled rendering</b> </div><p> <br />
</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_1_1"></a>
8.6.1.1. Choosing Cache Tile Size</h4>
<p>On NX, the cache tile size is controlled by software through NVN API. It is recommended that a cache tile occupy a fraction of the L2 cache, such as 1/3 of the L2 cache. This is done by dividing the size of the portion of the L2 cache which is desired to be used per cache tile by the sum of the bytes per pixels for all active render targets, such as Color and Z. The calculation should factor whether MSAA is enabled and the MSAA factor.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_1_2"></a>
8.6.1.2. When to insert a tile cache flush</h4>
<p>In general, a tile cache flush should be sent whenever the render target changes. There are however exceptions to this rule:</p>
<ul>
<li>It is recommended that if the set of render targets is a subset of original set, e.g. if a particular target like a color render surface in an MRT is disabled, then there is no need to flush.</li>
<li>When rendering to an MSAA surface and an on-chip AA resolve is desired, then a flush should not be inserted between the MSAA rendering and MSAA resolve phases which involves a change of render targets. Instead, the flush should be performed after the MSAA resolve.</li>
</ul>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_1_3"></a>
8.6.1.3. Events that can cause tile to flush</h4>
<p>The most common cause of binner flushes is running out of resources to store attributes in the Circular Buffer in L2 or in the internal structures in WDX that hold primitives. Other causes of flushes are methods that idle the pipe, some non-binnable state changes such as changes that affect Zcull or the behavior of the binner itself, occlusion queries, the Pass Barrier with tilingBoundary set to true indicating global synchronization, or an explicit flush in TiledCacheAction method.</p>
<p>Other cases that can cause tiles to flush include heavyweight ROP state changes (lightweight ROP state changes don’t) and using all 128 constant versions in flight.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_1_4"></a>
8.6.1.4. Tiling performance considerations</h4>
<p>The main performance benefit of tiling is to reduce the memory traffic to DRAM, and thus avoid the memory bandwidth bottleneck. Tiling can also reduce performance in some cases. Tiling increases the workload on Setup and the raster pipeline due to primitives that span multiple tiles. For example, it may be advantageous to disable tiling when using tessellation or when doing Z-only rendering, since such cases tend to be primitive rate limited in Setup/raster and are not commonly FB limited.</p>
<p>In addition, switching between Early-Z and Late-Z can lead to a bubble in the pipeline, and so if work that uses both modes is tiled together, then such transitions are repeated for each tile, which can cause a performance slowdown. Similarly binning across pass barriers can result in significant synchronization overhead, since the overhead is paid for every tile instead of just once per state change.</p>
<p>Frequent events that cause a tile flush (see previous section) can adversely affect tiling performance, reducing the benefit from tiling by reducing FB locality while increasing overhead by increasing tiles to render.</p>
<p>Note that clears also run in late Z mode, so binning across clears can often cause a per-tile early Z to late Z transition. Given Zero-bandiwidth Clear compression state is usually enabled, performing clearing outside of (or between) tiling regions will not cause an increase in bandwidth used.</p>
<p>Lastly, tiling can reduce the effectiveness of Zcull. This is because we play back all the primitives that affect one tile before moving to the next, and so Zcull may not have formed good occluders when it receives the primitives since the feedback from ZROP for forming occluders has a latency associated with it.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_2"></a>
8.6.2. Setup</h3>
<p>Setup receives screen space vertices, per primitive data, and state data from WDX. Setup will, upon receiving a primitive, use the vertices of the primitives to compute the edge equations that define the boundaries of the primitive, and the plane equation terms that define the plane in which the primitive lies.</p>
<p>Performance</p>
<table class="doxtable">
<tr>
<th>Primitive </th><th>Clocks  </th></tr>
<tr>
<td>Point </td><td>1 </td></tr>
<tr>
<td>Line Strip, Loop, Independent </td><td>1 </td></tr>
<tr>
<td>First triangle of a strip or fan </td><td>2 </td></tr>
<tr>
<td>Subsequent triangles of a strip or fan </td><td>1 </td></tr>
<tr>
<td>Independent triangles </td><td>1.5 - 3 (1) </td></tr>
</table>
<p><b>Table 4: Setup throughput</b></p>
<p>(1) depends on CBE vertex packing. Setup can access two consecutive vertices from the cross bar. If the vertices are packed such that setup gets 2 vertices for the first triangle, then 1 vertex for the first triangle and 1 for the second, then 2 vertices for the second it can fetch 2 triangles in 3 clocks for an average rate of 1.5 clocks per triangle. The worst case would be that setup only gets 1 vertex out of the two for a triangle in which case it takes three clocks.</p>
<p>The edge equation data is sent to coarse raster to be rasterized, and partial plane equation terms are sent to the PE unit which is used to determine the plane equations for each attribute of the primitive.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_3"></a>
8.6.3. Coarse Raster</h3>
<p>The coarse raster unit uses the edge equation data from the setup unit to determine which coarse tiles lay on the primitive. The setup unit gives coarse raster a starting tile location which coarse raster uses to start rasterization. Setup indicates whether the starting tile lies on the primitive or not. If not, coarse raster performs a search operation to locate the first tile on the primitive.</p>
<p>The coarse raster unit outputs one 16x16 pixel coarse tile per clock.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_4"></a>
8.6.4. Zcull</h3>
<p>Zcull kills pixels before shading, using an on-chip conservative, approximate representation of the Z-buffer. When the pixel shader affects sample coverage Zcull can remove pixels when early-z is off, reducing the number of pixels that get shaded. When early-z is enabled, Zcull can very quickly remove pixels before Fine Raster and ZROP, improving throughput and reducing the number of z-reads and the amount of traffic send over the cross-bar.</p>
<p>Similarly, when possible, Zcull kills pixels that it determines will fail their stencil test, or will fail their depth bounds test, or are outside the near/far planes. Further, when it can determine that the ROP Z-compare will definitely pass, it marks those pixels with a "Trivial Z Accept" result which informs the ROP that no Z read is needed.</p>
<p>The Zcull RAMs are not a cache that get loaded and flushed to the memory system on the fly. Instead it is a persistent resource that is explicitly assigned to Z surfaces in bulk. This means for a very large single surface, it is possible to run out of Zcull ram and only get partial coverage. In order to allow the Zcull RAMS to cover a large number of different surfaces, the Zcull RAMs can be stored to and loaded from the memory system as an application switches between surfaces.</p>
<p>On NX, the on chip Zcull rams can be configured with different compression formats. The most heavily compressed formats can cover approximately 4 megapixels with relatively low precision, while the least compressed formats can cover 0.5 megapixels with relatively high quality Z. When stencil culling is in use, only one format is available.</p>
<p>Zcull can be optimized to cull with the depth func less/less_equal or greater/greater_equal. This is specified for each Zcull region using the ZcullDirFormat state (Zdir). When depth writes are on and the depth func moves Z in a direction opposite of Zdir, Zcull will not cull, and will not start to cull again until all writes with that depth test have been processed by ZROP and fed back to Zcull. Zcull's RAMs are lazily updated with feedback from ZROP. For overlapping triangles rasterized close together in time, it is possible that Zcull will not receive feedback from ZROP in time to form an occluder for culling.</p>
<p>Zcull can only be configured to cull with a single stencil criterion at a time. For example Zcull might be configured to cull when the criterion Sfunc=EQUAL, Sref=0x3, Smask 0xff. Zcull will then form stencil occluder and be able to cull for that criteria (provided there are no stencil writes active to remove occluders). The stencil criteria needs to be set before the drawing that forms the stencil occluders are drawn. Changing the criteria also invalidates Zcull until the next Z or stencil clear. The criterion is therefore typically set at the time of a Z clear based on the characteristics of the previous frame.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_4_1"></a>
8.6.4.1. Zcull and Shader Modified Z</h4>
<p>Depth culling in Zcull uses an approximate and conservative depth range estimated from the intersection of a triangle and 8x8 pixel tile. Often when depth is computed in the pixel shader (shader modified Z) the depth range of a primitive can be arbitrarily large or small, so the estimated depth range is Zcull is not valid and Zcull is disabled. However, when it is known that Z will only move one direction (greater or less) than the samples z of the primitive geometry, Zcull can be partially enabled. If Zdir is LESS, and Z is only modified to be larger (further) than the samples geometric depth, Zcull depth culling for depth function less continues as usual, but trivial accepts are disabled. If Z is modified to be closer (smaller) then depth culling is still disabled, but trivial accepts can occur. The inverse of the above is true for Zdir GREATER.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_5"></a>
8.6.5. Fine Raster</h3>
<p>Fine raster determines which sample points are inside a primitive. Fine raster evaluates sample points in 8x8 sample tiles. The size of the tile in pixels is based on how many samples per pixel there are. Tile sizes can range from 8x8 pixels for 1xAA to 2x2 pixels for 16xAA. Fine raster typically processes an 8x8 sample tile in a single clock.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_6"></a>
8.6.6. STRI and TRAM</h3>
<p>STRI (the Shader Triangle subunit of PE) computes plane equations for attributes at the rate of 1 plane equation/clock, and writes the results into TRAM (Triangle RAM)region, residing in the SM's Shared memory. One of the attributes for each primitive is always 1/w. STRI also takes a minimum of 4 clocks to process a primitive. So, for example, STRI will run at the peak rate of processing 1 primitive every 4 clocks when processing 1/w and 3 color attributes (RGB). There is one STRI unit in each TPC. STRI fetches the vertex attributes it needs from the Beta Circular Buffer in L2. STRI is designed to hide the latency of an L2 hit, and has a local cache to avoid over-fetching vertices that are shared by more than one primitive.</p>
<p>In NX there is 16 KBytes of TRAM in SM. Each plane equation is 12 bytes. The TRAM space is allocated in FIFO order. When the TRAM fills up, it prevents new Pixel Shader warps from being launched.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_7"></a>
8.6.7. PROP</h3>
<p>The PROP unit feeds and drains the fragment shader at quad-quad rate - i.e., 4 quads per cycle - belonging to 1 or 2 primitives.</p>
<p>PROP orchestrates the transfers of data and state from GPC to the ROPs in FBP. In terms of zeta (depth, stencil) data flowing from Raster to ZROP, PROP transfers one FRTX (8x8-sample tile) per cycle. . For color data, PROP operates at the following peak rates for fill mode or single-source blending with or without shader-modified coverage or alpha to coverage (shdCvg). Internally, C8 and C16 behave like C32.</p>
<table class="doxtable">
<tr>
<th>Pixel/Clk </th><th>Data Type </th><th>ShdCvg </th><th>AA Mode  </th></tr>
<tr>
<td>14.40 </td><td>C32 </td><td>No </td><td>1 - 2xAA, including sRGB </td></tr>
<tr>
<td>10.66 </td><td>C32 </td><td>No </td><td>4xAA, including sRGB </td></tr>
<tr>
<td>12.80 </td><td>C32 </td><td>Yes </td><td>1-4xAA, faster with shader kill </td></tr>
<tr>
<td>8.00 </td><td>C32 </td><td>Yes </td><td>8xAA </td></tr>
<tr>
<td>7.20 </td><td>C64 </td><td>No </td><td>1-4xAA </td></tr>
<tr>
<td>7.11 </td><td>C64 </td><td>Yes </td><td>1-4xAA, faster with shader kill </td></tr>
<tr>
<td>5.33 </td><td>C64 </td><td>Yes </td><td>8xAA </td></tr>
<tr>
<td>3.60 </td><td>C128</td><td>Yes </td><td>1-8xAA </td></tr>
</table>
<p>In dual-source blending, where shader emits 2 colors per fragment, PROP operates at the following rates:</p>
<table class="doxtable">
<tr>
<th>Pixel/Clk </th><th>Data Type </th><th>ShdCvg </th><th>AA Mode  </th></tr>
<tr>
<td>8.00 </td><td>C32 </td><td>No </td><td>1-8xAA </td></tr>
<tr>
<td>7.11 </td><td>C32 </td><td>Yes </td><td>1-8xAA </td></tr>
<tr>
<td>4.00 </td><td>C32 </td><td>Yes </td><td>16xAA </td></tr>
<tr>
<td>4.00 </td><td>C64 </td><td>No </td><td>1-16xAA </td></tr>
<tr>
<td>3.76 </td><td>C64 </td><td>Yes </td><td>1-8xAA </td></tr>
<tr>
<td>3.56 </td><td>C64 </td><td>Yes </td><td>16xAA </td></tr>
<tr>
<td>2.00 </td><td>C128 </td><td>No </td><td>1-16xAA </td></tr>
<tr>
<td>1.94 </td><td>C128 </td><td>Yes </td><td>8xAA </td></tr>
<tr>
<td>1.88 </td><td>C128 </td><td>Yes </td><td>16xAA </td></tr>
</table>
<p>For modes with shader-modified coverage and no color, PROP operates at the following rates:</p>
<table class="doxtable">
<tr>
<th>Pixel/Clk </th><th>AA Mode  </th></tr>
<tr>
<td>16.00 </td><td>1-4xAA </td></tr>
<tr>
<td>8.00 </td><td>8xAA </td></tr>
<tr>
<td>4.00 </td><td>16xAA </td></tr>
</table>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_7_1"></a>
8.6.7.1. Raster Tiling and Subtiling</h4>
<p>Fine raster outputs pixel data grouped into 16x16 regions, called "tiles". This corresponds to the pixels in one FB partition in aliased mode. PROP will split these tiles into subtiles, based on shader resources used (TRAM allocation, RF allocation per pixel, etc). In the SM, Texture operations are done a subtile at a time, ensuring better L1 cache locality.</p>
<p>In some modes, like AA resolve, where multiple neighboring samples are accessed per pixel, the subtile size will be set to the size of a warp, ensuring that subsequent accesses per pixel have a reasonable chance of hitting in the L1 cache.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_7_2"></a>
8.6.7.2. Multiple Render Targets</h4>
<p>In MRT (Multiple Render Targets) mode, the fragment program generates N outputs, which are drained into N corresponding targets independently. Each of those targets can have a different format, which can imply different draining rate per target, according to the single-target rates quoted above. The overall cost of draining all targets is the sum of the draining cost for each render target.</p>
<p>Shader geneated pixel kill runs at full speed (either limited at 16PPC or raster sample rate, depending on AA mode), regardless of the number of render targets.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_7_3"></a>
8.6.7.3. ROP State</h4>
<p>State destined to ROP is split into light-weight and heavy-weight. Heavy-weight state exists to simplify the design for higher end GPUs. The only difference on NX is heavy weight state changes force a tiled caching flush where light weight state changes don’t.</p>
<p>Some of the heavyweight state bundles are:</p>
<p>BETA {1,4} <br />
 BLEND_{FLOAT_OPTION, OPT_CONTROL} <br />
 BLEND_PER_FORMAT_ENABLE <br />
 COLOR_PATTERN_{R5G6B5, X1R5G5B5, X8R8G8B8, Y8} <br />
 CT_MARK <br />
 DST_{BLOCK_SIZE,FORMAT,HEIGHT,LAYOUT,LOWER,UPPER,PITCH,WIDTH} <br />
 DS_FORCE_HEAVYBUNDLE_SYNC <br />
 FLUSH_AND_INVALIDATE_ROP_MINI_CACHE <br />
 FLUSH_PENDING_WRITES <br />
 FORCE_HEAVYWEIGHT_METHOD_SYNC <br />
 GO_IDLE <br />
 INCREMENT_SYNC_POINT <br />
 ITERATED_BLEND <br />
 ITERATED_BLEND_CONSTANT_BLUE_{0,1,2,3,4,5,6,7} <br />
 ITERATED_BLEND_CONSTANT_GREEN_{0,1,2,3,4,5,6,7} <br />
 ITERATED_BLEND_CONSTANT_RED_{0,1,2,3,4,5,6,7} <br />
 ITERATED_BLEND_PASS <br />
 LOAD_ITERATED_BLEND_{INSTRUCTION, INSTRUCTION_POINTER} <br />
 MONO_PATTERN{0,1} <br />
 MONO_PATTERN_COLOR{0,1,FORMAT} <br />
 MONO_PATTERN_FORMAT <br />
 OPERATION <br />
 PATTERN_{OFFSET,SELECT} <br />
 PIXEL_SHADER_BARRIER <br />
 PM_TRIGGER_END <br />
 REDUCE_COLOR_THRESHOLDS_{FP11,FP16,SRGB8,UNORM10,UNORM16,UNORM8} <br />
 REPORT_SEMAPHORE_D <br />
 ROP <br />
 ROP_GO_IDLE <br />
 RSTR2D_BLITFINISHED <br />
 RSTR2D_BLIT_CORRAL <br />
 TWOD_PKTSWITCH_SYNC <br />
</p>
<p>On a one GPC system like NX, there's no synchronization overhead for heavyweight state. However it does cause the tiler to flush when heavyweight state is updated.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_8"></a>
8.6.8. ZROP</h3>
<p>ZROP operates on depth (z) and stencil data. ZROP receives per cycle an 8x8-sample transaction from Raster - called Fine-Raster Transaction (FRTX) - with coverage of each sample and the corresponding z-plane equation of the single primitive covering that 8x8 tile. Z operations are performed as much as possible in compressed form (FRTX z planes), but need to operate per sample, in cases where z-plane testing is ambiguous.</p>
<p>Z-plane compression allows operating at 8x8-tile granularity, rather than per sample, by comparing bounds of z planes, rather than z values of samples. Trivial accepts and rejects are performed by comparing ZMin and ZMax of two planar z tiles (8x8s). If the min-max test is ambiguous at tile granularity (i.e., the planes are too close to each other or intersect), ZROP must expand and perform the comparisons at sample granularity.</p>
<p>ZROP expands planes into samples at 11 samples/clk, sharing expanders between src and dst. If dst doesn’t need to be expanded (for instance if it’s a constant value), then Z compare runs at 11 samples per clock. If both src and dst need to be expanded Z compare runs at 5.5 samples/clk.</p>
<p>ZROP throughput is 32 Bytes/clock. Compressed Z-data hold up to 2 destination Z-planes in 32 Bytes. Sampled Z-data contain 8/~11/16 samples in Z32/Z24/Z16 mode. The actual samples covered by 32 Bytes are roughly organized in 4x2, 4x3, 8x2 sample blocks. The actual throughput is determined by the maximum number of 32 Bytes processed anywhere in the ZROP pipe.</p>
<p>The 32 Bytes throughput also applies to processing stencil data. The stencil values for an 8x8 occupy two 32 Bytes chunks organized as 4x8 blocks. Therefore stencil processing requires 1 or 2 cycles depending on which samples are covered. Stencil 32 Bytes chunks are stored independently of z 32 Bytes chunks—i.e., when dealing with only z, stencil data does not need to be fetched, and vice versa.</p>
<p>Stencil only surfaces support ZROP throughput of 64-samples/clk when stencil can be compressed. Note that Stencil+Z surfaces don’t support Stencil compression, and run at 32 stencil samples/clk. Also note that ZROP can either process one stencil packet for a given clock or a Z packet. In modes where Z and Stencil are processed, the ZROP works on packets sequentially.</p>
<p>NX has two ZROPs, each of which can process data at 32 Bytes/clk.</p>
<p>There are two possible Z16 compression modes. Z16 compression can support to 1 or 2 planes compressed but not support compressed clears, or it can support Zero bandwidth clears without plane compression (not both simultaneously). Z16 throughput runs at Z24 rate when not bandwidth limited.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_8_1"></a>
8.6.8.1. Shader-Modified Z</h4>
<p>In shader-z mode, ZROP receives individual z-values for each covered sample (i.e. not a single Z-plane for all covered samples). ZROP can receive and process up to 8 such samples per cycle. However, bottlenecks upstream of ZROP (in PROP and SM) limit the overall throughput to 4 samples/cycle. Even though we talk in terms of samples, we should remember that shader-z is computed once per pixel and in MSAA modes all samples in a pixel get the same value.</p>
<p>Shader-z data is not compressible.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_9"></a>
8.6.9. CROP</h3>
<p>CROP operates on color data received from upstream (source fragment shader) and from downstream (destination buffer). The source (src) data is either directly written out to the destination buffer, in fill mode, or blended with destination (dst) color data. CROP receives from upstream 2 quads of source (src) color data per clock in 32-bit, 16-bit, and 8-bit per-pixel formats, half that rate for 64-bit formats, and a quarter of that rate for 128-bit formats. CROP reads from memory at the rate of 32 Bytes per clock and writes at the same rate.</p>
<p>The following table documents peak performance for important CROP cases:</p>
<table class="doxtable">
<tr>
<th>Parameter </th><th>Value  </th></tr>
<tr>
<td>non-blended, (c32 or less) (1) </td><td>8 pixels/clk </td></tr>
<tr>
<td>non-blended, (c64) (1) </td><td>4 pixels/clk </td></tr>
<tr>
<td>non-blended, (c128) (1) </td><td>2 pixels/clk </td></tr>
<tr>
<td>non-blended, sRGB, BF10GF11RF11 </td><td>8 pixels/clk </td></tr>
<tr>
<td>blended, sRGB, BF10GF11RF11 </td><td>4 pixels/clk </td></tr>
<tr>
<td>blended, unorm8 </td><td>8 pixels/clk </td></tr>
<tr>
<td>blended, snorm8 </td><td>8 pixels/clk </td></tr>
<tr>
<td>blended, unorm16 </td><td>4 pixels/clk </td></tr>
<tr>
<td>blended, unorm10 </td><td>4 pixels/clk </td></tr>
<tr>
<td>blended, snorm16 </td><td>4 pixels/clk </td></tr>
<tr>
<td>blended, fp16 </td><td>4 pixels/clk </td></tr>
<tr>
<td>blended, fp32 </td><td>2 pixels/clk </td></tr>
</table>
<p><b>Table 6: CROP throughput</b></p>
<p>(1) C32 refers to all 32 bit per pixel formats (ie Unorm8x4), C64 refers to all 64 bit per pixel formats (ie fp16x4), C128 refers to 128 bit per pixel formats (fp32x4)</p>
<p>Note: this table is per-ROP. NX has two ROPs</p>
<p>CROP implements optimizations for saving processing and FB bandwidth when blending. These are called blend optimizations and try to identify cases where dst data is not needed (save the read) or when the blending result is the same as the destination pixel (save the blending and processing and the write). There are three types of blend optimization controls set by CROP as follows: Kill blend, Read suppress, Fill override.</p>
<p>A regular blend operation is performed according to the following blend equation:</p>
<div style="text-align: center;" markdown="1"> <em>blend result = src coeff * src color + dst coeff * dst color</em> </div><p> <br />
</p>
<p><b>Blendopt Kill</b>: If ROP determines that for some of the pixels the blend result is going to be the same as the dst value already in memory - e.g. if src coefficient or src color is zero and dst coefficient is one, the blend result is dst_color - ROP sets the blendopt on those pixels, such that they get killed and don't go through blending.</p>
<p><b>Blendopt Read-Suppress</b>: If ROP determines that dst data is not required - e.g. if dst coefficient is zero, the blend result is not going to depend on the dst color value and blending does not require any dst data reads - ROP sets blendopt read-suppress on those pixels.</p>
<p><b>Blendopt Fill-Override</b>: This is a special case of the "Read
Suppress" setting, where ROP determines that not only dst data is not required, but also the blend result is going to be the same as the src data. Consider the case where dst coefficient is zero and src coefficient is one, the blend result is independent of dst color value and equal to src pixel. The blend operation mimics the fill mode functionality and no blend operation needs to be performed. Another case is when blending src_coefficient * src data &gt;= 1 in UNORM format, which allows skipping the blend and just writing out 1. The regular-blend operations as defined by the above equation run at the ‘blended’ rates from the above table. OpenGL advanced-blending equations can take multiple clocks to run.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_9_1"></a>
8.6.9.1. MSAA Pixel to Sample Expansion</h4>
<p>If all coverages of pixels/samples in the ROP tile are full and the frame buffer compression format of destination data from L2 cache and/or outgoing tile produced by CROP is reduced format, CROP can process the tile in reduced format by pixel. Otherwise, CROP expands the tile to samples and operates at the smaller throughput than that of reduced format processing.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_10"></a>
8.6.10. Work Distribution to SMs</h3>
<p>The SMs process multiple types of work. In the graphics pipeline, the SMs are processing either vertex or pixel work. In the compute pipeline, the SMs are processing CTAs. Described below are the general mechanisms used to determine the distribution of work between the SMs for each pipeline.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_10_1"></a>
8.6.10.1. Work distribution in the graphics pipeline</h4>
<p>When the 3d graphics subchannel is active, the SMs process both world-space and pixel work.</p>
<p>The PD unit collects groups of vertices into warp-sized units called a batch. PD then determines which SM to send the batch to. Within a GPC the batches of vertex work are distributed in round-robin fashion between the SMs.</p>
<p>For pixel work, the GPM unit receives a series of packets from PROP that define the pixels associated with a subtile. The subtile is sent to the SM which has the most free slots in its input FIFO. After SM processing, GPM collects the resulting pixel data and sends it back to PROP. The output of the SMs is collected in the order it was launched so that the subtile data returned to PROP is in API order. Since pixel work is allocated to the SM with the least work in its input FIFO, this dynamically load balances the work between SMs by keeping all SMs busy, regardless of shader lifetime and complexity.</p>
<p>The frontend of the SM that is responsible for resource allocation and work scheduling is the MPC unit. It receives both vertex and pixel work simultaneously over separate interfaces and is responsible for allocating the SM resources and determining which work to launch next on the SM. MPC uses the resource requirements for the vertex and pixel work to determine which work to launch next on the SM. The overall goal of the algorithm is to balance the launches between vertex and pixel work when both are available.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_11"></a>
8.6.11. Transitions between earlyZ and lateZ</h3>
<p>The graphics pipeline can be configured in either late-z (traditional) or early-z (better performance) mode. The 'early' versus 'late' refers to where the z test is performed with respect to fragment shading. Late-z is the traditional graphics-pipeline mode and refers to performing the z test after shading the fragments. Early-z refers to performing the z test before shading the fragments, which allows culling (and not shading) fragments that would not make it into the final image. By default, the pipe is configured in late-z mode. Based on state and on a hysteresis setting, the pipe can switch to early-z.</p>
<p>The SetOpportunisticEarlyZHysteresis method controls how quickly a lateZ to earlyZ transition is performed, for mitigating overhead from the pipe flush required to switch between the late-z and early-z modes. The ‘instantaneous’ setting of that method makes earlyZ-lateZ decisions instantaneously ahead of the next primitive to be rendered. There is also a LATEZ_ALWAYS setting, which forces the pipe to operate in late-z. The remaining settings control lateZ-earlyZ transitions based on accumulated screen-space area for primitives being rendered. If the accumulated area for a given primitive is exceeded, immediate transition of mode (lateZ-earlyZ) is performed ahead of rendering that primitive. The earlyZ-lateZ decision takes place when primitives are in the pipe; transient state settings when no primitives are in the pipe doesn’t cause spurious transitions to lateZ mode.</p>
<p>It’s also possible to force early Z operation, using the MandatedEarlyZ method. This is useful to ensure that the coverage mask in the shader reflects the coverage mask post-depth test.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_12"></a>
8.6.12. New feature performance</h3>
<p>This section details performance information with respect to some of the new features on Maxwell.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_12_1"></a>
8.6.12.1. FastGS</h4>
<p>The fast GS runs in the same shader stage as a regular Geometry Shader (GS), e.g. after a VS or after a TES (DS). A fast GS can only output per-primitive attributes, which means it can't modify per-vertex attributes and it can't change the connectivity (topology) of the vertices. The driver/compiler uses a fast GS in place of a regular GS when special conditions are met, as described in the best practices document. Fast GS has several performance advantages over a regular GS. One, since it does not change the topology, it maintains sharing of vertices between adjacent primitives, such as in a triangle strip which makes processing in VSC and Setup more efficient. Second, when tessellation is not being used, such that the pipeline simply consists of VS -&gt; Fast GS, then we run the Fast GS shader on the same TPC as the VS, and thus avoid the overhead of re-distribution of work and time-slicing of Alpha/Beta phases described earlier.</p>
<p>The VSC unit consumes the vertex attributes and primitive indices from the last world-pipe stage prior to the Fast GS, and combines this with the per-primitive attributes generated by the Fast GS. The VSC unit ensures that each primitive has a unique provoking vertex, and creates vertices as necessary to meet this condition, and copies the per-primitive attributes into the provoking vertex.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_12_2"></a>
8.6.12.2. Vertex Position Coordinate Swizzle and Viewport Multicast</h4>
<p>The VSC sub-unit has a new feature called viewport multicast. This allows the last world-space shader (e.g. VS, TES or GS) to specify a 16-bit mask of viewports. This mask, called Viewport Array Mask, replaces the viewport index attribute. The VSC unit will replicate the primitive for each viewport that is enabled in the mask. In addition, if the mask value is 0 for all viewports, then the primitive is culled. A related feature is vertex position coordinate swizzle, where the position attributes are swizzled based on the state for each specific viewport that a primitive is being sent to.</p>
<p>It is generally advantageous for performance to use a Fast GS to emit the viewport array mask rather than an earlier pipeline stage. This is because the VSC pipeline is optimized to process the mask at the rate of 8 primitives/clk when it is written by Fast GS, and runs at the standard rate of 1 primitive/clk otherwise. The higher rate in VSC makes culling primitives faster.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_12_3"></a>
8.6.12.3. Programmable Sample Position</h4>
<p>The location of sample positions inside a pixel are stored inside a 16 entry lookup table that can be pipelined as state. The ZROP unit know the active sample position state and evaluates and compresses zeta using plane equations evaluated at those programmed locations. In fact, for units like ZROP and Raster, sample positions are always considered programmable and are by default configured to standard locations. Readers other than ZROP (like texture) are not programmable sample position aware. In order for these clients to read a surface that has non-standard sample positions, the surface must be decompressed with a "decompress in place pass" that has ZROP read the compressed surface, evaluate the Zeta at the programmed locations, and then write the surface out uncompressed. The read and write are skipped for the already uncompressed ZROP tiles. The same procedure is also needed when sample positions are changed for a compressed zeta surface. One alternative to doing a decompress-in-place is to disable zeta compression.</p>
<p>Programmable sample positions, along with other features, are used in Maxwell Multi-frame AA (MFAA) rendering, where AA sample locations are changed per-frame and a temporal filter is used to increase the AA quality at a given performance level.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_12_4"></a>
8.6.12.4. FP16 atomics</h4>
<p>FP16 atomics are performed at the rate of one pair of atomics per clock per L2 slice. In contrast, 32-bit integer atomics run at 8 per clock per L2 slice.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_12_5"></a>
8.6.12.5. Pixel Shader Interlock</h4>
<p>Pixel Shader Interlock (PSI, also known as Raster Order View) provides a mechanism for ordered execution of fragments in the pixel shader so that UAV accesses can be ordered. It is generally recommended to use ROP whenever possible for performing operations on surfaces. ROP implements several optimizations not present in PSI, such as having a significantly lower interlock latency and compression. Note that it is allowed to use PSI on one UAV surface while continuing to use ROP on other render targets.</p>
<p>PSI restricts only one fragment at a given xy can enter the critical region of the pixel shader code at a time, in API order. Frequent conflicts can affect performance. PSI performs best when there is a large number of non-conflicting (i.e. to different screen space X,Y locations) fragments in the pipeline.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_12_6"></a>
8.6.12.6. Iterated Blend</h4>
<p>Iterated blend is a microcoded engine in CROP that allows for more complex blending equations. Each instruction takes one iteration, where each iteration uses the standard blend hardware. Therefore, a two instruction program runs at half ROP rate, three instructions at a third, etc.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_6_12_7"></a>
8.6.12.7. Target Independent Rasterization</h4>
<p>Maxwell allows the rasterization and depth test rate to be at a higher frequency than the color rate. This can be used to increase AA rate with less of an impact than full multisampling. For instance 4:1 accumulated coverage AA (ACAA) uses four Z samples/pixel and one color value, using post-Z coverage to scale the color output and accumulate into the render target.</p>
<h2><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_7"></a>
8.7. COMPUTE PIPELINE</h2>
<p>This section describes some performance characteristics of the compute pipeline, outside the SM and memory subsystem.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_7_1"></a>
8.7.1. Launch Throughput</h3>
<p>For each compute task launch, SW posts a series of methods to Host, which typically includes:</p>
<ol type="1">
<li>DMA methods to fill out the QMD</li>
<li>DMA methods to set up any other state for the task (e.g. constant buffers)</li>
<li>PCAS method to tell SKED to launch the task</li>
</ol>
<p>This in total is usually at least ~50 methods, which the Host-&gt;FE portion of the pipe can handle at about 1 method per clk.</p>
<p>If Host needs to channel switch to get to the next task, this can cause a slowdown if channel switches happen frequently and the compute workload per-switch is very low. See the Host subsection above for more details.</p>
<p>SKED can launch one task every ~50 clks or so.</p>
<p>CWD has a fixed number of task slots for actively executing tasks. This size can limit task throughput if CWD runs out of slots.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_7_2"></a>
8.7.2. Dependent Launch Latency</h3>
<p>A task with just one single-threaded CTA that just exits immediately will launch, execute, and synchronize with the next dependent task in a total of about 2000-4000 clks. Some factors that affect this latency:</p>
<ul>
<li>Type of synchronization used between tasks (e.g. semaphores, WFI, syncpoints)</li>
<li>End-of-grid membar latency (depends on scope, what’s in flight, memory system size)</li>
<li>Chip memory latency</li>
</ul>
<p>You can measure this with a program like this:</p>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;start = get_time();</div>
<div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;for (int i=0; i&lt;n; ++i)</div>
<div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;{</div>
<div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;    async_launch_grid();</div>
<div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;}</div>
<div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;wait_for_grids_to_complete();</div>
<div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;stop = get_time();</div>
<div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;overhead = (stop-start)/n;</div>
</div><!-- fragment --><p>This is typically referred to as the pipelined launch latency. Note that there can be a non-trivial latency cost for SW to post the launch to the GPU. If this latency is greater than the 2000-4000 clks for the HW to launch, execute, and sync, then the SW portion will be the long pole, and the above program will measure the SW latency instead of HW latency.</p>
<p>There is also the case where the CPU waits for completion of the grid on each iteration:</p>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;start = get_time();</div>
<div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;for (int i=0; i&lt;n; ++i)</div>
<div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;{</div>
<div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;    async_launch_grid();</div>
<div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;    wait_for_grid_to_complete();</div>
<div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;}</div>
<div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;stop = get_time();</div>
<div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;overhead = (stop-start)/n;</div>
</div><!-- fragment --><p> In this case, both the HW and SW latencies are in the critical path. This is often called the launch-join launch latency. This can be highly dependent on the SW latency, which usually dominates the HW latency.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_7_3"></a>
8.7.3. Work distribution in the compute pipeline</h3>
<p>When the compute subchannel is active, the SMs are processing CTAs. The SKED unit manages a list of up to 16 compute tasks within the CWD unit that are eligible to run. For each SM in the system, CWD chooses a task from this list to execute and sends task resource requirements to the SM. The MPC unit associated with the SM will compute the number of CTAs that the SM can execute for the task after factoring in the CTAs that are already in progress within the SM. MPC returns this "free slot" information to CWD who in turn uses it to assign CTAs to the SM. CWD will only send a CTA to an SM if the SM has the resources available to process it. Additionally, if the same task has been assigned to multiple SMs, CWD waits until it has the free slot information from all the SMs before distributing CTAs for the task. It does this in order to better balance the CTA distribution across the available SMs.</p>
<p>This causes CWD to distribute breadth first (i.e. across all SMs) when all SMs are initially idle, and then work is distributed dynamically, as work slots free up.</p>
<p>CWD can launch one CTA every two clocks. From the list of SMs that have been scheduled CTAs to process, CWD chooses the SM with the most available free slots to send the next CTA to.</p>
<h2><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_perf_TEXTURE"></a>
8.8. TEXTURE</h2>
<p>The Texture unit is designed to have a maximum throughput of four threads per clock when reading a 64-bit or smaller, 2D texture with linear blending. Table 7 and 8 summarizes speed-of-light rates for various types/instructions/formats in threads per clock per Texture pipeline.</p>
<p>TLD, TLD4, and its variants are shader instructions. TLD fetches raw (unfiltered) texture data. TLD4 fetches four scalar values from a 2x2 neighborhood of texel and returns it to the Shader. The .DC extension causes the texture unit to perform a depth compare and return the status per-texel. The .PTP extension allows the shader to supply a small per-texel offset to allow more complex neighborhood operations. See the instruction set documentation for more detail. Nearest, bilinear, and trilinear are different filtering modes.</p>
<table class="doxtable">
<tr>
<th>Format </th><th>Nearest(1)/TLD </th><th>Bilinear </th><th>Trilinear </th><th>TLD4 </th><th>TLD4.DC </th><th>TLD4.PTP <br />
 TLD4.PTP.DC  </th></tr>
<tr>
<td>RGBA8 </td><td>4 </td><td>4 </td><td>2 </td><td>4 </td><td>4 </td><td>2 </td></tr>
<tr>
<td>FP16x2 </td><td>4 </td><td>4 </td><td>2 </td><td>4 </td><td>4 </td><td>2 </td></tr>
<tr>
<td>FP16x4 </td><td>4 </td><td>4 </td><td>2 </td><td>4 </td><td>4 </td><td>2 </td></tr>
<tr>
<td>RGB10A2 </td><td>4 </td><td>4 </td><td>2 </td><td>4 </td><td>4 </td><td>2 </td></tr>
<tr>
<td>E5B9G9R9 </td><td>4 </td><td>4 </td><td>2 </td><td>4 </td><td>4 </td><td>2 </td></tr>
<tr>
<td>ASTC </td><td>4 </td><td>4 </td><td>2 </td><td>4 </td><td>4 </td><td>2 </td></tr>
<tr>
<td>BC7 (2)</td><td>4 </td><td>4 </td><td>2 </td><td>4 </td><td>4 </td><td>2 </td></tr>
<tr>
<td>DXTn </td><td>4 </td><td>4 </td><td>2 </td><td>4 </td><td>4 </td><td>2 </td></tr>
<tr>
<td>R11F_G11F_B10F </td><td>4 </td><td>4 </td><td>2 </td><td>4 </td><td>4 </td><td>2 </td></tr>
<tr>
<td>FP32x1 </td><td>4 </td><td>4 </td><td>2 </td><td>2 </td><td>4 </td><td>2 </td></tr>
<tr>
<td>BC6 (3)</td><td>2 </td><td>2 </td><td>1 </td><td>2 </td><td>2 </td><td>2 </td></tr>
<tr>
<td>FP32x2 </td><td>4 </td><td>4 </td><td>2 </td><td>2 </td><td>4 </td><td>2 </td></tr>
<tr>
<td>FP32x3 </td><td>1.33 (4)</td><td>1.33 (4)</td><td>0.667 </td><td>2 </td><td>4 </td><td>2 </td></tr>
<tr>
<td>FP32x4 </td><td>2 (5)</td><td>1 (5)</td><td>0.5 </td><td>2 </td><td>4 </td><td>2 </td></tr>
</table>
<p><b>Table 8: Texture Throughput for 1D/2D/Cubemap Textures</b></p>
<table class="doxtable">
<tr>
<th>Format </th><th>Nearest(1)/TLD </th><th>Bilinear </th><th>Trilinear  </th></tr>
<tr>
<td>RGBA8 </td><td>4 </td><td>2 </td><td>1 </td></tr>
<tr>
<td>FP16x2 </td><td>4 </td><td>2 </td><td>1 </td></tr>
<tr>
<td>FP16x4 </td><td>4 </td><td>2 </td><td>1 </td></tr>
<tr>
<td>RGB10A2 </td><td>4 </td><td>2 </td><td>1 </td></tr>
<tr>
<td>E5B9G9R9 </td><td>4 </td><td>2 </td><td>1 </td></tr>
<tr>
<td>ASTC </td><td>4 </td><td>2 </td><td>1 </td></tr>
<tr>
<td>BC7 </td><td>4 </td><td>2 </td><td>1 </td></tr>
<tr>
<td>DXTn </td><td>4 </td><td>2 </td><td>1 </td></tr>
<tr>
<td>R11F_G11F_B10F </td><td>4 </td><td>2 </td><td>1 </td></tr>
<tr>
<td>FP32x1 </td><td>4 </td><td>2 </td><td>1 </td></tr>
<tr>
<td>BC6 (3)</td><td>0.5 </td><td>0.5 </td><td>0.25 </td></tr>
<tr>
<td>FP32x2 </td><td>4 </td><td>2 </td><td>1 </td></tr>
<tr>
<td>FP32x3 </td><td>1.33 </td><td>0.667 </td><td>0.333 </td></tr>
<tr>
<td>FP32x4 </td><td>2 </td><td>0.25 </td><td>0.125 </td></tr>
</table>
<p><b>Table 9: Texture Throughput for 3D Textures</b></p>
<p>(1) Nearest blending slows to bilinear speeds for heterogeneous component data types.<br />
 (2) BC7 decompresses to 8-bit components.<br />
 (3) BC6 decompresses to 16-bit components.<br />
 (4) FP32x3 runs faster with component optimization. If only R,G are written speed increases to 8/3. If only B is written, speed increases to 4.<br />
 (5) FP32x4 doubles in speed with component optimization. If only one component is specified, or if only RG or BA is specified, component optimization is enabled.<br />
</p>
<p>Anisotropic blending slows down by a factor at most equal to the degree of anisotropy: 2x aniso runs at half speed, 4x aniso runs a quarter speed, etc. (the Texture unit only supports even degrees of anisotropy). Since anisotropic ratio is calculated dynamically, based on spacing of pixels in a quad in texture space, the anistropic ratio specified by software is the maximum anisotropic ratio.</p>
<p>ASTC decompression logic is built to decode at full throughput texels from up to two arbitrary compressed blocks for each 2x2 footprint fetch. The case of 2x2 footprint spanning across corners of four compressed blocks is decoded at half throughput. Texture pipeline is handling four pixels (threads) simultaneously and throughput of all four will be affected in case one or more pixels’ footprint spans across corners.</p>
<p>While it is possible for the FP32x3 format to achieve the maximum rate of 4/3 pixels-per-clock, in practice this format suffers from many set and/or bank conflicts in the L1 data cache. This can results in performance lower than that of FP32x4. It might be necessary to convert FP32x3 textures to FP32x4 to achieve the best performance.</p>
<p>Volume textures run at two threads per clock, maximum.</p>
<p>The texture unit supports “component optimization” for formats with components greater than 16 bits in size. If the write mask in the Texture instruction or the component swizzles in the header indicate that either the first two or last two components are not needed, the throughput increases (see footnotes in table above).</p>
<h2><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_9"></a>
8.9. COMPRESSION</h2>
<p><a class="el" href="gpu_overview__maxwell_technical_overview_index.html#gpuOverview_MaxwellTechnicalOverview_guide_highlight_FRAME_BUFFER_COMPRESSION">3.3. FRAME BUFFER COMPRESSION</a> and <a class="el" href="gpu_overview__maxwell_technical_overview_index.html#gpuOverview_MaxwellTechnicalOverview_guide_graphics_FRAME_BUFFER_COMPRESSION">5.5. FRAME BUFFER COMPRESSION</a> give background on compression. To summarize: frame buffer data can be compressed by the ROP unit for Z and color render targets. Compression generally is lossless (there are optimizations that allow lossy compression in some restricted scenarios). Compression saves bandwidth during writes and reads, but not storage, since compressed data for a tile of pixels is stored in a portion of the storage allocated for the tile in an uncompressed buffer.</p>
<p>Compression reduces traffic to DRAM and in some cases to the L2 cache. Compression improves performance when DRAM bandwidth is the limit, but may not if performance is limited by another bottleneck. Compression can also increase the effective size of the L2 cache, since a single 128 Byte cache line can store the data for 256 Bytes of buffer memory.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_9_1"></a>
8.9.1. Generating compressed data</h3>
<p>Clears and ZROP and CROP writes are the only operations that can compress data.</p>
<p><b>Clears</b> to compressible buffers are implemented by having the rasterizer iterate over coarse screen-space tiles and convey tile clear commands to the L2 cache, which updates the compression state for a 64 Bytesx16 tile in a single clock cycle. No data is written to the frame buffer during a zero-bandwidth clear operation, but graphics pipeline cycles are consumed: one 64 Bytesx16 (1KBytes) tile per clock. A fixed-size table exists on-chip mapping ZBC values for color and Z. If a clear value is needed beyond the ones written to the table, the hardware will write the clear data as compressed color and Z using the most efficient Non-ZBC compression mode selected.</p>
<p><b>ZROP</b> writes fully-covered tiles of Z data in a single-clock 32 Bytes compressed write. For more complex tiles with more than one surface visible, ZROP has a processing pipeline whose throughput is proportional to the number of planes of Z data that must be processed. Two zplanes pack in each 32 Bytes data sector. ZROP supports Z compression formats with: 1 32 Bytes sector (1-2 zplanes visible in the tile), 2 32 Bytes sectors (3-4 zplanes visible in the tile). If more than 4 zplanes are visible, ZROP expands to samples and operates at the sample-limited rate.</p>
<p><b>CROP</b> can write fully-covered tiles of color data in compressed form. The data may be:</p>
<ul>
<li>8:1 compressed (one 32 Bytes sector conveys the data for a 32 Bytesx8 tile)</li>
<li>4:1 compressed (two 32 Bytes sectors convey the data for a 32 Bytesx8 tile)</li>
<li>2:1 compressed (four 32 Bytes sectors convey the data for a 32 Bytesx8 tile)</li>
</ul>
<p>Tiles that do not compress are written uncompressed (eight 32 Bytes sectors for a 32 Bytesx8 tile). However, when writing uncompressed, if the tile in the frame buffer is uncompressed, only sectors with coverage need to be written. If a partially covered write touches a compressed tile, CROP (or the L2 for naïve clients) reads the compressed data, decompresses it, and merges the new data with the decompressed data, then writes out the full tile of uncompressed data. The performance consequences of these read-modify-write expands is discussed shortly.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_9_2"></a>
8.9.2. GPU compression-aware clients</h3>
<p>When a read is issued to a compressed tile of data, only the compressed sectors are returned and cached in the L2 cache. For the following "compression-aware" clients on the GPU, the GPU L2 returns the data compressed:</p>
<p><b>ZROP and CROP</b>: ZROP and CROP can read and operate on compressed data directly, at a higher rate, and are the only clients that can write compressed data to the L2. See the ROP section for more detail.</p>
<p><b>Texture</b>: Texture can accept reduced data from the L2, expanding it into the L1. ZBC and arithmetically compressed data still needs to be decompressed in the L2. By sending reduced data to the texture unit, XBAR bandwidth is reduced for MSAA buffers, greatly improving the performance of workloads such as MSAA resolves.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_9_3"></a>
8.9.3. Naive clients</h3>
<p>Clients that are unaware of compression are called naive clients. Naive clients on the GPU can benefit from compression because reads of compressed data by the L2 only incur the bandwidth costs of reading the compressed data. The GPU L2 cache decompresses the data before returning to the client, which are unaware that the data was compressed. Note that although texture does some of its own decompression, it also leverages the L2 decompressor to decompress ZBC and arithmetically compressed data.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_9_4"></a>
8.9.4. Color compression formats</h3>
<p>Maxwell supports multiple compression formats suited to the type of data stored in the buffer. This section will describe the subset of formats allocated by the driver and resource manager.</p>
<p>Compression is only supported for block linear buffers. When a tile of a color buffer is uncompressed, it has the same representation or packing as an uncompressible block linear buffer.</p>
<p>For compressible color, the following compressible states are supported:</p>
<ul>
<li>Uncompressed</li>
<li>Zero Bandwidth Cleared</li>
<li>4:1 sample reduction (groups of 4 samples share the same color)</li>
<li>8:1 sample reduction (groups of 8 samples share the same color)</li>
<li>2:1 arithmetic compression</li>
<li>Both 4:1 reduction and 2:1 arithmetic compression, yielding 8:1 compression</li>
</ul>
<p>Color compression is supported for four component, 32 bit, 64 and 128 bit pixel depths, with 1xAA, 2xAA, 4xAA and 8xAA multisample formats.</p>
<p>There are 2 compress bits per 256 Bytes tile, allowing four encodings of the compression state. State 0 always indicates uncompressed. Therefore, only three of the compressed states above are supported to a given render target.</p>
<p>The following table lists the compressible formats used by NVIDIA software:</p>
<table class="doxtable">
<tr>
<th>Bit Depth,<br />
 per pixel </th><th>MSAA mode </th><th>Uncompressed </th><th>ZBC </th><th>4:1 Reduce </th><th>8:1 Reduce </th><th>Arithmetic </th><th>Both <br />
 (4:1 Reduce + Arithmetic)  </th></tr>
<tr>
<td>32, 64 </td><td>NoAA </td><td>Y </td><td>Y </td><td>Y </td><td>N </td><td>Y </td><td>N </td></tr>
<tr>
<td>32, 64 </td><td>2xAA </td><td>Y </td><td>Y </td><td>Y </td><td>N </td><td>Y </td><td>N </td></tr>
<tr>
<td>32, 64 </td><td>4xAA </td><td>Y </td><td>Y </td><td>Y </td><td>N </td><td>Y(1) </td><td>Y(1) </td></tr>
<tr>
<td>32, 64 </td><td>8xAA </td><td>Y </td><td>Y </td><td>N </td><td>Y </td><td>Y </td><td>N </td></tr>
<tr>
<td>128 </td><td>NoAA </td><td>Y </td><td>Y </td><td>Y </td><td>N </td><td>N </td><td>N </td></tr>
</table>
<p><b>Table 10: Available color compression states</b></p>
<p>(1) depending on the API, either Both or Arithmetic is used for 4xAA. DX uses Both, NVN uses Arithmetic. Software can decide to use any format in the future.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_9_5"></a>
8.9.5. Depth compression formats</h3>
<p>As with color, Maxwell supports multiple compression formats suited to the type of data stored in the buffer. All depth kinds specify the depth/stencil format and multisample mode. Each depth/stencil format has an uncompressed kind, whose layout matched the uncompressed state for a block in a compressed buffer.</p>
<p>As with color, there are 2 compress bits per 256 Bytes. Depth compression is done on tiles of 8x8 samples, so the number of compress bits per depth tile varies with the pixel depth:</p>
<ul>
<li><b>16-bit depth formats</b> have 1 compress bit per 8x8 sample tile (128B), allowing only one compressed encoding per tile, which can be zero bandwidth clear, or 8:1 depth. Current software uses both formats.</li>
<li><b>32-bit depth formats</b> have 2 compress bits per 8x8 sample tile, allowing 4 encodings.</li>
<li><b>64-bit depth formats</b> have 4 compress bits per 8x8 sample tile, in principle allowing 16 encodings, but we only use the four associated with the 32-bit Z values.</li>
</ul>
<p>For 32 and 64-bit depth formats, the four possible compression states are:</p>
<ul>
<li>Uncompressed</li>
<li>Zero Bandwidth Cleared</li>
<li>8:1 compression: Two Z planes stored in one 32 Bytes sector</li>
<li>4:1 compression: Four Z planes stored in two 32 Bytes sector</li>
</ul>
<p>Software supports compression for aliased, 2xAA, 4xAA and 8xAA depth surfaces.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_8_9_6"></a>
8.9.6. Dst reduce optimization (lossy compression)</h3>
<p>Color reduction is a compression scheme that conditionally compresses fully-covered tiles of samples as they are written from CROP to the L2 cache. Compression is achieved by storing a single value per pixel-the average color of the samples that contribute to a pixel-rather than storing every sample. In order for this to be done, all samples contributing to a pixel must have color values that lie within a configurable threshold of each other. If the threshold is larger than zero, compression may be lossy. Maxwell supports two thresholds: a low threshold is used when a heuristic suggests the tile contains a silhouette edge; a larger, more aggressive threshold is used when the heuristic indicates the tile contains no silhouette edges. Support for both thresholds improves compression for interior pixels while preserving high quality anti-aliased edges.</p>
<p>The heuristic works as follows: A fully covered tile where each sample of each pixel is touched exactly once is assumed not to contain silhouette edges. A fully covered tile where samples are touched different numbers of times is assumed to contain a silhouette edge.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_perf_read_modify_write"></a>
8.9.7. Read-modify-write expands</h3>
<p>When a tile is compressed and a write only partially covers the tile, the tile may need to be decompressed.</p>
<p>CROP and ZROP both handle this case internally. In such cases, they fetch the compressed data, merge the partial update, and attempt to recompress. If successful, the updated tile is written compressed. If unsuccessful, the tile is written uncompressed.</p>
<p>When a naive client updates a compressed tile, the L2 must do a read-modify-expand. It schedules a read of the compressed data, decompresses it, merges the updates to the tile, then marks the tile as dirty.</p>
<p>The net bandwidth consumed for a read-modify-write expand can exceed the bandwidth of updating an uncompressed buffer, even when the initial write of the uncompressed buffer is considered. For example, consider a write of two 32 Bytes sectors to a 4:1 compressed tile:</p>
<table class="doxtable">
<tr>
<th>Compressed Bandwidth </th><th>Uncompressed Bandwidth  </th></tr>
<tr>
<td>64 Bytes (Write tile 4:1 compressed) </td><td>256 Bytes (Write tile uncompressed) </td></tr>
<tr>
<td>64 Bytes (Read compressed tile) </td><td>N/A </td></tr>
<tr>
<td>256 Bytes (Write expanded tile) </td><td>64 Bytes (Update touched sectors) </td></tr>
<tr>
<td>384 Bytes (Total) </td><td>320 Bytes (Total) </td></tr>
</table>
<h1><a class="anchor" id="gpuOverview_MaxwellTechnicalOverview_guide_sec_9"></a>
9. GLOSSARY OF TERMS</h1>
<p>This section lists all abbreviations and nvidia-specific terms used in this document for easy reference.</p>
<p><b>AOS</b> Array of Structures.</p>
<p><b>CBE</b> Circular Buffer Entry. Used to pass data between shader stages, possibly on different SMs.</p>
<p><b>CE</b> Copy Engine.</p>
<p><b>COMPUTE</b> Abbreviation for GPU Compute, also called GPGPU.</p>
<p><b>CROP</b> Color ROP. Part of the ROP unit. Performs blending and color write.</p>
<p><b>CTA</b> cooperative thread array, also known as a Thread block.</p>
<p><b>CWD</b> Compute work distributor. Distributes Thread blocks to SMs.</p>
<p><b>ENGINE</b> A group of units that HOST sends a command stream. Examples are the Graphics engine, async copy engines, display video encode and decode. This document focuses exclusively on the graphics engine.</p>
<p><b>FBP</b> Frame Buffer Partition. Includes the L2, ROP, and on discrete GPU, the DRAM controller.</p>
<p><b>FB</b> Discrete GPU term for the DRAM controller for video memory.</p>
<p><b>GPC</b> GPU Processing Cluster. Scalable block including most fixed function graphics units, and multiple TPCs.</p>
<p><b>GPGPU</b> General Purpose GPU programming. Also called "GPU Compute”
and "Compute".</p>
<p><b>GP_PUT</b> Pointer value in host, specifying the last location written in the command stream.</p>
<p><b>GP_GET</b> Pointer value in host, specifying the next position host will read from the command stream.</p>
<p><b>GPU ENGINE</b> Also called the Graphics Engine. The group of units that run graphics streams, compute streams, and RSTR2D stream.</p>
<p><b>GRID</b> In GPU compute, a Grid is a block of threads that all run the same program.</p>
<p><b>GS</b> Geometry Shader.</p>
<p><b>HOST</b> Work scheduling unit in the GPU. Command streams are written and the host select the order the streams are run and dispatches work to the proper engines.</p>
<p><b>HUB</b> Top level block that contains the non-scalable units, including HOST, PD and CWD.</p>
<p><b>Instance Block</b> Region of memory that contains per-context information, including saved state.</p>
<p><b>ISBE</b> Inter-stage buffer entry. A structure stored in shared memory that passes data between PE and SM and between shader stages on the same SM (VS to TCS and TES to GS).</p>
<p><b>L1</b> also called TEXL1, the L1 cache for SM data, including tex commands, local and global reads.</p>
<p><b>L2</b> Level 2 cache. Unified cache for all memory accesses. Includes decompression hardware for non-ROP clients and caching of compressed status bits.</p>
<p><b>MPC</b> Modular Pipe Controller. Allocates resources and Launches work to a given SM.</p>
<p><b>PD</b> Primitive distributor. In Hub, distributes initial geometry work to SMs.</p>
<p><b>PDB</b> primitive distributor “B”. distributes tessellated work and work for the GS to SMs.</p>
<p><b>PE</b> Primitive engine. A collection of fixed function units that work on geometry data. PE is divided into to parts: A PEL, which contains units allocated per-TPC, and a PES unit that contains units shared with 2 TPCs.</p>
<p><b>PEL</b> Primitive Engine Local. A per-TPC block that contains the VAF unit and STRI.</p>
<p><b>PES</b> Primitive Engine Shared. A unit shared by tpcs, that performs tessellation and VSC.</p>
<p><b>PROP</b> Pre-rop. The unit that does address calculation and sends pixel work to TPCs and ROPs.</p>
<p><b>PS</b> Pixel Shader.</p>
<p><b>QMD</b> Queue Meta Data. The record in DRAM that describes a compute grid, or task. Used by CWD to launch work to the TPCs.</p>
<p><b>ROP</b> Raster OP unit. Performs read-modify write graphics operations based on the x, y address. Comprised of the CROP and ZROP.</p>
<p><b>RSTR2D</b> Rasterizer for the 2d class. Performs blits, fills, format converts and other 2d operations.</p>
<p><b>SCC</b> State cache controller. Portion of PD that handles state and constant versioning. For the graphics pipeline.</p>
<p><b>SM</b> Streaming Multiprocessor. The programmable shader core for the GPU.</p>
<p><b>SOA</b> Structure of Arrays.</p>
<p><b>STRI</b> Shader TRI. A portion of PE that calculates plane equations for attributes for the pixel shader. Results are stored in the TRAM area of shared memory.</p>
<p><b>TCS</b> Tessellation control shader. First shader in the tessellation pipeline. Also called the Hull shader by some APIs.</p>
<p><b>TES</b> Tesselation evaluation shader. Second shader in the tessellation pipe. Also called the Domain shader by some APIs.</p>
<p><b>TEX</b> Texture unit. Two per TPC.</p>
<p><b>TEXL1</b> Texture L1, also called just the L1. The cache in the texture pipeline where all non-shared memory accesses go through.</p>
<p><b>Thread Block</b> GPU compute term, referring to a group of threads in a grid that run on the same SM and can communicate with each other through global and shared memory. Also called a CTA.</p>
<p><b>TPC</b> Texture Processing cluster. A unit of scalability within the GPC that contains the SM, MPC, PEL and two TEX units.</p>
<p><b>TRAM</b> Triangle RAM. A region of shared memory where per-attribute, per-primitive plane equations are stored for per-pixel evaluation.</p>
<p><b>USERD</b> Portion of instance block used by HOST to store the current state of an instance’s command stream.</p>
<p><b>VAF</b> Vertex attribute fetch. Portion of PE that fetches and formats input vertex data for use by the VS.</p>
<p><b>VIC</b> Video Image Compositor. A unit in NX outside the GPU that performs compositing operations for the video pipeline.</p>
<p><b>VS</b> Vertex shader. First shader in the graphics pipeline.</p>
<p><b>VSC</b> Viewport, Stream output, Culling and Clipping unit. Last fixed function unit in the graphics primitive pipe before sending data to tiler and setup. Part of the PES unit.</p>
<p><b>WARP</b> A group of threads that are executed in parallel, in SIMT fashion, by the SM. Maxwell’s warp size is up to 32 thread.</p>
<p><b>WDX</b> Work distribution crossbar. In graphics, distributes work from the world space pipe to the pixel pipe. Includes the Binning block used in tiled caching.</p>
<p><b>XBAR</b> Crossbar datapaths used to communicate from Hub to GPCs, GPCs to FBs, and Hub to FBPs.</p>
<p><b>ZROP</b> The portion of the ROP that performs the Z and Stencil read, compare and conditional write. </p>
</div></div><!-- contents -->
<!-- HTML footer for doxygen 1.8.8-->
<!-- start footer part -->
</body>
</html>
