<!-- HTML header for doxygen 1.8.8-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.10"/>
<title>NintendoSDK API Reference: Maxwell Best Practices</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="openclose.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { init_search(); });
</script>
<link href="stylesheet.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">NintendoSDK API Reference
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- 構築: Doxygen 1.8.10 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'検索');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>総合概要</span></a></li>
      <li class="current"><a href="pages.html"><span>諸情報</span></a></li>
      <li><a href="modules.html"><span>モジュール</span></a></li>
      <li><a href="namespaces.html"><span>名前空間</span></a></li>
      <li><a href="annotated.html"><span>クラス</span></a></li>
      <li><a href="files.html"><span>ファイル</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="検索" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="_page_graphics_for_n_x.html">グラフィックス for NX</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Maxwell Best Practices </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>目次</h3>
<ul><li class="level1"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_1">1. Overview</a></li>
<li class="level1"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2">2. Graphics</a><ul><li class="level2"><a href="#gpuOverview_MaxwellBestPractices_earlyZ">2.1. EarlyZ</a><ul><li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_EarlyZRequirements">2.1.1. EarlyZ Requirements</a><ul><li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_1_1_1">2.1.1.1. Global Conditions</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_1_1_2">2.1.1.2. Local Conditions</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_1_1_3">2.1.1.3. EarlyZ Override</a></li>
</ul>
</li>
</ul>
</li>
<li class="level2"><a href="#gpuOverview_MaxwellBestPractices_zcull">2.2. ZCull</a><ul><li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_ZCullTest">2.2.1. Per-Tile Depth Testing and Culling</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_2_2">2.2.2. ZCull Buffer Updates</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_2_3">2.2.3. Performance Considerations</a></li>
</ul>
</li>
<li class="level2"><a href="#gpuOverview_MaxwellBestPractices_fastZ">2.3. FastZStencil</a></li>
<li class="level2"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_4">2.4. Frame Buffer Compression</a></li>
<li class="level2"><a href="#gpuOverview_MaxwellBestPractices_fastClear">2.5. Fast Clear (Zero Bandwidth Clear)</a></li>
<li class="level2"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_6">2.6. Fast Geometry Shader</a></li>
<li class="level2"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_7">2.7. Shader</a><ul><li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_7_1">2.7.1. General Suggestions</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_7_2">2.7.2. Half Float (FP16) Operations</a><ul><li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_7_2_1">2.7.2.1. Benefits of FP16 Operations</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_7_2_2">2.7.2.2. HW and Instructions with FP16 Support</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_7_2_3">2.7.2.3. FP16 Limitations</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_7_2_4">2.7.2.4. Guideline on FP16 Usage in Shaders</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_7_2_5">2.7.2.5. Expected Peformance Gain</a></li>
</ul>
</li>
<li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_StaticAnalysis">2.7.3. Static Analysis</a><ul><li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_7_3_1">2.7.3.1. Precautions</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_WarpLimiters">2.7.3.2. Warp Throughput Limiters</a></li>
</ul>
</li>
</ul>
</li>
<li class="level2"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_8">2.8. Vertex Processing</a><ul><li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_8_1">2.8.1. Optimize your index-buffer</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_8_2">2.8.2. Optimize Vertex-arrays fetches</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_8_3">2.8.3. Optimize world space to pixel space data transfer</a></li>
</ul>
</li>
<li class="level2"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_9">2.9. Data Copies</a></li>
<li class="level2"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_10">2.10. Texturing</a><ul><li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_10_1">2.10.1. General Suggestions</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_TextureCopies">2.10.2. Texture Copies</a><ul><li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_10_2_1">2.10.2.1. Texture Copy with 3D Pipeline</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_10_2_2">2.10.2.2. Texture Copy with RSTR2D Unit</a></li>
<li class="level4"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_2_10_2_3">2.10.2.3. Additional Performance Considerations</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="level1"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_3">3. Compute</a><ul><li class="level2"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_3_1">3.1. Interaction with Graphics Workload</a></li>
<li class="level2"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_3_2">3.2. Shaders</a><ul><li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_3_2_1">3.2.1. UBO</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_3_2_2">3.2.2. SSBO</a></li>
<li class="level3"><a href="#gpuOverview_MaxwellBestPractices_guide_sec_3_2_3">3.2.3. Shared Memory</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><h1><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_1"></a>
1. Overview</h1>
<p>This document contains suggestions on how to optimize graphics and compute workloads for the Maxwell GPU architecture as used in the NX SOC. It contains highlights of hardware features and suggestions on how best to write applications to maximize performance. For a detailed overview of the GPU architecture please refer to <a class="el" href="gpu_overview__maxwell_technical_overview_index.html">Maxwell Technical Overview</a>.</p>
<h1><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2"></a>
2. Graphics</h1>
<h2><a class="anchor" id="gpuOverview_MaxwellBestPractices_earlyZ"></a>
2.1. EarlyZ</h2>
<p>In NX GPU, the read-test-write of the Z-buffer is always done only once in one of two places within the graphics pipeline:</p>
<ul>
<li>If EarlyZ is enabled, the read-test-write of the Z-buffer occurs before the fragment shader execution by the SM.</li>
<li>If EarlyZ is disabled (LateZ mode), the read-test-write of the Z-buffer occurs after the fragment shader execution by the SM.</li>
</ul>
<p>The EarlyZ mode can have a positive impact on performance by culling fragments earlier in the pipeline, saving the SM from processing fragments that would not be visible anyway.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_EarlyZRequirements"></a>
2.1.1. EarlyZ Requirements</h3>
<p>Because EarlyZ mode determines the final value of each fragment's Z value before the fragment shader stage it can only be enabled if no subsequent stage in the pipeline could affect the Z or coverage of a fragment.</p>
<p>Refer to the tables below for conditions that enable and disable EarlyZ mode.</p>
<p>The conditions are divided into 2 groups:</p><ul>
<li>Global conditions which always disable EarlyZ if not satisfied.</li>
<li>Local conditions which can disable EarlyZ only when depth and/or stencil surfaces can be modified by the draw call.</li>
</ul>
<p>The method to check if EarlyZ is enabled for a draw call is the following:</p><ul>
<li>If any global condition is not satisfied then EarlyZ is disabled.</li>
<li>If all global conditions are satisfied and the draw call can not modify depth and/or stencil surfaces, then the local conditions can be ignored and EarlyZ is enabled.</li>
<li>If all global conditions are satisfied and the draw call can modify depth and/or stencil surfaces, then EarlyZ is enabled only if all local conditions are satisfied.</li>
</ul>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_1_1_1"></a>
2.1.1.1. Global Conditions</h4>
<p>All conditions in the table below must be satisfied to enable EarlyZ.</p>
<table class="doxtable">
<tr>
<th>Operation </th><th>EarlyZ  </th></tr>
<tr>
<td>Fragment shader does not modify depth </td><td>enabled </td></tr>
<tr>
<td>Fragment shader does not modify SSBO </td><td>enabled </td></tr>
<tr>
<td>Fragment shader does not call imageStore() or imageAtomic*() </td><td>enabled </td></tr>
</table>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_1_1_2"></a>
2.1.1.2. Local Conditions</h4>
<p>If the draw call can not modify depth and/or stencil values, the local conditions in the table below can be ignored. Otherwise all conditions below must be satisfied to enable EarlyZ.</p>
<p>A draw call cannot modify depth value if at least one of the following is true:</p><ul>
<li>No depth render target is bound.</li>
<li>Depth test is disabled (When depth test is disabled, writes to depth are also disabled).</li>
<li>Writes to depth buffer is disabled.</li>
<li>Depth function is set to EQUAL.</li>
</ul>
<p>A draw call cannot modify stencil value if at least one of the following is true:</p><ul>
<li>No stencil render target is bound.</li>
<li>Stencil test is disabled.</li>
<li>Stencil mask bits are all 0s for front and back stencil tests.</li>
<li>Any combination of stencil func and stencil ops for front and back stencil tests which do not result in modifying any stencil values.</li>
</ul>
<table class="doxtable">
<tr>
<th>Operation </th><th>EarlyZ  </th></tr>
<tr>
<td>Alpha test disabled </td><td>enabled </td></tr>
<tr>
<td>Alpha to coverage disabled </td><td>enabled </td></tr>
<tr>
<td>Fragment shader does not modify gl_SampleMask </td><td>enabled </td></tr>
<tr>
<td>Fragment shader does not contain discards </td><td>enabled </td></tr>
</table>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_1_1_3"></a>
2.1.1.3. EarlyZ Override</h4>
<p>Using the <em>early_fragment_tests</em> layout qualifier in the fragment shader can also be used to force EarlyZ mode. If a fragment shader declares the <em>early_fragment_tests</em> qualifer, both global and local conditions above will be ignored and EarlyZ mode will always be enabled. For details please refer to the OpenGL extension ARB_shader_image_load_store.</p>
<h2><a class="anchor" id="gpuOverview_MaxwellBestPractices_zcull"></a>
2.2. ZCull</h2>
<p>The ZCull unit operates during rasterization in between the coarse raster and fine raster phase. Using conservative depth tests it trivially accepts or rejects/culls the 16x16 pixel tiles generated by the coarse raster unit.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_ZCullTest"></a>
2.2.1. Per-Tile Depth Testing and Culling</h3>
<p>ZCull maintains an occluder map (ZCull buffer) in memory to store the minimum or maximum Z values for each tile location depending on the current ZCull depth direction set by the driver.</p>
<p>Based on this depth direction, ZCull will trivially accept or conservatively reject incoming pixels from raster, potentially saving further stages in the graphics pipeline from doing additional work.</p>
<p>In NVN the ZCull depth direction command is generated by the <a class="el" href="group__nvn__c__functions.html#gae5f7adad6751d801a89cd1101253fa85" title="Clear a depth/stencil buffer. ">nvnCommandBufferClearDepthStencil()</a> function if the depthMask parameter is true:</p>
<ul>
<li>If the depthValue parameter is greater or equal to 0.5 the ZCull depth direction is set to LESS.</li>
<li>If the depthValue parameter is less than 0.5 the ZCull depth direction is set to GREATER.</li>
</ul>
<p>Everytime the command to set ZCull direction is executed, the entire ZCull buffer is invalidated.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_2_2"></a>
2.2.2. ZCull Buffer Updates</h3>
<p>The ZCull buffer is updated when the ZROP unit does the final depth test of per-fragment Z value. This update occurs at the same time as updates to the Z buffer value.</p>
<p>This update operation occurs at different locations in the graphics pipeline depending on whether EarlyZ is active or not:</p>
<ul>
<li>If EarlyZ is active, the ZROP operation occurs before the fragment is processed by the fragment shader.</li>
<li>If EarlyZ is not active (LateZ), the ZROP operation occurs after the fragment is processed by the fragment shader.</li>
</ul>
<h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_2_3"></a>
2.2.3. Performance Considerations</h3>
<p>ZCull's performance can be reduced or disabled entirely by the following factors:</p>
<ul>
<li>Changing ZCull depth direction or switching render targets causes the entire ZCull buffer to be invalidated, effectively losing all accumulated depth information.</li>
<li>ZCull will be less effective if the ZCull buffer contains information that is not up to date. This is much more likely to happen when EarlyZ is not enabled (LateZ mode).</li>
<li>ZCull can be ineffective (unable to cull) if the ZCull depth direction does not match the currently set depth compare function.</li>
</ul>
<p>The following table summarizes different conditions and their impact on ZCull performance.</p>
<table class="doxtable">
<tr>
<th></th><th>Buffer Invalidated </th><th>Reduced Performance </th><th>Unable to cull  </th></tr>
<tr>
<td>ZCull-GFX Depth Direction Mismatch </td><td>No </td><td>No </td><td>Yes </td></tr>
<tr>
<td>EarlyZ Not Enabled </td><td>No </td><td>Yes </td><td>No </td></tr>
<tr>
<td>ZCull Depth Direction Changed </td><td>Yes </td><td>No </td><td>No </td></tr>
<tr>
<td>Render Target Changed </td><td>Yes </td><td>No </td><td>No </td></tr>
</table>
<p>For details on how the ZCull depth direction is set refer to <a class="el" href="gpu_overview__maxwell_best_practices_index.html#gpuOverview_MaxwellBestPractices_guide_ZCullTest">2.2.1. Per-Tile Depth Testing and Culling</a>.</p>
<p>For conditions on when EarlyZ is enabled please refer <a class="el" href="gpu_overview__maxwell_best_practices_index.html#gpuOverview_MaxwellBestPractices_guide_EarlyZRequirements">2.1.1. EarlyZ Requirements</a>.</p>
<p>When switching render targets it is possible to save the current ZCull buffer for future restore using the NVN ZCull save and restore functions. Please refer to the NVN programming guide section "ZCull
Save and Restore" for a detailed description.</p>
<h2><a class="anchor" id="gpuOverview_MaxwellBestPractices_fastZ"></a>
2.3. FastZStencil</h2>
<p>Maxwell GPU has a fast 4x performance mode when rendering only writes to the Z and stencil buffer.</p>
<p>Refer to the table below for enabling and disabling conditions of FastZStencil mode. Please note that any single "disabled" condition is enough to turn off FastZStencil mode. On the other hand, although an "enabled" condition will not disable FastZStencil, it is still possible that something else will.</p>
<table class="doxtable">
<tr>
<th>Condition </th><th>FastZStencil  </th></tr>
<tr>
<td>EarlyZ disabled </td><td>disabled </td></tr>
<tr>
<td>EarlyZ enabled </td><td>enabled </td></tr>
<tr>
<td>Shader writes color </td><td>disabled </td></tr>
<tr>
<td>Shader does not write color </td><td>enabled </td></tr>
</table>
<p>Disabling shader color writes can be done in several ways:</p>
<ul>
<li>Binding NULL fragment shader</li>
<li>Binding NULL for color targets</li>
<li>Disable color channels</li>
</ul>
<p>For conditions necessary for enabling EarlyZ please refer to <a class="el" href="gpu_overview__maxwell_best_practices_index.html#gpuOverview_MaxwellBestPractices_guide_EarlyZRequirements">2.1.1. EarlyZ Requirements</a>.</p>
<h2><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_4"></a>
2.4. Frame Buffer Compression</h2>
<p>Maxwell GPU in NX SOC supports frame buffer compression which can significantly reduce pressure on memory bandwidth. This feature is automatically enabled for all supported formats so the only thing application needs to be aware of is to select the correct format.</p>
<p>The list of compressible surface formats will be added to the NVN programming guide.</p>
<h2><a class="anchor" id="gpuOverview_MaxwellBestPractices_fastClear"></a>
2.5. Fast Clear (Zero Bandwidth Clear)</h2>
<p>Maxwell GPU in NX SOC supports a special frame buffer compression format which can be cleared with almost zero memory bandwidth requirement. One restriction is that Fast Clears can only be done when the clear color has been registered to the driver.</p>
<p>For further details on how to register the clear color please refer to the NVN programming guide document on the function <a class="el" href="group__nvn__c__functions.html#gace7813b0e44d122d452df918525ef8a1" title="Register a fast clear color value. ">nvnDeviceRegisterFastClearColor()</a>.</p>
<p>The table below summarizes the conditions that could enable/disable Fast Clear operations. Please note that any single "disabled" condition is enough to turn off Fast Clear. On the other hand, although an "enabled" condition will not disable Fast Clear, it is possible that something else will.</p>
<table class="doxtable">
<tr>
<th>Condition </th><th>Fast Clear  </th></tr>
<tr>
<td>Surface format is compressible </td><td>Enabled </td></tr>
<tr>
<td>Surface format is not compressible </td><td>Disabled </td></tr>
<tr>
<td>Clear color is registered </td><td>Enabled </td></tr>
<tr>
<td>Clear color is not registerd </td><td>Disabled </td></tr>
</table>
<h2><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_6"></a>
2.6. Fast Geometry Shader</h2>
<p>Maxwell GPU contains the following geometry shader features which can be used together to improve performance for certain types of rendering tasks:</p>
<ul>
<li>High performance geometry shader when certain conditions are met.</li>
<li>Render a primitive to multiple layers, each with its own unique viewport (multicast).</li>
<li>Swizzling of vertex position members for each target surface.</li>
</ul>
<p>These features can be used to accelerate certain types of rendering tasks such as cubemap rendering, cascaded shadow mapping, and multi-resolution rendering.</p>
<p>The table below summarizes the conditions that could enable/disable fast geometry shader. Please note that any single "disabled" condition is enough to turn off the feature. On the other hand, although an "enabled" condition will not disable it, it is possible that something else will.</p>
<table class="doxtable">
<tr>
<th>Condition </th><th>Fast GS  </th></tr>
<tr>
<td>Geometry shader writes vertex attributes </td><td>disabled </td></tr>
<tr>
<td>Geometry shader does not write vertex attributes </td><td>enabled </td></tr>
<tr>
<td>Geometry shader writes primitive attributes </td><td>enabled </td></tr>
<tr>
<td>Geometry shader emits new vertices </td><td>disabled </td></tr>
<tr>
<td>Geometry shader does not emit new vertices </td><td>enabled </td></tr>
</table>
<p>Please refer to the following OpenGL extensions also for details on how to enable FastGS and viewport multicast:</p>
<ul>
<li>NV_geometry_shader_passthrough for details and examples on FastGS enabled geometry shaders.</li>
<li>NV_viewport_array2 for details and examples on viewport multicast.</li>
<li>Example for vertex position swizzling will be included in the upcoming GameWork CubemapRendering sample.</li>
</ul>
<h2><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_7"></a>
2.7. Shader</h2>
<h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_7_1"></a>
2.7.1. General Suggestions</h3>
<p>Shader program array access performance differs depending on the divergence of the access patterns. High divergence access is when shader calculation of fragments occupying adjacent pixels have a high probability of reading different locations. Low divergence access is when the calculations have a high probability of accessing the same location. Recommend the following storage types for different access patterns:</p>
<ul>
<li>Data with low divergent or non-divergent access patterns should use UBOs.</li>
<li>Data with highly divergent access patterns should use textures or SSBOs.</li>
</ul>
<p>Writes to SSBO should be done in a sequential manner if possible.</p>
<p>Avoid passing data that have constant values as varyings to minimize attribute bandwidth pressure.</p>
<p>Avoid frequent switching between graphics and compute shaders. This causes the GPU to execute expensive context switches. Recommend batching graphics shader draw calls and compute shader dispatch calls as much as possible.</p>
<p>Avoid accessing arrays with variable indexes as this can reduce performance. For example, the following is not recommended: </p><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;int index = ... ;</div>
<div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;...</div>
<div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;float foo = array[index];</div>
</div><!-- fragment --><p> Instead when possible use a constant index: </p><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;float foo = array[2];</div>
</div><!-- fragment --><p> Variable indexed arrays can cause the following to occur:</p><ul>
<li>The compiler may store the array in local memory instead of registers which would cause loads and stores to be much slower.</li>
<li>If the value of the array is used as a conditional variable this may cause performance reducing thread divergence in the warp execution when actually not necessary.</li>
</ul>
<p>In addition variable indexed arrays cannot be used for shader specialization.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_7_2"></a>
2.7.2. Half Float (FP16) Operations</h3>
<p>This section contains additional information on FP16 mode as described in <a class="el" href="gpu_overview__maxwell_technical_overview_index.html#gpuOverview_MaxwellTechnicalOverview_guide_performance_FP16">8.2.3.4. FP16 support</a>. The following subsections will provide further details on:</p><ul>
<li>What are the potential benefits of FP16 mode.</li>
<li>Which HW and instructions support FP16 operations and what are the potential benefits for each.</li>
<li>What are the various HW and compiler limitations of FP16 operations in NX.</li>
<li>How to enable FP16 operations in shaders and guidelines on how to maximize their benefits.</li>
<li>Expected performance improvement that can be gained by converting shader to use FP16 operations based on past experience along with some shader examples.</li>
</ul>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_7_2_1"></a>
2.7.2.1. Benefits of FP16 Operations</h4>
<p>Using FP16 operations in NX may lead to the following benefits:</p>
<ul>
<li>Vectorization of FP16 operations when applicable. As detailed in <a class="el" href="gpu_overview__maxwell_technical_overview_index.html#gpuOverview_MaxwellTechnicalOverview_guide_performance_FP16">8.2.3.4. FP16 support</a>, the NX SM unit is capable of combining 2 FP16 operations into a single instruction. Under ideal conditions this may lead to FP16 operations having double the peak performance of FP32 instructions as shown in the "SM FMAs/clk (fp16)" entry of table 3 in the section <a class="el" href="gpu_overview__maxwell_technical_overview_index.html#gpuOverview_MaxwellTechnicalOverview_guide_performance">8.1. PERFORMANCE SUMMARY</a>. However only certain FP16 instructions support vectorization and those instructions may not be vectorized under certain conditions. Please refer to sections below for details.</li>
<li>Reduced register usage. Because two FP16 values can be packed into a single FP32 register, FP16 operations requires only half the number of registers of its FP32 counterpart. This reduction may lead to increased performance by increasing warp occupancy (the number of warps that can be simultaneously active within the same SM sub-partition), which in turn can lead to better latency hiding of high latency operations such as texture fetches. Reduced register usage may also help compiler to better optimize shaders in various ways. Please refer to <a class="el" href="gpu_overview__maxwell_best_practices_index.html#gpuOverview_MaxwellBestPractices_guide_StaticAnalysis">2.7.3. Static Analysis</a> for more details on how register limitation and warp occupancy can affect performance.</li>
</ul>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_7_2_2"></a>
2.7.2.2. HW and Instructions with FP16 Support</h4>
<p>NX supports FP16 operations with the possibility of vectorizing some operations under certain conditions. Discrete GPUs earlier than the Pascal generation however, do not support FP16 operations. On these discrete GPUs shader FP16 operations are emulated using FP32 instructions and will not gain the benefits listed above.</p>
<p>Please refer to the table below for the list of NX instructions which supports FP16 operation along with their possible benefits.</p>
<table class="doxtable">
<tr>
<th>SM Instruction </th><th>Supports Vectorization (SIMD mode) </th><th>Reduced Register Usage  </th></tr>
<tr>
<td>TEXS </td><td>No </td><td>Yes </td></tr>
<tr>
<td>HADD2 </td><td>Yes (1) </td><td>Yes </td></tr>
<tr>
<td>HMUL2 </td><td>Yes (1) </td><td>Yes </td></tr>
<tr>
<td>HFMA2 </td><td>Yes (1) </td><td>Yes </td></tr>
<tr>
<td>HSET2 </td><td>Yes (1) </td><td>Yes </td></tr>
<tr>
<td>HSETP2 </td><td>Yes (1) </td><td>Yes </td></tr>
</table>
<p>(1) Cannot be vectorized under certain conditions</p>
<p>Although executing FP16 operations which do not support vectorization will not increase performance, it will still gain the reduced register usage benefits.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_7_2_3"></a>
2.7.2.3. FP16 Limitations</h4>
<p>Although some subset of FP16 instructions supports vectorization as listed above, there are some HW limitations which would prevent these instructions to operate in vectorized (SIMD) mode as listed below:</p>
<ul>
<li>If some or all of the input and output of the instruction is of FP32 precision then it would be necessary for the SM to do inline conversion from FP16 to FP32. This inline conversion prevents vectorization.</li>
<li>In NX uniform and SSBO accesses are treated as loads of FP32 values regardless if the uniform/SSBO type is FP16 or FP32. This means that FP16 instructions which accesses uniform/SSBO values must do inline conversions to FP16 which prevents vectorization.</li>
<li>Similar to uniforms and SSBO, all input/output attribute accesses are also treated as FP32 accesses in NX, preventing vectorization of FP16 instructions which reads and/or writes from/to attributes.</li>
<li>For 2 FP16 instructions to be vectorized it must have identical input and output modifiers.</li>
<li>For 2 FP16 instructions to be vectorized it must have identical predicate condition.</li>
</ul>
<p>Additionally glslc currently behaves in the following manner:</p>
<ul>
<li>If an input or output of an FP16 instruction is of precision FP32, glslc will favor inline conversion from FP32 to FP16 instead of generating an extra conversion instruction. This would prevent vectorization of the FP16 instruction. There are a few cases where vectorization is favored over inline conversion at the expense of extra instruction(s) but these cases are the minority.</li>
<li>Any FP16 instructions with predicates are not vectorized.</li>
<li>FP16 instruction pairs with different input/output modifiers will not be vectorized. The compiler will make no attempt to add extra instructions to make vectorization possible.</li>
<li>FP16 instruction pairs which belong to different blocks in the control flow will not be vectorized.</li>
<li>GLSLC will compile certain intrinsic functions into instructions that do not support FP16. Using the outputs of these instructions in FP16 operations requires inline conversion which will prevent vectorization.</li>
</ul>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_7_2_4"></a>
2.7.2.4. Guideline on FP16 Usage in Shaders</h4>
<p>To enable FP16 operations declare the default float precision to mediump or lowp. Both will map to FP16.</p>
<p>Enabling FP16 operations has the potential to yield performance gain and typically does not lead to performance losses.</p>
<p>However, in some situations, the increased warp count resulting from enabling FP16 can cause cache thrashing, leading to poorer performance than the FP32 baseline. A good rule of thumb is as follows:</p><ul>
<li>Experiment with enabling FP16 in compositing shaders. Rationale: such shaders are memory bound and require peak warps to saturate memory. They do not rely on cache locality for performance.</li>
<li>Experiment with enabling FP16 in math heavy shaders (warp throughput limiters identified as "issue" or "fp" by shader statistic analysis).</li>
</ul>
<p>Because of the potential for performance loss we recommend testing for regression whenever experimenting with enabling FP16 operations for a shader.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_7_2_5"></a>
2.7.2.5. Expected Peformance Gain</h4>
<p>As mentioned above under ideal conditions FP16 instruction can achieve twice the throughput of its FP32 counterpart. The following shader shows 2x performance in FP16 mode:</p>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;precision mediump float;</div>
<div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;</div>
<div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;void main() {</div>
<div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;</div>
<div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;  // conversion from fp32 to fp16</div>
<div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;  vec4 vColor = vtxColor;</div>
<div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;  vec4 temp1 = value1;</div>
<div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;  vec4 temp2 = value2.xyxy;</div>
<div class="line"><a name="l00009"></a><span class="lineno">    9</span>&#160;</div>
<div class="line"><a name="l00010"></a><span class="lineno">   10</span>&#160;  // math bound shader</div>
<div class="line"><a name="l00011"></a><span class="lineno">   11</span>&#160;  for (int i = 0; i &lt; 1024; i++) {</div>
<div class="line"><a name="l00012"></a><span class="lineno">   12</span>&#160;</div>
<div class="line"><a name="l00013"></a><span class="lineno">   13</span>&#160;      // pure fp16 mul instructions</div>
<div class="line"><a name="l00014"></a><span class="lineno">   14</span>&#160;      // can be vectorized</div>
<div class="line"><a name="l00015"></a><span class="lineno">   15</span>&#160;      vColor *= temp1;</div>
<div class="line"><a name="l00016"></a><span class="lineno">   16</span>&#160;      vColor += temp2;</div>
<div class="line"><a name="l00017"></a><span class="lineno">   17</span>&#160;      vColor *= temp1;</div>
<div class="line"><a name="l00018"></a><span class="lineno">   18</span>&#160;      vColor += temp2;</div>
<div class="line"><a name="l00019"></a><span class="lineno">   19</span>&#160;      vColor *= temp1;</div>
<div class="line"><a name="l00020"></a><span class="lineno">   20</span>&#160;      vColor += temp2;</div>
<div class="line"><a name="l00021"></a><span class="lineno">   21</span>&#160;      vColor *= temp1;</div>
<div class="line"><a name="l00022"></a><span class="lineno">   22</span>&#160;      vColor += temp2;</div>
<div class="line"><a name="l00023"></a><span class="lineno">   23</span>&#160;      vColor *= temp1;</div>
<div class="line"><a name="l00024"></a><span class="lineno">   24</span>&#160;      vColor += temp2;</div>
<div class="line"><a name="l00025"></a><span class="lineno">   25</span>&#160;      vColor *= temp1;</div>
<div class="line"><a name="l00026"></a><span class="lineno">   26</span>&#160;      vColor += temp2;</div>
<div class="line"><a name="l00027"></a><span class="lineno">   27</span>&#160;      vColor *= temp1;</div>
<div class="line"><a name="l00028"></a><span class="lineno">   28</span>&#160;      vColor += temp2;</div>
<div class="line"><a name="l00029"></a><span class="lineno">   29</span>&#160;      vColor *= temp1;</div>
<div class="line"><a name="l00030"></a><span class="lineno">   30</span>&#160;      vColor += temp2;</div>
<div class="line"><a name="l00031"></a><span class="lineno">   31</span>&#160;  }</div>
<div class="line"><a name="l00032"></a><span class="lineno">   32</span>&#160;</div>
<div class="line"><a name="l00033"></a><span class="lineno">   33</span>&#160;  // conversion back to fp32</div>
<div class="line"><a name="l00034"></a><span class="lineno">   34</span>&#160;  outColor = vColor*vtxColor;</div>
<div class="line"><a name="l00035"></a><span class="lineno">   35</span>&#160;}</div>
</div><!-- fragment --><p>However due to HW and compiler limitations, it is possible to see very small or no performance improvement for typical game shaders which are either not bottlenecked by register usage or are not optimized to take advantage of the conditions for enabling vectorization as described above.</p>
<p>For example, the following shader does not gain any benefit from conversion to FP16 mode due to inline conversion necessary because normalize() generates instructions that do not support FP16:</p>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;precision mediump float;</div>
<div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;in mediump vec3 vNormal;</div>
<div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;</div>
<div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;void main(void) {</div>
<div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;  mediump vec3 color = normalize(vNormal);</div>
<div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;  gl_FragColor.rgb = color;</div>
<div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;  gl_FragColor.a = 1.0;</div>
<div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;}</div>
<div class="line"><a name="l00009"></a><span class="lineno">    9</span>&#160;</div>
<div class="line"><a name="l00010"></a><span class="lineno">   10</span>&#160;Disassembly:</div>
<div class="line"><a name="l00011"></a><span class="lineno">   11</span>&#160;</div>
<div class="line"><a name="l00012"></a><span class="lineno">   12</span>&#160;     IPA.PASS   R5, a[0x7c]                   &amp;wr=0                     ?OFF_DECK;</div>
<div class="line"><a name="l00013"></a><span class="lineno">   13</span>&#160;     MUFU.RCP   R5, R5                        &amp;req={0} &amp;wr=0            ?OFF_DECK_YIELD6;</div>
<div class="line"><a name="l00014"></a><span class="lineno">   14</span>&#160;     IPA        R0, a[0x80], R5               &amp;req={0} &amp;wr=0            ?WAIT1;</div>
<div class="line"><a name="l00015"></a><span class="lineno">   15</span>&#160;     IPA        R1, a[0x84], R5               &amp;wr=1                     ?WAIT1;</div>
<div class="line"><a name="l00016"></a><span class="lineno">   16</span>&#160;     IPA        R2, a[0x88], R5               &amp;wr=2                     ?OFF_DECK;        // no fp16 variant of IPA</div>
<div class="line"><a name="l00017"></a><span class="lineno">   17</span>&#160;     HMUL2.MRG_H0 R3, R0.F32, R0.F32          &amp;req={0}                  ?WAIT6;</div>
<div class="line"><a name="l00018"></a><span class="lineno">   18</span>&#160;     HFMA2.MRG_H1 R3, R1.F32, R1.F32, R3.H0_H0  &amp;req={1}                ?WAIT6;</div>
<div class="line"><a name="l00019"></a><span class="lineno">   19</span>&#160;     HFMA2.F32  R4, R2.F32, R2.F32, R3.H1_H1  &amp;req={2}                  ?WAIT2;</div>
<div class="line"><a name="l00020"></a><span class="lineno">   20</span>&#160;     MUFU.RSQ   R3, R4                        &amp;wr=0                     ?OFF_DECK_YIELD6; // no fp16 variant of RSQ</div>
<div class="line"><a name="l00021"></a><span class="lineno">   21</span>&#160;     HMUL2.F32  R0, R3.F32, R0.F32            &amp;req={0}                  ?WAIT1_END_GROUP;</div>
<div class="line"><a name="l00022"></a><span class="lineno">   22</span>&#160;     HMUL2.F32  R1, R3.reuse.F32, R1.F32                                ?WAIT1;</div>
<div class="line"><a name="l00023"></a><span class="lineno">   23</span>&#160;     HMUL2.F32  R2, R3.F32, R2.F32                                      ?WAIT1;</div>
<div class="line"><a name="l00024"></a><span class="lineno">   24</span>&#160;     MOV32I     R3, 1.0                                                 ?PAIR;</div>
<div class="line"><a name="l00025"></a><span class="lineno">   25</span>&#160;     EXIT                                                               ?OFF_DECK;</div>
</div><!-- fragment --><h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_StaticAnalysis"></a>
2.7.3. Static Analysis</h3>
<p>The shader compiler GLSLC can output performance statistics during shader compilation. These can be used as hints to find bottlenecks and what area to concentrate on to improve shader performance. This section will discuss in detail the meaning of various performance statistics items along with suggestions for optimizations. For details on how to output performance statistics please refer to the GLSLC documentation.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_7_3_1"></a>
2.7.3.1. Precautions</h4>
<p>Because of the nature of compile time shader analysis there are a few points to keep in mind when using analysis results for optimization:</p>
<ul>
<li>Static shader analysis can give performance data which affect shader performance only. Note that overall rendering performance may be bottlenecked by something other than shaders such as vertex attribute fetch, primitive processing, etc. In these cases shader static analysis data will not be helpful for optimizing rendering performance.</li>
<li>Because of the static nature of the analysis used to generate these data, they may not be accurate for shaders whose behavior can change dynamically depending on runtime conditions. For example, static analysis of shaders containing many conditional branches and loops may not yield accurate data.</li>
<li>Finally, it is also important to note that optimizing for one particular item/operation may worsen the performance of another operation and lead to no improvement or even worse overall performance. It is important to iterate and consider the balance of all limiters.</li>
</ul>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_WarpLimiters"></a>
2.7.3.2. Warp Throughput Limiters</h4>
<p>Warp throughput limiters show how a particular operation type is limiting how many warp can be run in parallel for a particular shader. The unit is warps/cycle. The operation type with the lowest warps/cycle value is the shader bottleneck that needs to be optimized. The warp limiter for each operation type is discussed in detail below.</p>
<h5>issue, fp, half, ipa, transcendental</h5>
<p>The issue warp limiter indicates warp throughput as limited by shader math instructions of all type.</p>
<p>Warp throughput as limited by specific types of math instructions are indicated by the following:</p><ul>
<li>fp - 32-bit floating point math operations.</li>
<li>half - 16-bit floating point math operations.</li>
<li>ipa - varying interpolation operations.</li>
<li>transcendental - transcendental math operations (SIN, COS, LOG, etc).</li>
</ul>
<p>Suggestions:</p>
<p>When possible use mediump float precision. This would assist the compiler in generating SIMD fp16 instructions which can be executed simultaneously in pairs to reduce math operation latency. Additionally, SIMD fp16 instructions also has the benefit of reduced power consumption.</p>
<p>When there are branches and conditionals, group math instructions as close as possible with the code that is using the result of their calculations. This will assist the compiler in grouping instructions in such a way to avoid executing operations in branches that are not taken. For example:</p>
<p>In the "good" case below, calculation for value_a or value_b is done only when their corresponding branches are taken:</p>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;if (do_a) {</div>
<div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;  value_a = calculate_a();</div>
<div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;  value = value_a + value_c;</div>
<div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;} else {</div>
<div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;  value_b = calculate_b();</div>
<div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;  value = value_b + value_c;</div>
<div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;}</div>
<div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;return value;</div>
</div><!-- fragment --><p>In the "bad" case below, calculation for both value_a and value_b can potentially be done even though only one of them will be used:</p>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;value_a = calculate_a();</div>
<div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;value_b = calculate_b();</div>
<div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;if (do_a) {</div>
<div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;  value = value_a + value_c;</div>
<div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;} else {</div>
<div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;  value = value_b + value_c;</div>
<div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;}</div>
<div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;return value;</div>
</div><!-- fragment --><h5>shared</h5>
<p>This indicates warp throughput as limited by the following operations:</p><ul>
<li>Vertex attributes and varying loads and stores.</li>
<li>Shared memory loads and stores for compute shaders.</li>
</ul>
<p>Suggestions:</p><ul>
<li>When possible combine attributes and varyings. For example 2 vec2 attributes can be combined into a single vec4.</li>
<li>Use uniform to pass value instead of vertex attributes if possible.</li>
<li>For compute shaders, replace shared memory loads and stores with the shuffleNV() instruction which allows for exchange of values between threads in the same warp without accessing shared memory. Please refer to NV_shader_thread_shuffle extension for details.</li>
</ul>
<h5>controlFlow</h5>
<p>This indicates warp throughput as limited by the number of branches.</p>
<p>Suggestions:</p><ul>
<li>Eliminate branches whenever possible.</li>
</ul>
<h5>texLoadStore</h5>
<p>This indicates warp throughput as limited by the following operations.</p>
<p>Global memory loads and stores:</p><ul>
<li>Texture fetches.</li>
<li>SSBO loads and stores.</li>
<li>Image loads and stores.</li>
</ul>
<p>Local memory loads and stores:</p><ul>
<li>Load and stores of register values spilled to local memory.</li>
</ul>
<p>Suggestions:</p><ul>
<li>Reduce texture operations.</li>
<li>Use textureGather() and textureGatherOffsets() for better parallelization.</li>
<li>Use uniforms instead of SSBO.</li>
<li>Minimize the number of local variables.</li>
</ul>
<h5>reg</h5>
<p>This indicates warp throughput as limited by the register usage count per-warp and the shader register usage pattern.</p>
<p>In every sub-partition there are 512 registers that must be shared by all active warps.</p>
<p>If the number of registers needed by each warp is too high it could limit throughput by reducing warp occupancy, which is the ratio of the number of possible warps that can be active and the maximum number of active warps possible in the HW.</p>
<p>If the shader register usage pattern requires that registers are needed for long periods then throughput can be limited by the increased shader latency because there is not enough free register to launch additional warps.</p>
<p>The reg warp limiter captures how throughput is limited by both warp occupancy and shader latency.</p>
<p>Suggestions:</p>
<p>When possible use mediump float precision. Two fp16 float values can be packed into a single fp32 register to reduce register usage. Additionally, SIMD fp16 instructions also has the benefit of reduced power consumption.</p>
<p>Improve shader latency by assisting compiler in batching together high latency operations such as texture fetches. One of the ways the compiler can batch high latency operations is by loop unrolling. To help compiler in loop unrolling we suggest the following ways to write loops:</p><ul>
<li>Loops with known iteration count and increment operator of add or substract can be unrolled completely. For example: <div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;for (uint i = 0; i &lt; 5; i++) {</div>
<div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;  ..</div>
<div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;}</div>
</div><!-- fragment --></li>
<li>Loops with increment operator other than add or substract cannot be unrolled. If possible avoid creating such loops. For example the following loop cannot be unrolled: <div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;for (uint i = 1; i &lt; 10; i *= 2) {</div>
<div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;  ..</div>
<div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;}</div>
</div><!-- fragment --></li>
<li>Compiler is less efficient at unrolling for loops which contain if conditional blocks. If possible avoid them. For example the following loop cannot be efficiently unrolled: <div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;for (..) {</div>
<div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;  ..</div>
<div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;  if (..) {</div>
<div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;    ..</div>
<div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;  }</div>
<div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;  ..</div>
<div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;}</div>
</div><!-- fragment --></li>
<li>In general avoid complex conditions if possible to assist in unrolling.</li>
</ul>
<p>How successful the compiler has been in unrolling loops can be discovered by checking the values of "Loop data", a part of the performance statistics output:</p><ul>
<li>PartiallyUnrolled indicates the number of loops which the compiler was able to unroll to some degree but not completely.</li>
<li>Nonunrolled indicates the number of loops which the compiler was not able to unroll.</li>
</ul>
<h5>warp</h5>
<p>This indicates warp throughput as limited by the time it takes to launch the warp within the SM sub-partition.</p>
<p>Suggestions:</p>
<p>Usually this limiter becomes the bottleneck when the workload of the warp is so light that its execution is dominated by the warp launch itself. As such optimizing the shader itself will usually not lead to performance improvements. Instead, we suggest searching for bottlenecks other than shaders (i.e. fixed function processing, attribute fetches, etc).</p>
<h2><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_8"></a>
2.8. Vertex Processing</h2>
<p>Optimize your geometry for spatial and memory locality of vertex accesses.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_8_1"></a>
2.8.1. Optimize your index-buffer</h3>
<p>The NX primitive distributor (PD) builds batches containing up-to 32 triangles or 32 unique indices. Reordering your index-buffer so that triangles sharing vertices are close enough to fit in a batch allows to generate less batches and therefore improves performance.</p>
<p>The PD is optimized for 1 new index per triangle. Triangles ordered in strip/fan pattern or forming an hilbert-ish curve will allow the PD to function at maximum speed.</p>
<p>There are several known algorithms that allow to optimize the index-buffer. We recommend using the forsyth algorithm [For06] for the following reasons:</p><ul>
<li>it generates few number of batches</li>
<li>it generates hilbert-ish triangle order maximizing PD performance</li>
<li>it is fast to run</li>
</ul>
<p>[For06] <a href="http://tomforsyth1000.github.io/papers/fast_vert_cache_opt.html">http://tomforsyth1000.github.io/papers/fast_vert_cache_opt.html</a></p>
<h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_8_2"></a>
2.8.2. Optimize Vertex-arrays fetches</h3>
<p>This optimization is generally done after optimizing the index-buffer by reordering vertices in the same order as they come in the index-buffer so that vertex-data is accessed roughly in linear order.</p>
<p>Vertex attributes that are accessed together in shader should be interleaved in memory.</p>
<p>Vertex attributes are fetched in vec4 units so recommend packing attributes into vec4s.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_8_3"></a>
2.8.3. Optimize world space to pixel space data transfer</h3>
<p>When pixel space processing is enabled, overall performance must take into account the write/read latencies to and from the L2 cache needed to transfer vertex attributes and other data from the world space to pixel space pipelines. This latency may become a perf bottleneck when vertex shader invocation count is high and/or there is a large number of varyings. In these cases recommend packing the varyings as much as possible and/or minimize vertex shader invocations to reduce L2 read/write bandwidth needed.</p>
<p>Please refer to the GPU overview document for details on L2 cache performance and Beta Circular Buffers (CBs).</p>
<h2><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_9"></a>
2.9. Data Copies</h2>
<p>NVN provides four commands allowing applications to schedule GPU operations copying data from one buffer object or texture to another buffer object or texture. For further details please refer to the "Data Copy Commands" section of the NVN programming guide.</p>
<p>When these copy commands are submitted to a queue (via a command set), they are executed in order relative to commands before and after the copy. The copy commands will see the results of any commands sent to the queue prior to the copy, and any commands sent to the queue after the copy will see the results of the copy. To guarantee this the 3D pipeline will be idled before executing the copy command and subsequent command after the copy will not be started until the copy operation has been completed. Because idling the 3D pipeline may reduce performance, we recommend batching data copies to share this overhead over as many copy operations as possible.</p>
<p>Please also refer to <a class="el" href="gpu_overview__maxwell_best_practices_index.html#gpuOverview_MaxwellBestPractices_guide_TextureCopies">2.10.2. Texture Copies</a> for more details and suggestions about texture copies.</p>
<h2><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_10"></a>
2.10. Texturing</h2>
<h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_10_1"></a>
2.10.1. General Suggestions</h3>
<p>Texture filtering operation in the GPU is optimized for bilinear interpolation. Additional interpolation will reduce throughput through serialization. For example trilinear filtering is bilinear filtering done twice.</p>
<p>When possible use textureGatherOffsets() to explicitly parallelize texture sampling.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_TextureCopies"></a>
2.10.2. Texture Copies</h3>
<p>This section contains guidelines for ensuring correctness and optimizing performance of texture copies. There are 2 methods available in NVN to copy the content of one texture to another which will be discussed below.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_10_2_1"></a>
2.10.2.1. Texture Copy with 3D Pipeline</h4>
<p>Texture content can be copied by executing a draw call which uses a fragment shader that samples the content of the source texture and writes it out to the destination texture bound as a render target. To ensure correctness a barrier needs to be inserted before the copy texture draw call if there are previous draw calls which write to the source texture. Similarly a barrier needs to be inserted after the copy texture draw call if there are subsequent draw calls which samples from the destination texture.</p>
<p>The barrier bits which are used as arguments to <a class="el" href="group__nvn__c__functions.html#ga8004a74da7c68a5424ac4466653dce34" title="Specify a barrier ordering execution of GPU commands and invalidating internal GPU caches...">nvnCommandBufferBarrier()</a> in the cases above must be carefully selected to ensure both correctness and optimal performance. Incorrect barrier bits will cause incorrect result while sub-optimal barrier bits may reduce performance by causing the 3D pipeline to stall longer than necessary.</p>
<p>The following are general recommendations:</p><ul>
<li>Barriers with INVALIDATE_TEXTURE bits are usually necessary before and after the operation to ensure that the copy will not fetch old cached values from the source texture and subsequent draw calls do not fetch old cached values from the destination texture.</li>
<li>Barriers with one of ORDER_PRIMITIVES, ORDER_FRAGMENTS, or ORDER_FRAGMENTS_TILED bits are also necessary to ensure that the copy doesn't start until previous rendering commands that write to the source have finished and that subsequent rendering commands don't start until the copy is completed.</li>
<li>ORDER_FRAGMENTS should be sufficient if the textures can be written to and sampled from fragment shaders only before and after the copy operation. Otherwise the more restrictive ORDER_PRIMITIVES would be necessary.</li>
<li>ORDER_FRAGMENTS_TILED can be used instead of ORDER_FRAGMENTS if and only if any copied texel is accessed only when processing fragments with the same (x,y) coordinates.</li>
<li>If application code is already using barriers for other purposes, overhead can be reduced by ordering rendering operations so that the same barriers can be used to order copy operations at the same time.</li>
</ul>
<p>Note that a single barrier can be specified with multiple bits. Please refer to the "Barriers" section of the NVN programming guide document for detailed description of the various barrier bits as well as usage guidelines and examples.</p>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_10_2_2"></a>
2.10.2.2. Texture Copy with RSTR2D Unit</h4>
<p>Texture content can be copied through command generated by calling <a class="el" href="group__nvn__c__functions.html#gaf7137adca4ad85a28917d01374c09219" title="Copy texture data from a region of one NVNtexture object into a region of another NVNtexture object...">nvnCommandBufferCopyTextureToTexture()</a>. Please refer to the NVN programming guide for details on how to use this function.</p>
<p>Copying texture with this method makes use of Maxwell's RSTR2D unit. Please see <a class="el" href="gpu_overview__maxwell_technical_overview_index.html#gpuOverview_MaxwellTechnicalOverview_guide_unit_descriptions_RSTR2D">4.1.6. RSTR2D (2d Rasterizer)</a> for a description of this unit.</p>
<p>Texture copies which used the RSTR2D unit will always idle the 3D pipeline before the start of the texture copy and will not restart the 3D pipeline until the copy has been completed. This ensures that all previous draw calls that write to the source texture have completed and subsequent draw calls that sample from the destination texture will read the correct values.</p>
<p>Because idling the 3D pipeline can reduce performance we recommend the following to minimize its impact:</p><ul>
<li>If possible, execute the texture copy operations when the 3D pipeline is already idle.</li>
<li>Batch texture copy operations to share the cost of idling the 3D pipeline over as many texture copies as possible.</li>
</ul>
<h4><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_2_10_2_3"></a>
2.10.2.3. Additional Performance Considerations</h4>
<ul>
<li>Texture copies with RSTR2D unit has maximum performance of 8 pixels-per-clock whereas texture copies with the 3D pipeline has maximum performance of 14.4 pixels-per-clock as limited by PROP throughput in the GPC (please see <a class="el" href="gpu_overview__maxwell_technical_overview_index.html#gpuOverview_MaxwellTechnicalOverview_guide_performance">8.1. PERFORMANCE SUMMARY</a>). To achieve best performance it is recommended to use the 3D pipeline for texture copies.</li>
<li>The performance of texture copies and shader accesses of depth-stencil surfaces are lower than texture copies and shader accesses of color and depth-only surfaces. This is related to the layout of depth-stencil surfaces in memory where depth and stencil values are packed into separate blocks. (please see <a class="el" href="gpu_overview__maxwell_technical_overview_index.html#gpuOverview_MaxwellTechnicalOverview_guide_graphics_Z_AND_STENCIL_PACKING">5.5.1. Z and Stencil Packing</a>). Experiments show that access efficiency is reduced for two specific reasons:<ul>
<li>All Non-ZROP GPU accesses of depth-stencil surfaces are required to go through L2 for unpacking.</li>
<li>Memory to L2 transfers are likely to span more data bank sectors than optimal for depth-stencil surfaces because of the special memory layout mentioned above.</li>
</ul>
</li>
</ul>
<h1><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_3"></a>
3. Compute</h1>
<h2><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_3_1"></a>
3.1. Interaction with Graphics Workload</h2>
<p>The Maxwell architecture requires that the 3D pipeline be drained and idled before executing a compute shader. Similarly all compute shader executions must be finished before the 3D pipeline can be restarted.</p>
<p>Since this draining and idling can reduce performance we recommend the following when interleaving compute dispatch and graphics draw calls:</p><ul>
<li>If possible, execute compute dispatch calls when the 3D pipeline is already idle.</li>
<li>Batch compute dispatch operations to share the cost of idling the 3D pipeline over as many dispatches as possible.</li>
</ul>
<h2><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_3_2"></a>
3.2. Shaders</h2>
<h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_3_2_1"></a>
3.2.1. UBO</h3>
<p>On NX, hardware constant accesses are supported for only the first 5 uniform blocks. Accesses to uniform blocks with an assigned binding number of 5 or higher will be emulated using global memory accesses, usually with lower access performance. We recommend that compute shaders use no more than 5 uniform blocks to avoid this performance penalty. If this is not possible, applications should assign its most frequently accessed uniform blocks to bindings 0 though 4 using "layout(binding=N)" to minimize the performance penalty.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_3_2_2"></a>
3.2.2. SSBO</h3>
<p>If threads within a dispatch call loads and stores from shader storage blocks, try to make the access pattern as coherent as possible. That is, the range of addresses being loaded or stored by all threads should be as small as possible to maximize performance.</p>
<p>If multiple threads need to write to sequential locations within a single SSBO, glsl atomic operations such as atomicAdd() can be used to reserve space and maximize memory access locality.</p>
<h3><a class="anchor" id="gpuOverview_MaxwellBestPractices_guide_sec_3_2_3"></a>
3.2.3. Shared Memory</h3>
<p>If accesses to shared memory becomes a performance limiter in compute shaders consider using the shuffleNV() instruction which allows exchange of values between threads in the same warp without incurring any access to shared memory. Please see the "shared" section of <a class="el" href="gpu_overview__maxwell_best_practices_index.html#gpuOverview_MaxwellBestPractices_guide_WarpLimiters">2.7.3.2. Warp Throughput Limiters</a> for more on shared memory performance limiter based on static shader analysis. Please refer to NV_shader_thread_shuffle glsl extension for further details on shuffleNV() function. </p>
</div></div><!-- contents -->
<!-- HTML footer for doxygen 1.8.8-->
<!-- start footer part -->
</body>
</html>
